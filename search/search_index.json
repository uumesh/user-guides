{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ALCF User Guides","text":"<p>We are moving our ALCF documentation into GitHub to make it easier to contribute and collaborate to our user and machine guides.</p> <p>Our user guides contain information for: </p> <ul> <li>Account and Project Management: Information and instructions on how to manage your ALCF account and awarded project.  </li> <li>Data Management: Information on our file systems that are mounted globally across all of our production systems.</li> <li>Aurora/Sunspot: Information on getting your code ready for our upcoming exacale supercomputer.</li> <li>Polaris: Information on how to get started our newest supercomputer.</li> <li>AI Testbed: Information on how to use our AI Accelerators.</li> <li>Services: Information on how to use various services provided across clusters.</li> <li>Facility Policies: Information on our policies and procedures.</li> </ul>"},{"location":"#how-to-get-access","title":"How to Get Access","text":"<p>Researchers interested in using the ALCF systems (including Polaris and the AI Testbed\u2019s Cerebras CS-2 and SambaNova DataScale platforms) can now submit project proposals via the ALCF\u2019s Director\u2019s Discretionary program. Calls for porposals for additional allocation programs will be open at a later date.</p> <p>Submit your proposal requests at: Allocation Request Page</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>If you'd like to get started using our ALCF resources, our Getting Started webpage provides information on what you need to do in order to get time on our systems, get an account, and how to start running jobs.</p> <p>If you have an account and an award for Polaris, we suggest visiting on Getting on Polaris webpage.</p> <p>If you'd like to user our AI accelerators, visit our Getting Started on AI Testbed webpage.</p> <p>Please send feedback to support@alcf.anl.gov</p>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/","title":"Accounts and Access FAQ","text":""},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#how-do-i-request-a-new-projectallocation","title":"How do I request a new project/allocation?","text":"<p>There are 3 allocation opportunities at ALCF. Please see How to Get an Allocation on how to get time on our systems.</p>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#who-do-i-contact-if-my-discretionary-project-allocation-expires-or-if-i-need-to-request-additional-hours","title":"Who do I contact if my Discretionary Project Allocation expires or if I need to request additional hours?","text":"<p>To request an extension of your existing discretionary allocation or to request additional hours, please email support@alcf.anl.gov with answers to the following or fill out the form at request an extension/additional hours: - What you have accomplished with your original allocation?   - Please include a brief description of any publications or major presentations that were (or will be) generated in full or in part because of this allocation. - What you will do with the extra time? - What you are requesting as your new expiration date? - How many additional hours you are requesting?</p>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#how-do-i-join-a-project","title":"How do I join a project?","text":"<p>To join a project, please go to https://accounts.alcf.anl.gov, then click \"join a project\". Once there, scroll down to the project you want to join and click on it. At the bottom of the next page, please click on the \"Request Membership\" button. Once we receive approval from the PI regarding your membership request, we will provide you with access to the necessary resources.</p>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#how-do-i-request-a-reservation","title":"How do I request a reservation?","text":"<p>Reservation requests must include information detailed here: </p> <ul> <li>Machine Reservations: Please email the completed reservation request to support@alcf.anl.gov. We will contact you after your request is reviewed by our reservations committee.</li> </ul>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#how-do-i-apply-for-a-new-account","title":"How do I apply for a new account?","text":"<p>Note: All ALCF accounts must be associated with an allocated project.</p> <ul> <li>Request a new account: https://www.alcf.anl.gov/support-center/get-started/request-account</li> <li>ALCF Accounts: https://accounts.alcf.anl.gov/</li> </ul>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#what-do-i-do-when-my-alcf-account-expires","title":"What do I do when my ALCF account expires?","text":"<p>Please forward your account expiry email to your Sponsor. As soon as we receive an approval email from your Sponsor, we'll proceed with your account renewal process as needed.</p>"},{"location":"account-project-management/accounts-and-access/accounts-and-access-faqs/#what-do-i-do-when-i-receive-a-warning-that-my-593-has-expired-is-about-to-expire","title":"What do I do when I receive a warning that my 593 has expired / is about to expire?","text":"<p>If you are planning to extend this assignment/computer user account, please let us know, so a new 593 (Foreign Visit &amp; Assignment Request form) will be filed for you using the information from before. In case any other documents are needed from your end, you'll be contacted as necessary. In order to allow sufficient time for an indices check, it is recommended that your response be submitted as soon as possible.</p> <p>If you are not planning to extend your account, also let us know so that we may close out your records.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/","title":"ALCF Passcode Tokens","text":"<p>Please note: An account can be associated with a single token only (Mobile or Physical token). Please contact accounts@alcf.anl.gov to change your token preference.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#mobile-token","title":"Mobile Token","text":"<p>The SafeNet MobilePass+ Mobile Token allows access to ALCF systems. This security mobile token uses one-time passwords combined with your PIN for controlled access to the login systems. The mobile token utilizes an app that is keyed to your user account and for which you are responsible on your Android, iPhone or Windows mobile device. Please safeguard your phone as you would your credit cards or house keys: Do not store username, PIN, or other account-related records with the token. Sharing of mobile tokens is strictly forbidden. A mobile token can be associated with a single device only.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#step-1-download-the-safenet-mobilepass-app-for-your-device","title":"Step 1. Download the SafeNet MobilePass+ app for your device:","text":"<p>The SafeNet MobilePASS+ app turns your device into a two-factor authentication device, removing the need to carry an additional hardware token. As a SafeNet MobilePASS+ user, you can generate passcodes on your mobile device and use those passcodes to authenticate on ALCF computing resources. See supported OS and platforms for more information.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#step-2-enroll-your-mobilepass-mobile-token","title":"Step 2. Enroll your MobilePass+ mobile token:","text":"<p>After you\u2019ve been provisioned a mobile token, you will receive a notification email with the subject line \"ALCF Mobile Token Self-Enrollment\" which you must access from the device on  which you wish to install the token.</p> <p>Auto-Enrollment (to enroll SafeNet MobilePass+ token automatically):</p> <ol> <li>Click on the http:// link in the email. The SafeNet Authentication Service Self-Enrollment will open.</li> <li>Click enroll your SafeNet MobilePass+ token.</li> <li>When prompted to open in MobilePass+ tap Open.</li> <li>You will now be prompted to enter a 6 digit all numeric PIN.</li> <li>Enter your PIN in the Token PIN field and repeat in the Confirm PIN field.</li> <li>You will be taken to the Enrollment Complete screen to name the token.</li> <li>Insert the desired name in the Token Name field or leave it as is. This name is not utilized by the server; it is for you only.</li> <li>The newly enrolled SafeNet MobilePass+ token is now displayed in the SafeNet MobilePass+ app.</li> </ol> <p>Manual Enrollment:</p> <ol> <li>Copy the activation string from the SafeNet provision email.</li> <li>Open the SafeNet MobilePass+ app and tap the manual option.</li> <li>Paste the enrollment string into the field provided and tap the Enroll button.</li> <li>You will now be prompted to enter a 6 all numeric PIN.</li> <li>Enter your PIN in the Token PIN field and repeat in the Confirm PIN field.</li> <li>You will be taken to the Enrollment Complete screen to name the token.</li> <li>Insert the desired name in the Token Name field or leave it as is. This name is not utilized by the server; it is for you only.</li> </ol>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#logging-in-to-an-alcf-system-using-a-mobile-token","title":"Logging in to an ALCF System using a Mobile Token","text":"<ol> <li>Open the MobilePASS+ app on your device. Then initiate an SSH session and type the following:</li> </ol> <pre><code>ssh &lt;ALCF username&gt;@&lt;system_name&gt;.alcf.anl.gov\n</code></pre> <ol> <li> <p>When prompted for a password, click the SafeNet MobilePASS+ app on your phone. Click on the token name listed within the app, and enter your PIN.</p> </li> <li> <p>The app will display your passcode immediately.  Enter the passcode as the login password for the system within the SSH session. Please Note: You do NOT have to enter the PIN on the SSH screen when logging into a resource. This only needs to be done to access the passcode within the SafeNet MobilePASS+ app.</p> </li> <li> <p>Each generated passcode is valid on the SafeNet MobilePass+ app window until your mobile device screen times out.</p> </li> </ol>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#troubleshooting-your-mobile-token","title":"Troubleshooting your Mobile Token","text":"<p>Case 1: Forgotten PIN: If you enter a PIN for your mobile Token and you get an invalid PIN, you will be asked to re-enter your PIN. After 6 failed attempts your token will be deleted and you will need to call the ALCF help desk or send an email to ALCF support to have a new mobile token provisioned.</p> <p>Case 2: Account Lockout: If you fail to enter the correct password 6 times, you will get a permission denied error on the SSH screen. Upon 4 more failed attempts, your IP will be blocked. You will need to call the ALCF help desk and submit a ticket to have the IP unblocked.</p> <p>Case 3: PIN Change: While logged in to the mobile token, click on token settings then tap change PIN. Enter the current PIN followed by the new PIN and confirm.</p> <p>Case 4: Re-Sync: If you are unable to log in to a resource after entering the correct PIN and passcode your token may be out of sync with the server. Please email ALCF Service Desk at accounts at alcf.anl.gov for assistance.</p> <p>Case 5: New Mobile Device: If you have a new mobile device, please email the ALCF Service Desk at accounts at alcf.anl.gov to have a new mobile token provisioned. </p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#physical-token","title":"Physical Token","text":"<p>The physical token allows access to the ALCF systems. This security token uses one-time passwords combined with your PIN for controlled access to the login systems. The physical token is a tracked asset for which you are responsible and is keyed to your use. Please safeguard your token as you would your credit cards or house keys: Do not store username, PIN, or other account-related records with the token. Sharing of tokens is strictly forbidden. Please do not mark on the token or alter it in any way.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#enabling-your-alcf-physical-token","title":"Enabling Your ALCF Physical Token","text":"<p>Upon receipt of CRYPTOCard token, contact accounts@alcf.anl.gov to verify your identity and activate the token. If this step is not performed the CRYPTOCard token will not be able to log on to the ALCF resource.</p> <p>ALCF Accounts Service Desk Info Hours: Monday-Friday 9 a.m. - 5 p.m. (Central time); Email: accounts@alcf.anl.gov</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#logging-in-to-an-alcf-system-using-a-physical-token","title":"Logging in to an ALCF System using a Physical Token","text":"<p>When the physical token is activated, an initial PIN will be provided. This will be a four-digit number that will prepend to the one-time password string generated by the token.</p> <p>Upon INITIAL login (to one of the ALCF machines), a prompt to change the PIN will appear. PINs must be at least four characters long and must only contain numbers.</p> <ol> <li> <p>Initiate an SSH session using: <pre><code>ssh &lt;ALCF username&gt;@&lt;system_name&gt;.alcf.anl.gov\n</code></pre></p> </li> <li> <p>A password prompt will be received. At this point, push the button on the physical token once.</p> </li> <li> <p>An eight-character, one-time password made up of letters and numbers will appear on the token\u2019s display. This one-time password is case-sensitive.</p> </li> <li> <p>Type your PIN followed immediately by the one-time password at the SSH password prompt.</p> </li> </ol> <p>For example, if your PIN is 1234 and you received the one-time password string ABCD9876, you would type 1234ABCD9876 at the password prompt.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#troubleshooting-your-physical-token","title":"Troubleshooting Your Physical Token","text":"<p>Case 1: It says \"locked\": The physical token may be locked due to too many failed attempts. Please contact the ALCF Help Desk to return the defective token and so a replacement can be sent.</p> <p>Case 2: You have a PIN for your physical token: Once a PIN has been set for your physical token, you will need to prepend your PIN to the token password. Otherwise you will not be able to log in. If you do not remember your PIN, please email us so we can verify your identity and reset your Initial PIN.</p> <p>Case 3: It does not say \"locked\" but still does not work: It is likely that your token has fallen out of sync with the server. If you have pushed the button on your physical token more than 10 times without successfully logging in, it will fail to authenticate because it has lost synchronization with the server. Please try connecting to Polaris first. If it still fails, please follow the re-sync instructions below.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#re-sync-instructions","title":"Re-Sync Instructions","text":"<p>If you have pushed the button on your physical token more than 10 times, it will fail to authenticate because it has lost synchronization with the server. You can re-synchronize your token using the following procedure:</p> <pre><code>1. Have your physical token ready.\n\n2. Obtain a challenge sequence:\n    - Initiate an SSH session to a host that allows token\n      authentication (such as polaris.alcf.anl.gov). At the password\n      prompt, just hit 'Enter'. This will cause the Cryptocard service\n      to produce a challenge string consisting of 8 numbers.\n\n3. Hold down the button on your token for a few seconds until the\n    display says \"Init\", then let go.\n\n4. The token will scroll through a series of menu options. When it\n    displays \"ReSync\", hit the button again.\n\n5. The display will say\n\n     Resync?0\n\n6. The number at the end will start cycling from 0 to 9, over and over.\n\n7. Look at the numbers in your challenge string. When the number\n    displayed on your token changes to the first number of the challenge\n    string, press the button. The display will now show this number, and\n    the second digit will start cycling.\n\n8. Enter each of the numbers from your challenge string in the same\n    manner, until the display on your token matches the entire challenge string.\n    Choose the \"&lt;\" to backspace and re-enter the previous number if\n    necessary.\n\n9. Once you've entered all 8 digits, re-check to make sure they're\n    accurate. Then, while all 8 digits are displayed on the token, press\n    the button to generate a new password.\n\n10. Enter your PIN followed by the new password, and hit 'Enter'. \n     If successful, you will be logged in to the resource. You're now back \n     in sync with the authentication server.\n\nIf you are unsuccessful, you will be presented with another challenge string. \nAt this point, you may need to perform the re-sync instructions again.\n</code></pre> <p>If there are still problems after completing the re-synchronization procedures, please email us at accounts@alcf.anl.gov so we can run a test on the physical token to determine if it is defective. </p> <p>If it is found to be defective we will promptly replace it. Physical tokens are the property of Argonne National Laboratory.</p> <p>Please return them to us at:</p> <pre><code>ALCF Help Desk\nArgonne National Laboratory\n9700 S. Cass Ave.\nBldg. 240, Rm. 2129\nLemont, IL 60439\n</code></pre>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#resetting-the-physical-token-pin","title":"Resetting the Physical Token PIN","text":"<p>Please email us at support at alcf.anl.gov for PIN resets. Once your identity has been verified, we will provide you with a new PIN for your CRYPTOcard token.</p>"},{"location":"account-project-management/accounts-and-access/alcf-passcode-tokens/#returning-a-physical-token","title":"Returning a Physical Token","text":"<p>If you no longer need your physical token, please return it to this address:</p> <pre><code>ALCF Help Desk\nArgonne National Laboratory\n9700 S. Cass Ave.\nBldg. 240, Rm. 2129\nLemont, IL 60439\n</code></pre>"},{"location":"account-project-management/accounts-and-access/user-account-overview/","title":"ALCF User Account Overview","text":"<p>All computing carried out on the ALCF systems is associated with a user \"account.\" This account is used to log onto the login servers and run jobs on the resources. If someone has a user account, then he or she has a login name that is recorded in the user database. This web page describes the process that users will need to understand to manage account details, including policies and procedures.</p> <p>If you need an account, visit the Accounts and Project Management website: Request an account</p> <p>If you want to learn how to get started, visit the Get Started Guide: Get Started Guide</p>"},{"location":"account-project-management/accounts-and-access/user-account-overview/#who-can-get-an-account","title":"Who Can Get an Account","text":"<p>Those who are interested in having an account on a ALCF resource must first request an allocation and provide a detailed description of the work, including computational requirements and coding capabilities for the Blue Gene platform. Another means of acquiring an allocation on the ALCF system is to be part of a project team that already has an active allocation. Once an allocation has been granted, new users should complete an account request. A project\u2019s Principal Investigator (PI) must sponsor these accounts\u2014if the PI is the user, an ALCF staff member must serve as sponsor. Sponsors are asked annually to evaluate the accounts they have sponsored to determine whether or not these accounts should be kept active.</p>"},{"location":"account-project-management/accounts-and-access/user-account-overview/#account-abilities","title":"Account Abilities","text":"<p>A user with an active account can login to the ALCF login servers (e.g., polaris.alcf.anl.gov) This account will have some home directory space, where file transfer can occur from that space via the login nodes, and where development activities, such as editing and compiling, can also occur.</p>"},{"location":"account-project-management/accounts-and-access/user-account-overview/#account-states","title":"Account States","text":"<p>Accounts are classified in one of the following categories:</p> <ul> <li>Pending: An account that has been requested but has not yet been created.</li> <li>Active: An account that can be used to interact with the ALCF Login Servers. This is the normal state for all accounts.</li> <li>Inactive: An account that still exists on the system (that is, the account continues to be registered in the database and the user's files exist on disk) but the user cannot interact with the ALCF Login Servers. An account might be disabled due to misuse, security concerns, or because it is no longer allocated.</li> <li>Deleted: An account that existed on the system and is thus in the records and backups, but whose user no longer has access to the systems or files on disk.</li> </ul>"},{"location":"account-project-management/accounts-and-access/user-account-overview/#more-information","title":"More Information","text":"<ul> <li>Account Policy</li> <li>User Authentication Policy</li> <li>Account Sponsorship and Retention Policy</li> </ul>"},{"location":"account-project-management/allocation-management/allocation-management/","title":"Managing Your Allocations","text":"<p>Allocations require management \u2013 balance checks, resource allocation, requesting more time, etc.</p>"},{"location":"account-project-management/allocation-management/allocation-management/#checking-for-an-active-allocation","title":"Checking for an Active Allocation","text":"<p>To determine if there is an active allocation, check Job Submission.</p> <p>For information on how to run the query, look at our documentation on our sbank Allocations Accounting System or email support@alcf.anl.gov and ask for all active allocations.</p>"},{"location":"account-project-management/allocation-management/allocation-management/#using-sbank-to-determine-the-balance-of-an-allocation","title":"Using sbank to Determine the Balance of an Allocation","text":"<p>To determine which platforms have an active balance, check our allocation accounting system sbank.</p> <ul> <li>To obtain the allocation balance, check the sbank command sbank-list-allocations.</li> <li>DD projects with a negative balance will not be able to run jobs until they have requested additional time, see Getting more time below.</li> <li>INCITE and ALCC PIs automatically email a summary of project usage.  If this is a DD project, please email support@alcf.anl.gov.</li> </ul>"},{"location":"account-project-management/allocation-management/allocation-management/#allocation-expiration","title":"Allocation Expiration","text":"<p>Projects and allocations at the ALCF are different.  A particular project might have multiple allocations of time. For example, a discretionary project that has been approved for more than 3 times will have 3 allocations (2 are probably expired) but just one project. Projects will not expire, allocations will. If allocations are expired, or have no hours left, jobs will not be able to run. Use the two bullets above (Checking for an active allocation and Determining the balance of an allocation) to determine active allocations.</p>"},{"location":"account-project-management/allocation-management/allocation-management/#getting-more-time","title":"Getting More Time","text":"<p>To request an extension of your existing discretionary allocation or to request additional hours, please email support@alcf.anl.gov with answers to the following:</p> <ul> <li>What you have accomplished with your original allocation?</li> <li>Please include a brief description of any publications or major presentations that were (or will be) generated in full or in part because of this allocation.</li> <li>What you will do with the extra time?</li> <li>What you are requesting as your new expiration date?</li> <li>How many additional hours you are requesting?</li> </ul>"},{"location":"account-project-management/allocation-management/allocation-management/#sub-allocations","title":"Sub-allocations","text":"<p>Suballocations let PIs control who in their team can runs jobs, how much they are allowed to consume (allocation amount), and when they are allowed to run jobs (start and end dates)</p> <p>Step 1: Create Suballocations (Project PI):</p> <p>PI creates suballocations </p> <p><code>sbank new sub &lt;allocationid&gt; **-name &lt;nameofsuballoc&gt;</code></p> <p>Tip: see <code>sbank new suballocation -h</code> for all the options. </p> <p>Step 2: Manage Suballocations (Project PI)</p> <p>PI adds users to suballocations</p> <p><code>sbank e sub &lt;projectname&gt;::&lt;nameofsuballoc&gt; --add-user=\"&lt;username1&gt; &lt;username2&gt; ...\"</code></p> <p>PI can change the name of a suballocation </p> <p><code>sbank e sub &lt;suballocationID&gt; --name=&lt;new_name_of_suballocation&gt;</code></p> <p>By default, the primary suballocation (which is the default suballocation created when the allocation is created by ALCF) is unrestricted .i.e. enabled for all project members.  That means all project members can submit jobs against the primary suballocation by default. All other suballocations are restricted by default and users have to be added for each of them.</p> <p>To change the default for the primary suballocation to restrict usage, PI must first edit the suballocation:</p> <p><code>sbank-edit-suballocation --restrict &lt;primary suballocation id&gt;</code></p> <p>Then add users with this command:</p> <p><code>sbank e sub &lt;primary suballocation id&gt;  --add-user=\"&lt;username1&gt; &lt;username2&gt; ...\"</code></p> <p>PI changes start and end dates for a suballocation:</p> <p><code>sbank e sub &lt;suallocationID&gt; -S &lt;start_date&gt; -E &lt;end_date&gt;</code></p> <p>PI adds hours to a suballocation:</p> <p><code>sbank e sub &lt;projectname&gt;::&lt;nameofsuballoc&gt; --hours-to-move &lt;hours&gt; --to-suballocation &lt;projectname&gt;::&lt;nameofsuballoc2&gt;</code></p> <p>Note: <code>hourstomove</code> must be greater than or equal to the available balance for the suballocation <code>nameofsuballoc</code></p> <p>Tip: see <code>sbank e suballocation -h</code> for all the options</p> <p>Step 3: Submit Jobs  (Project team)</p> <p>Submit jobs to a suballocation. Note that the user should be on the suballocation\u2019s user list </p> <p><code>Eg: qsub -l select=10,walltime=30:00,filesystems=grand:home -A &lt;suballoctionID&gt; -q demand test.sh</code></p> <p>Note: Once submanagement is enabled for a project allocation, all job submissions must specify the <code>suballocationID</code></p> <p>Useful commands: List all suballocations for a project that shows number of jobs run, charges, allocation balance, suballocation name, and list of users</p> <p><code>sbank-list-allocations -r polaris -p &lt;projectname&gt; -f\u201d+subname users_list\u201d</code></p> <p>Tip: see <code>sbank l a -h</code> for all the options and <code>sbank \u2013f\\?</code> for list of fields that can be displayed </p>"},{"location":"account-project-management/allocation-management/overview/","title":"Allocations on ALCF Computing Resources","text":""},{"location":"account-project-management/allocation-management/overview/#getting-an-allocation-award","title":"Getting an Allocation Award","text":""},{"location":"account-project-management/allocation-management/overview/#incite-alcc-and-adsp","title":"INCITE, ALCC, and ADSP","text":"<p>Researchers gain access to ALCF systems for computational science and engineering projects\u2014typically with awards of millions of core-hours\u2014through competitive, peer-reviewed allocation programs supported by the DOE and Argonne. Our peer-reviewed award programs consist of the INCITE, ALCC, and ADSP programs. More information about the programs, including dates for our CFPs, can be found on their web pages.</p>"},{"location":"account-project-management/allocation-management/overview/#directors-discretionary","title":"Director's Discretionary","text":"<p>Alternatively, ALCF offers a Director's Discretionary allocation award program to leadership computing preparation, INCITE and ALCC scaling, and application performance to maximize scientific application efficiency and productivity on leadership computing platforms. See the Director's Discretionary (DD) Program page for more information.</p>"},{"location":"account-project-management/allocation-management/overview/#initializing-your-awarded-allocation","title":"Initializing Your Awarded Allocation","text":"<p>Projects with INCITE, ALCC, and ADSP awards will be contacted directly by the ALCF staff with information on creating accounts.</p> <p>Director's Discretionary awards will receive information in the award confirmation email. </p>"},{"location":"account-project-management/allocation-management/overview/#allocation-resources","title":"Allocation Resources","text":"<p>While requesting an allocation, users can choose from:</p> <p>Computes:  * Polaris</p> <p>File System:  * Grand * Eagle (Community Sharing)</p>"},{"location":"account-project-management/allocation-management/overview/#policy-information-related-to-allocations","title":"Policy Information Related to Allocations","text":"<p>Pullback Policy</p>"},{"location":"account-project-management/allocation-management/overview/#requesting-additional-allocation-hours","title":"Requesting Additional Allocation Hours","text":"<p>If you are a PI of a Director's Discretionary project that has an active allocation, you can request additional time or an extension using the allocation request form.</p> <p> </p> To request more hours, renew your project using the allocation request form."},{"location":"account-project-management/allocation-management/sbank-allocation-accounting-system/","title":"sbank Allocation Accounting System","text":"<p>sbank is the accounting system used within the ALCF. It tracks project allocations, usage charges, and refunds. sbank allows queries about the balance and expiration of project allocations, and has replaced the outdated cbank accounting system.</p> <p>The sbank accounting system helps users manage their allocations and usage per job. It gives the PIs the ability to monitor their allocation usage by user, job, and machine. It also allows the user to monitor their usage per allocation and provides insight on how many hours are left on the project.</p>"},{"location":"account-project-management/allocation-management/sbank-allocation-accounting-system/#getting-started-with-sbank","title":"Getting Started with sbank","text":"<p>sbank Example Commands provides a set of example commands on how to use the most common commands. </p>"},{"location":"account-project-management/allocation-management/sbank-allocation-accounting-system/#sbank-man-pages","title":"sbank Man Pages","text":"<p>Use these sbank man pages to get information on how to use the commands.</p> <ul> <li>sbank</li> <li>sbank-detail</li> <li>sbank-detail-allocations</li> <li>sbank-detail-jobs</li> <li>sbank-detail-projects</li> <li>sbank-detail-transactions</li> <li>sbank-detail-users</li> <li>sbank-list</li> <li>sbank-list-allocations</li> <li>sbank-list-jobs</li> <li>sbank-list-projects</li> <li>sbank-list-transactions</li> <li>sbank-list-users</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/","title":"Manpage for sbank-detail-allocations","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#sbank-detail-allocations-options","title":"sbank-detail-allocations [options] [ ... ] <p>Detail allocation information. </p> <p>NOTE:    1. The list of  arguments are optional.    2. you can also enter  list by using the -a option multiple times.    3. regardless, both are optional, and you can get detail allocation info using the option filters below.","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-version","title":"--version","text":"<p>show program's version number and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-h-help","title":"-h, --help","text":"<p>show this help message and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>filter on allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-e-event_id-event-idevent_id","title":"-e EVENT_ID, --event-id=EVENT_ID","text":"<p>filter on event id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p>FIELD_INFO is [:], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-j-jobid-jobidjobid","title":"-j JOBID, --jobid=JOBID","text":"<p>filter on jobid</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>set number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-t-transaction_id-transaction-idtransaction_id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID","text":"<p>filter on transaction id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width=","text":"<p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-e-end-endend","title":"-E END, --end=END","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>**Date Parsing Precedence: **</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-h-human-readable","title":"-H, --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions), ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-i-get-inactive","title":"-I, --get-inactive","text":"<p>also get inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-o-get-only-inactive","title":"-O, --get-only-inactive","text":"<p>only inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-s-start-startstart","title":"-S START, --start=START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-t-transaction_type-transaction-typetransaction_type","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE","text":"<p>transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-award-type-nameaward_type_name","title":"--award-type-name=AWARD_TYPE_NAME","text":"<p>filter on award type name</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-award-categoryaward_category","title":"--award-category=AWARD_CATEGORY","text":"<p>filter on award category</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-cbank-refcbank_ref","title":"--cbank-ref=CBANK_REF","text":"<p>filter on Clusterbank reference id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-createdcreated_timestamp","title":"--created=CREATED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-get-deleted","title":"--get-deleted","text":"<p>also get deleted objects</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-get-only-deleted","title":"--get-only-deleted","text":"<p>only deleted objects</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-all-charges","title":"--all-charges","text":"<p>only show list info that have charges regardless of project/user relationship</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-history-date-rangeend","title":"--history-date-range=END","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-last-updatedlast_updated_timestamp","title":"--last-updated=LAST_UPDATED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults: </p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-no-header","title":"--no-header","text":"<p>do not display the header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-no-history","title":"--no-history","text":"<p>do not show history information</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-no-rows","title":"--no-rows","text":"<p>do not display the row data</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-allocations/#-no-totals","title":"--no-totals","text":"<p>do not display the totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-jobs/","title":"sbank-detail-jobs","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-jobs/#sbank-detail-jobs-options","title":"sbank-detail-jobs [options] [  |  ...  | ] <p>Detail job information.  NOTE: </p> <ol> <li>The arguments  or  are NOT REQUIRED;  <li>event_id is the JOB DATABASE ID; </li> <li> is the SCHEDULER CREATED ID, such as Cobalt;  <li> can also be entered using option -j  ;  <li> can also be entered using option -e  ;  <li> can also be entered using option -r  ;  <li>regardless, you can use options or arguments to get detail job information</li>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-jobs/#options","title":"OPTIONS","text":"<p>--version</p> <p>show program's version number and exit</p> <p>-h, --help</p> <p>show this help message and exit</p> <p>-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID</p> <p>filter on allocation id</p> <p>-e EVENT_ID, --event-id=EVENT_ID</p> <p>filter on event id</p> <p>-f FIELD_INFO, --field-to-display=FIELD_INFO</p> <p>FIELD_INFO is [:], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\" <p>-j JOBID, --jobid=JOBID</p> <p>filter on jobid</p> <p>-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY</p> <p>set number of fields to display</p> <p>-p PROJECT, --project=PROJECT</p> <p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p> <p>-r RESOURCE, --resource=RESOURCE</p> <p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p> <p>-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID</p> <p>filter on transaction id</p> <p>-u USER, --user=USER</p> <p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p> <p>-w \"FIELD_INFO\", --field-width</p> <p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\" <p>-E END, --end=END</p> <p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le,\u00a0lt,\u00a0eq or &gt;=, &gt;, &lt;=, &lt;, ==. Operator Defaults: OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 <p>-H, --human-readable</p> <p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p> <p>-S START, --start=START</p> <p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le,\u00a0lt,\u00a0eq or &gt;=, &gt;,&lt;=, &lt;, == . Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 <p>-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE</p> <p>transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID</p> <p>--created=CREATED_TIMESTAMP</p> <p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le,\u00a0lt,\u00a0eq or &gt;=, &gt;, &lt;=, &lt;, ==. Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 <p>--debug=DEBUG_LEVEL</p> <p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p> <p>--eligible=ELIGIBLE_TIMESTAMP</p> <p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le,\u00a0lt,\u00a0eq or &gt;=, &gt;, &lt;=, &lt;, ==. Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 <p>--get-not-charged</p> <p>only un-charged jobs</p> <p>--history-date-range=END</p> <p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le,\u00a0lt,\u00a0eq or &gt;=, &gt;, &lt;=, &lt;, ==. Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 <p>--last-updated=LAST_UPDATED_TIMESTAMP</p> <p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le,\u00a0lt,\u00a0eq or &gt;=, &gt;, &lt;=, &lt;, ==. Operator Defaults: OPER1 is 'gt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012 <p>--no-commas</p> <p>remove commas from comma separated thousands</p> <p>--no-header</p> <p>do not display the header</p> <p>--no-history</p> <p>do not show history information</p> <p>--no-rows</p> <p>do not display the row data</p> <p>--no-sys-msg</p> <p>do not display system message</p> <p>--no-totals</p> <p>do not display the totals</p> <p>--queued=QUEUED_TIMESTAMP</p> <p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following: ge, gt, le,\u00a0lt,\u00a0eq or &gt;=, &gt;, &lt;=, &lt;, ==. Operator Defaults: OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. Date Parsing Precedence: YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/","title":"Manpage for sbank-detail-projects","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#sbank-detail-projects-options","title":"sbank-detail-projects [options] [ ... ] <p>Detail project information. </p> <p>NOTE:    1. The list of  arguments are optional   2. you can also enter  list by using the -p option multiple times   3. regardless, both are optional, and you can get detail project info using the option filters below","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-version","title":"--version","text":"<p>show program's version number and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-h-help","title":"-h, --help","text":"<p>show this help message and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>filter on allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p>FIELD_INFO is [:], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>set number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-e-end-endend","title":"-E END, --end=END","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:    - YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-h-human-readable","title":"-H, --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-i-get-inactive","title":"-I, --get-inactive","text":"<p>get inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-s-start-startstart","title":"-S START, --start=START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-all-charges","title":"--all-charges","text":"<p>only show list info that have charges regardless of project/user relationship</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-no-header","title":"--no-header","text":"<p>do not display the header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-no-rows","title":"--no-rows","text":"<p>do not display the row data</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-projects/#-no-totals","title":"--no-totals","text":"<p>do not display the totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/","title":"Manpage for sbank-detail-transactions","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#sbank-detail-transactions-options","title":"sbank-detail-transactions [options] [ ... ] <p>Detail transaction information. </p> <p>NOTE:    1. The list of  arguments are optional   2. you can also enter  list by using the -t option multiple times   3. regardless, both are optional, and you can get detail transaction info using the option filters below","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-version","title":"--version","text":"<p>show program's version number and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-h-help","title":"-h, --help","text":"<p>show this help message and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>filter on allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-c-comment","title":"-c, --comment","text":"<p>display comment</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-e-event_id-event-idevent_id","title":"-e EVENT_ID, --event-id=EVENT_ID","text":"<p>filter on event id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p>FIELD_INFO is [:] for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-j-jobid-jobidjobid","title":"-j JOBID, --jobid=JOBID","text":"<p>filter on jobid</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>set number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-t-transaction_id-transaction-idtransaction_id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID","text":"<p>filter on transaction id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width=","text":"<p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-e-job_end-endjob_end","title":"-E JOB_END, --end=JOB_END","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-h-human-readable","title":"-H, --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-s-job_start-startjob_start","title":"-S JOB_START, --start=JOB_START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-t-transaction_type-transaction-typetransaction_type","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE","text":"<p>transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-attransaction_at_timestamp","title":"--at=TRANSACTION_AT_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-cbank-refcbank_ref","title":"--cbank-ref=CBANK_REF","text":"<p>filter on Clusterbank reference id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-createdjob_created_timestamp","title":"--created=JOB_CREATED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-no-header","title":"--no-header","text":"<p>do not display the header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-no-rows","title":"--no-rows","text":"<p>do not display the row data</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-no-totals","title":"--no-totals","text":"<p>do not display the totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-transactions/#-queuedjob_queued_timestamp","title":"--queued=JOB_QUEUED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:  <ul> <li>ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==. </li> </ul> <p>Operator Defaults:</p> <ul> <li>OPER1 is 'ge' for single date entry</li> <li>OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence:</p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/","title":"Manpage for sbank-detail-users","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#sbank-detail-users-options","title":"sbank-detail-users [options] [ ... ] <p>Detail user information. </p> <p>**NOTE: **   1. Use -I to include inactive allocations   2. the list of  arguments are optional   3. you can also enter  list by using the -u option multiple times   4. regardless, both are optional, and you can get detail user info using the option filters below","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-version","title":"--version","text":"<p>show program's version number and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-h-help","title":"-h, --help","text":"<p>show this help message and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>filter on allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p>FIELD_INFO is [:], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>set number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-e-end-endend","title":"-E END, --end=END","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-h-human-readable","title":"-H, --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-i-get-inactive","title":"-I, --get-inactive","text":"<p>get inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-s-start-startstart","title":"-S START, --start=START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-all-charges","title":"--all-charges","text":"<p>only show list info that have charges regardless of project/user relationship</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-no-header","title":"--no-header","text":"<p>do not display the header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-no-rows","title":"--no-rows","text":"<p>do not display the row data</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail-users/#-no-totals","title":"--no-totals","text":"<p>do not display the totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/","title":"Manpage for sbank-detail","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#sbank-detail-options","title":"sbank-detail  [options] <p>Detail Meta Command</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#commands","title":"COMMANDS","text":"<ul> <li>allocations [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] (DEFAULT) </li> <li>categories [-f|-n|-w|...] </li> <li>messages [-f|-n|-w|...] </li> <li>names [-f|-n|-w|...] jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] </li> <li>projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...] </li> <li>transactions [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] </li> <li>users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...]</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-a-allocation","title":"-a --allocation","text":"<p>enter allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-c-comment","title":"-c --comment","text":"<p>enter comment for new or edit commands, display comment for list commands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-e-event-id","title":"-e --event-id","text":"<p>enter event db id; event db id is an internal id created by the charging system</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-f-field","title":"-f --field","text":"<p>enter [:], width is optional; enter -f? or -f \"?\" for available fields, + to add fields"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-h-help","title":"-h --help","text":"<p>command line help</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-j-jobid","title":"-j --jobid","text":"<p>enter jobid; jobid is created by the scheduler and is not unique</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-n-num-field","title":"-n --num-field","text":"<p>enter number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-p-project","title":"-p --project","text":"<p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-r-resource","title":"-r --resource","text":"<p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-s-suballocation","title":"-s --suballocation","text":"<p>enter suballocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-t-transaction","title":"-t --transaction","text":"<p>enter transaction id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-u-user","title":"-u --user","text":"<p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-w-field-width","title":"-w --field-width","text":"<p>enter the field width as follows: :, enter -w? or -w \"?\" for available fields"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-e-end","title":"-E --end","text":"<p>enter end datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-h-human-readable","title":"-H --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-i-get-inactive","title":"-I --get-inactive","text":"<p>include inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-o-get-only-inactive","title":"-O --get-only-inactive","text":"<p>get only inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-s-start","title":"-S --start","text":"<p>enter start datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-t-type","title":"-T --Type","text":"<p>enter type of transaction</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-all-charges","title":"--all-charges","text":"<p>for list allocations | projects | users, only show info with charges</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-at","title":"--at","text":"<p>enter transaction created datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-award-category","title":"--award-category","text":"<p>enter allocation award category</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-award-type-name","title":"--award-type-name","text":"<p>enter allocation award-type name</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-created","title":"--created","text":"<p>enter created datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-debug","title":"--debug","text":"<p>enter debug level</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-get-deleted","title":"--get-deleted","text":"<p>get deleted objects</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-get-not-charged","title":"--get-not-charged","text":"<p>get jobs that have not been charged</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-get-only-deleted","title":"--get-only-deleted","text":"<p>get only deleted objects</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-history-date-range","title":"--history-date-range","text":"<p>enter history datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-last-updated","title":"--last-updated","text":"<p>enter last updated datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma-separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-no-header","title":"--no-header","text":"<p>do not display header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-no-history","title":"--no-history","text":"<p>do not display history information</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-no-rows","title":"--no-rows","text":"<p>do not display rows</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-no-totals","title":"--no-totals","text":"<p>do not display totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-detail/#-queued","title":"--queued","text":"<p>enter queued datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/","title":"sbank Example Commands","text":"<p>Below is a set of helpful commands to help you better manage the projects you have running at the ALCF.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#view-your-projects-allocations","title":"View your project's allocations","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#command-sbank-list-allocations","title":"Command: sbank-list-allocations","text":"<p>Use this command to list all of your active allocations for a specific project [Project-X]. This is useful when you need to provide this information in a report. <pre><code>&gt; sbank-list-allocations -p ProjectX -r all\n Id         Start       End         Resource   Project          Jobs        Charged          Available Balance \n ---------  ----------  ----------  ---------  ---------------  ----------  ---------------  ----------------- \n 2106       2016-01-04  2017-01-01  cooley     ProjectX              1,139          6,032.8           43,967.2 \n 2146       2016-01-14  2017-01-10  theta      ProjectX                983      1,084,770.3       25,483,927.5\n 6438       2020-09-22  2022-01-01  thetagpu   ProjectX                  3              0.0            2,000.0 \n\n\nTotals:\n  Rows: 3\n  Cooley:\n    Available Balance: 43,967.2 node hours\n    Charged          : 6,032.8 node hours\n    Jobs             : 1,139 \n Theta:\n    Available Balance: 25,483,927.5 node hours \n    Charged          : 1,084,770.3 node hours \n    Jobs             : 983 \n Thetagpu:\n    Available Balance: 2,000.0 node hours\n    Charged          : 0.0 node hours\n    Jobs             : 3 \n</code></pre></p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#list-your-projects-quota-on-grand-andor-eagle-file-system","title":"List your project's quota on Grand and/or Eagle File system","text":"<pre><code>&gt; sbank-list-allocations -p ProjectX -r grand\n Allocation  Suballocation  Start       End         Resource  Project      Quota\n ----------  -------------  ----------  ----------  --------  -----------  -----\n 6687        6555           2020-12-16  2022-01-01  grand     ProjectX    1.0\n\nTotals:\n  Rows: 1\n  Grand:\n    Quota: 1.0 TB\n\n&gt; sbank-list-allocations -p ProjectX -r eagle\n Allocation  Suballocation  Start       End         Resource  Project      Quota\n ----------  -------------  ----------  ----------  --------  -----------  -----\n 6688        6556           2020-12-16  2022-01-01  eagle     ProjectX    1.0\n\nTotals:\n  Rows: 1\n  Eagle:\n    Quota: 1.0 TB\n</code></pre>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#list-only-the-created-timestamp-field-for-all-allocations-that-were-created-before-01-01-2015-for-projectx-accross-all-resources","title":"List only the created timestamp field for all allocations that were created before 01-01-2015 for ProjectX accross all resources","text":"<pre><code>&gt; sbank-list-allocations  --created \"&lt;20150101\" -r all -p ProjectX \"-f created\"\n Created    \n ---------- \n 2016-01-04 \n 2016-01-14 \n 2016-01-15 \n\nTotals:\n  Rows: 3\nDate  filters (UTC): created &lt; \"2015-01-01 00:00:00\",  \n</code></pre>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#list-all-active-allocations-for-all-resources-for-project-projectx-and-add-the-field-created-to-the-display-list","title":"List all active allocations for all resources for project ProjectX and add the field Created \ufeffto the display list","text":"<pre><code>shrubbery~ &gt; sbank-list-allocations -r all  -p ProjectX -f \"+created\"\n Id         Start       End         Resource   Project          Jobs        Charged          Available Balance  Created    \n ---------  ----------  ----------  ---------  ---------------  ----------  ---------------  -----------------  ---------- \n 279        2011-08-30  2020-01-01  theta      ProjectX              6,361     12,332,699.9      -12,332,699.9  2013-02-22 \n 2106       2016-01-04  2017-01-01  cooley     ProjectX              1,150          6,080.9           43,919.1  2016-01-04  \n\nTotals:\n  Rows: 2\n  Theta:\n    Available Balance: -12,332,699.9 node hours\n    Charged          : 12,332,699.9 node hours\n    Jobs             : 6,361 \n  Cooley:\n    Available Balance: 43,919.1 node hours\n    Charged          : 6,080.9 node hours\n    Jobs             : 1,150 \n</code></pre>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#list-all-available-fields-for-the-sbank-list-allocations-command","title":"List all available fields for the sbank-list-allocations command","text":"<pre><code>&gt; sbank-list-allocations  -f \"?\"\navailable fields:\n id\n start_timestamp\n end_timestamp\n resource\n project_name\n jobs_count\n charged_sum\n available_balance_sum\n created_timestamp\n award_category\n award_type_name\n admin_name\n cbank_ref\n comment\n</code></pre>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#view-your-projects-users","title":"View your project's users","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-examples/#command-sbank-list-users","title":"Command: sbank-list-users","text":"<p>List all charges for userx on theta on project ProjectX <pre><code>&gt; sbank-list-users -p ProjectX -r theta -u userx\n User             Jobs        Charged         \n ---------------  ----------  --------------- \n userx                 1,814          9,884.5\n\nTotals:\n  Rows: 1\n  Resources: theta\n  Charged: 9,884.5 node hours\n  Jobs   : 1,814 \n  ```\n\n### List charges for all users in ProjectX on Cooley.\nThis works for project leads (i.e. PIs, Co-PIs, Proxies), since they can see everything in their own projects.\n</code></pre></p> <p>sbank-list-users -p ProjectX -r theta  User             Jobs        Charged         </p> <p>user1                   120          4,243.7   user2                     0              0.0   user3                     0              0.0   user4                   181          1,195.5   user5                     0              0.0   user6                 2,560         10,868.7   user7                     0              0.0   user8                     0              0.0   user9                     0              0.0   user10                    7              3.5   user11                    0              0.0 </p> <p>Totals:   Rows: 11   Resources: theta   Charged: 16,311.4 node hours   Jobs   : 2,868 </p> <p><pre><code>## View your project's jobs\nList jobs for user \"userx\" for jobs that started in the range 2016-02-15&lt;= started &lt; 2016-02-29 and add the transactions related to the job\n\n### **Command:** sbank-list-jobs\n\n**Note:** The job with the refund ```transaction_ids_list field can be shorten all the way to \"t\" in the -f \"+ t\"```\n</code></pre> shrubbery~ &gt; sbank-list-jobs -u userx -f \"+ t\" -S \"2016-02-15...2016-02-29\"  Id         Jobid      Resource   Project          Allocation  User       Duration   Charged          Transaction Ids </p> <p>1013857    730417     theta       ProjectX         1740        userx      1:53:07           61,776.8  CHARGE-1011230  1013860    730558     theta       ProjectX         1740        userx      1:53:07           61,776.8  CHARGE-1011233  1014168    730668     theta       ProjectX         1740        userx      1:53:25           61,940.6  CHARGE-1011541  </p> <p>Totals:   Rows: 3   Theta:     Charged      : 185,494.2 node hours     Duration     : 6:44:00  Date  filters (UTC): \"2016-02-15 00:00:00\" &lt;= start &lt; \"2016-02-29 00:00:00\", <pre><code>### List the nodes used, runtime and start timestamp for Cooley job 744160\n**Note**: To display the date and time we increased the the number of characters of start_timestamp to 19\n</code></pre> catapult~ &gt; sbank l j -r theta -j 50576 -f \"jobid nodes_used runtime start_timestamp:19\" Jobid Nodes Used Runtime Start --------- ---------- --------- ------------------- 50576 512 1:00:49 2013-01-16 21:49:30 Totals: Rows: 1 <pre><code>## View your project's transactions\n### **Command:** sbank-list-transactions\n\nList of transactions that where at or after 2016-02-29 for ProjectX add fields: job_duration, nodes_used and hosts\n\n**Note**: \n- job_duration, nodes_used and hosts are shorten, but they are still uniquely identified\n- host has the left justified width of 20, specified as \"h:-20\"\n</code></pre> catapult~ &gt; sbank-list-transactions -p ProjectX --at \"ge 2016-02-29\" -f \"+ job_d nodes_u h:-20\" -r theta  Id         Resource   Project          Allocation  At          User             Transaction Type  Amount           Jobid      Job Duration  Nodes Used  Hosts                </p> <p>1025426    theta       ProjectX         2147        2016-02-29  userx            CHARGE                   48,005.1  740587     1:27:54       2048        MIR-00800-33BF1-2048   1028046    theta       ProjectX         2147        2016-03-01  userx            CHARGE                  147,647.1  742090     4:30:21       2048        MIR-40000-733F1-2048   1028755    theta       ProjectX         2147        2016-03-02  userx            CHARGE                1,576,068.0  742126     6:00:44       16384       MIR-04000-77FF1-1638 </p> <p>Totals:   Rows: 3   Theta:     Charges Amount: 1,771,720.2 node hours     Job Duration  : 11:58:98  Date  filters (UTC) : at &gt;= \"2016-02-29 00:00:00\", ```</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/","title":"Manpage for sbank-list-allocations","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#sbank-list-allocations-options","title":"sbank-list-allocations [options]","text":"<p>Generate allocation list report. </p> <p>Notes:  1. Use -I to include inactive allocations 2. enter \"-r all\" to get information for all resources</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-version","title":"--version","text":"<p>show program's version number and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-h-help","title":"-h, --help","text":"<p>show this help message and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>filter on allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-c-comment","title":"-c, --comment","text":"<p>display comment</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-e-event_id-event-idevent_id","title":"-e EVENT_ID, --event-id=EVENT_ID","text":"<p>filter on event id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p>FIELD_INFO is [:], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-j-jobid-jobidjobid","title":"-j JOBID, --jobid=JOBID","text":"<p>filter on jobid</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>set number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-t-transaction_id-transaction-idtransaction_id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID","text":"<p>filter on transaction id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-e-end-endend","title":"-E END, --end=END","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-h-human-readable","title":"-H, --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-i-get-inactive","title":"-I, --get-inactive","text":"<p>get inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-o-get-only-inactive","title":"-O, --get-only-inactive","text":"<p>only inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-s-start-startstart","title":"-S START, --start=START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-t-transaction_type-transaction-typetransaction_type","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE","text":"<p>transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-award-type-nameaward_type_name","title":"--award-type-name=AWARD_TYPE_NAME","text":"<p>filter on award-type name</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-award-categoryaward_category","title":"--award-category=AWARD_CATEGORY","text":"<p>filter on award category</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-cbank-refcbank_ref","title":"--cbank-ref=CBANK_REF","text":"<p>filter on Clusterbank reference id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-createdcreated_timestamp","title":"--created=CREATED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-get-deleted","title":"--get-deleted","text":"<p>get deleted objects</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-get-only-deleted","title":"--get-only-deleted","text":"<p>get only deleted objects</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-all-charges","title":"--all-charges","text":"<p>only show list info that have charges regardless of project/user relationship</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-last-updatedlast_updated_timestamp","title":"--last-updated=LAST_UPDATED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma-separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-no-header","title":"--no-header","text":"<p>do not display the header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-no-rows","title":"--no-rows","text":"<p>do not display the row data</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-allocations/#-no-totals","title":"--no-totals","text":"<p>do not display the totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/","title":"Manpage for sbank-list-jobs","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#sbank-list-jobs-options","title":"sbank-list-jobs [options]","text":"<p>Generate job list report Note: To get information for all resources, enter \"-r all\".</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-version","title":"--version","text":"<p>show program's version number and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-h-help","title":"-h, --help","text":"<p>show this help message and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>filter on allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-e-event_id-event-idevent_id","title":"-e EVENT_ID, --event-id=EVENT_ID","text":"<p>filter on event id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p>FIELD_INFO is [:], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-j-jobid-jobidjobid","title":"-j JOBID, --jobid=JOBID","text":"<p>filter on jobid</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>set number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-t-transaction_id-transaction-idtransaction_id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID","text":"<p>filter on transaction id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-e-end-endend","title":"-E END, --end=END","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-h-human-readable","title":"-H, --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-s-start-startstart","title":"-S START, --start=START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-t-transaction_type-transaction-typetransaction_type","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE","text":"<p>transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-createdcreated_timestamp","title":"--created=CREATED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-eligibleeligible_timestamp","title":"--eligible=ELIGIBLE_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-get-not-charged","title":"--get-not-charged","text":"<p>get only jobs that have not been charged</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-last-updatedlast_updated_timestamp","title":"--last-updated=LAST_UPDATED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma-separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-no-header","title":"--no-header","text":"<p>do not display the header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-no-rows","title":"--no-rows","text":"<p>do not display the row data</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-no-totals","title":"--no-totals","text":"<p>do not display the totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-jobs/#-queuedqueued_timestamp","title":"--queued=QUEUED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/","title":"Manpage for sbank-list-projects","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#sbank-list-projects-options","title":"sbank-list-projects [options]","text":"<p>Generate project list report. </p> <p>Notes: </p> <ol> <li>Use -I to include inactive allocations</li> <li> <ol> <li>to get information for all resources, enter \"-r all\"</li> </ol> </li> </ol>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-version","title":"--version","text":"<p>show program's version number and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-h-help","title":"-h, --help","text":"<p>show this help message and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>filter on allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p>FIELD_INFO is [:], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>set number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\" <p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-h-human-readable","title":"-H, --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-i-get-inactive","title":"-I, --get-inactive","text":"<p>get inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-s-start-startstart","title":"-S START, --start=START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-all-charges","title":"--all-charges","text":"<p>only show list info that have charges regardless of project/user relationship</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma-separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-no-header","title":"--no-header","text":"<p>do not display the header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-no-rows","title":"--no-rows","text":"<p>do not display the row data</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-projects/#-no-totals","title":"--no-totals","text":"<p>do not display the totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/","title":"Manpage for sbank-list-transactions","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#sbank-list-transactions-options","title":"sbank-list-transactions [options]","text":"<p>Generate transaction list report. </p> <p>Note: To get information for all resources, enter \"-r all\".</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-version","title":"--version","text":"<p>show program's version number and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-h-help","title":"-h, --help","text":"<p>show this help message and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>filter on allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-c-comment","title":"-c, --comment","text":"<p>display comment</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-e-event_id-event-idevent_id","title":"-e EVENT_ID, --event-id=EVENT_ID","text":"<p>filter on event id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p>FIELD_INFO is [:], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-j-jobid-jobidjobid","title":"-j JOBID, --jobid=JOBID","text":"<p>filter on jobid</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>set number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-t-transaction_id-transaction-idtransaction_id","title":"-t TRANSACTION_ID, --transaction-id=TRANSACTION_ID","text":"<p>filter on transaction id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-e-job_end-endjob_end","title":"-E JOB_END, --end=JOB_END","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-h-human-readable","title":"-H, --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-s-job_start-startjob_start","title":"-S JOB_START, --start=JOB_START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-t-transaction_type-transaction-typetransaction_type","title":"-T TRANSACTION_TYPE, --transaction-type=TRANSACTION_TYPE","text":"<p>transaction types: CHARGE, REFUND, PULLBACK, DEPOSIT, VOID</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-attransaction_at_timestamp","title":"--at=TRANSACTION_AT_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-cbank-refcbank_ref","title":"--cbank-ref=CBANK_REF","text":"<p>filter on Clusterbank reference id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-createdjob_created_timestamp","title":"--created=JOB_CREATED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma-separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-no-header","title":"--no-header","text":"<p>do not display the header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-no-rows","title":"--no-rows","text":"<p>do not display the row data</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-no-totals","title":"--no-totals","text":"<p>do not display the totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-transactions/#-queuedjob_queued_timestamp","title":"--queued=JOB_QUEUED_TIMESTAMP","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/","title":"Manpage for sbank-list-users","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#sbank-list-users-options","title":"sbank-list-users [options]","text":"<p>Generate user list report. </p> <p>Notes: </p> <ol> <li>Use -I to include inactive allocations</li> <li> <ol> <li>for information for all resources, use \"-r all\"</li> </ol> </li> </ol>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-version","title":"--version","text":"<p>show program's version number and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-h-help","title":"-h, --help","text":"<p>show this help message and exit</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-a-allocation_id-allocation-idallocation_id","title":"-a ALLOCATION_ID, --allocation-id=ALLOCATION_ID","text":"<p>filter on allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-f-field_info-field-to-displayfield_info","title":"-f FIELD_INFO, --field-to-display=FIELD_INFO","text":"<p>FIELD_INFO is [:], for available fields enter -f? or -f \"?\", to add fields enter -f \"+ [:] ...\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-n-num_fields_to_display-num-fields-to-displaynum_fields_to_display","title":"-n NUM_FIELDS_TO_DISPLAY, --num-fields-to-display=NUM_FIELDS_TO_DISPLAY","text":"<p>set number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-p-project-projectproject","title":"-p PROJECT, --project=PROJECT","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-r-resource-resourceresource","title":"-r RESOURCE, --resource=RESOURCE","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-u-user-useruser","title":"-u USER, --user=USER","text":"<p>filter on name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-w-field_info-field-width","title":"-w \"FIELD_INFO\", --field-width","text":"<p>\"FIELD_INFO\" FIELD_INFO is :, for available fields enter -w? or -w \"?\""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-e-end-endend","title":"-E END, --end=END","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-h-human-readable","title":"-H, --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-i-get-inactive","title":"-I, --get-inactive","text":"<p>also get inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-s-start-startstart","title":"-S START, --start=START","text":"<p>[OPER1][...[OPER2]], where the operators OPER1 and OPER2 can be one of the following:    - ge, gt, le, lt, eq or &gt;=, &gt;, &lt;=, &lt;, ==.  <p>Operator Defaults: </p> <ul> <li>OPER1 is 'lt' for single date entry, OPER1 and OPER2 are 'ge' and 'lt', respectively, for range date entry. </li> </ul> <p>Date Parsing Precedence: </p> <ul> <li>YEAR then MONTH then DAY, i.e., 121101 is parsed as YYMMDD, hence Nov. 1, 2012</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-debugdebug_level","title":"--debug=DEBUG_LEVEL","text":"<p>SILENT, MUCH_LESS, LESS, MORE, VERBOSE, DEBUG, DEBUG1, DEBUG2</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-all-charges","title":"--all-charges","text":"<p>only show list info that have charges regardless of project/user relationship</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma-separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-no-header","title":"--no-header","text":"<p>do not display the header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-no-rows","title":"--no-rows","text":"<p>do not display the row data</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list-users/#-no-totals","title":"--no-totals","text":"<p>do not display the totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/","title":"Manpage for sbank-list","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#sbank-list-options","title":"sbank-list  [options] <p>List Meta Command</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#commands","title":"COMMANDS","text":"<ul> <li>allocations [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] (DEFAULT) </li> <li>categories [-f|-n|-w|...] messages [-f|-n|-w|...] names [-f|-n|-w|...] </li> <li>jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] </li> <li>projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...] </li> <li>transactions [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] </li> <li>users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...]</li> </ul>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-a-allocation","title":"-a --allocation","text":"<p>enter allocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-c-comment","title":"-c --comment","text":"<p>enter comment for new or edit commands, display comment for list commands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-e-event-id","title":"-e --event-id","text":"<p>enter event db id; event db id is an internal id created by the charging system</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-f-field","title":"-f --field","text":"<p>enter [:], width is optional; enter -f? or -f \"?\" for available fields, + to add fields"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-h-help","title":"-h --help","text":"<p>command line help</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-j-jobid","title":"-j --jobid","text":"<p>enter jobid; jobid is created by the scheduler and is not unique</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-n-num-field","title":"-n --num-field","text":"<p>enter number of fields to display</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-p-project","title":"-p --project","text":"<p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-r-resource","title":"-r --resource","text":"<p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-s-suballocation","title":"-s --suballocation","text":"<p>enter suballocation id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-t-transaction","title":"-t --transaction","text":"<p>enter transaction id</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-u-user","title":"-u --user","text":"<p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-w-field-width","title":"-w --field-width","text":"<p>enter the field width as follows: :, enter -w? or -w \"?\" for available fields"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-e-end","title":"-E --end","text":"<p>enter end datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-h-human-readable","title":"-H --human-readable","text":"<p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-i-get-inactive","title":"-I --get-inactive","text":"<p>include inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-o-get-only-inactive","title":"-O --get-only-inactive","text":"<p>include inactive allocations</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-s-start","title":"-S --start","text":"<p>enter start datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-t-type","title":"-T --Type","text":"<p>enter type of transaction</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-all-charges","title":"--all-charges","text":"<p>for list allocations | projects | users, only show info with charges</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-at","title":"--at","text":"<p>enter transaction-created datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-award-category","title":"--award-category","text":"<p>enter allocation award category</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-award-type-name","title":"--award-type-name","text":"<p>enter allocation award-type name</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-created","title":"--created","text":"<p>enter created datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-debug","title":"--debug","text":"<p>enter debug level</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-get-deleted","title":"--get-deleted","text":"<p>get deleted objects</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-get-not-charged","title":"--get-not-charged","text":"<p>get jobs that have not been charged</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-get-only-deleted","title":"--get-only-deleted","text":"<p>get only deleted objects</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-history-date-range","title":"--history-date-range","text":"<p>enter history datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-last-updated","title":"--last-updated","text":"<p>enter last updated datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-no-commas","title":"--no-commas","text":"<p>remove commas from comma-separated thousands</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-no-header","title":"--no-header","text":"<p>do not display header</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-no-history","title":"--no-history","text":"<p>do not display history information</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-no-rows","title":"--no-rows","text":"<p>do not display rows</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-no-sys-msg","title":"--no-sys-msg","text":"<p>do not display system message</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-no-totals","title":"--no-totals","text":"<p>do not display totals</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-list/#-queued","title":"--queued","text":"<p>enter queued datetime filter</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/","title":"Manpage for sbank Commands","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#sbank-options","title":"sbank   [options]","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#description","title":"DESCRIPTION","text":"<p>HPC Accounting System Command Line Interface</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#detail-meta-command","title":"detail meta command","text":"<p>\"detail\" meta command displays information in a long format with history updates, where appropriate.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#list-meta-command","title":"list meta command","text":"<p>\"list\" meta command displays information in a table format, but no history updates are displayed.</p> <p>IMPORTANT NOTES   1. All dates entered shall be interpreted as UTC   2. non-admin users will only be able to see their content (jobs, charges, etc.)   3. project admin users will be able to see all of the content for their projects   4. staff admin users will be able to see all the content   5. --help and -h are the help options.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#meta-commands","title":"META COMMANDS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-detail-options","title":"- detail  [options]","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-list-options-default","title":"- list  [options] (DEFAULT) <p>DETAIL COMMANDS   * allocations [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] [ ... ] (DEFAULT)    * jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] [ ... ]    * projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...] [ ... ]    * transactions [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] [ ... ]    * users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...] [ ... ] <p>LIST COMMANDS   * allocations [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-I|-O|-S|-T|...] (DEFAULT)    * jobs [-a|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...] projects [-a|-f|-n|-p|-r|-u|-w|-E|-H|-I|-S|...]    * transactions [-a|-c|-e|-f|-j|-n|-p|-r|-t|-u|-w|-E|-H|-S|-T|...]    * users [-a|-f|-n|-p|-r|-u|-w|-E|-H|-S|...]</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#options","title":"OPTIONS","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-a-allocation","title":"-a --allocation <p>enter allocation id</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-c-comment","title":"-c --comment <p>enter comment for new or edit commands, display comment for list commands</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-e-event-id","title":"-e --event-id <p>enter event db id; event db id is an internal id created by the charging system</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-f-field","title":"-f --field <p>enter [:], width is optional; enter -f? or -f \"?\" for available fields, + to add fields","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-h-help","title":"-h --help <p>command line help</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-j-jobid","title":"-j --jobid <p>enter jobid; jobid is created by the scheduler and is not unique</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-n-num-field","title":"-n --num-field <p>enter number of fields to display</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-p-project","title":"-p --project <p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-r-resource","title":"-r --resource <p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-s-suballocation","title":"-s --suballocation <p>enter suballocation id</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-t-transaction","title":"-t --transaction <p>enter transaction id</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-u-user","title":"-u --user <p>enter name or id, DO NOT MIX, enter 'all' to get all, wild cards '*' is allowed, but only on names</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-w-field-width","title":"-w --field-width <p>enter the field width as follows: :, enter -w? or -w \"?\" for available fields","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-e-end","title":"-E --end <p>enter end datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-h-human-readable","title":"-H --human-readable <p>abbreviate numbers and use unit suffixes: K (thousands), M (millions), G (billions), T (trillions) ...</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-i-get-inactive","title":"-I --get-inactive <p>include inactive allocations</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-o-get-only-inactive","title":"-O --get-only-inactive <p>get only inactive allocations</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-s-start","title":"-S --start <p>enter start datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-t-type","title":"-T --Type <p>enter type of transaction</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-all-charges","title":"--all-charges <p>for list allocations | projects | users, only show info with charges</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-at","title":"--at <p>enter transaction-created datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-award-category","title":"--award-category <p>enter allocation award category</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-award-type-name","title":"--award-type-name <p>enter allocation award-type name</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-created","title":"--created <p>enter created datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-debug","title":"--debug <p>enter debug level</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-get-deleted","title":"--get-deleted <p>get deleted objects</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-get-not-charged","title":"--get-not-charged <p>get jobs that have not been charged</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-get-only-deleted","title":"--get-only-deleted <p>get only deleted objects</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-history-date-range","title":"--history-date-range <p>enter history datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-home-dir","title":"--home-dir <p>enter the directory to store the pbs meta file</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-ignore-pbs-files","title":"--ignore-pbs-files <p>all new pbs files will be ignored and marked as processed</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-last-updated","title":"--last-updated <p>enter last updated datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-no-commas","title":"--no-commas <p>remove commas from comma-separated thousands</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-no-header","title":"--no-header <p>do not display header</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-no-history","title":"--no-history <p>do not display history information</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-no-rows","title":"--no-rows <p>do not display rows</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-no-sys-msg","title":"--no-sys-msg <p>do not display system message</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-no-totals","title":"--no-totals <p>do not display totals</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#-queued","title":"--queued <p>enter queued datetime filter</p>","text":""},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#more-option-explanations","title":"MORE OPTION EXPLANATIONS","text":"<p>For -a, -e, -f, -w, -j, -p, -r, -t, -u, -T, --award-categories, --award_type_names, --cbank_refs options:</p> <p>These options can be entered multiple times for different values or entered once for multiple values. </p> <p>Examples: </p> <ol> <li> <p>sbank-list-allocation -u \"pershey rojas allcock\" or &gt; sbank-list-allocation -u pershey -u rojas -u allcock </p> </li> <li> <p>sbank-list-allocation -f \"id p avail\" or &gt; sbank-list-allocation -f id -f p -f avail For -u, -p and -r the use of wild card \"*\" is allowed, but only on names, not ids: </p> </li> </ol> <p>Examples: </p> <ol> <li>The following command will find allocations for users whose names start with \"pers\" and also users rojas and allcock. &gt; sbank-list-allocation -u \"pers* rojas allcock\" </li> <li>The following command will find allocations for projects that contain \"ratio\" in the name. &gt; sbank-list-allocation -p ratio </li> <li>The following command will find allocations for projects that end with \"tion\" in the name. &gt; sbank-list-allocation -p *tion </li> <li>The following command will find allocations for projects that start with \"ab\" and end with \"ng\" in the name. &gt; sbank-list-allocation -p ab*ng</li> </ol> <p>For -f option: This option is the display field option. </p> <p>To get the available fields enter -f? or -f \"?\". Default fields columns will be displayed if no field option is specified. </p> <p>To replace the current fields to display, enter:  <pre><code>&gt; sbank-list-allocations ... -f \"FIELD[:WIDTH]...FIELD[:WIDTH]\" or &gt; sbank-list-allocations ... -f FIELD[:WIDTH] ... -f FIELD[:WIDTH] \n</code></pre></p> <p>If you wish to add fields to the default fields, enter one + symbol anywhere in the quoted string:  <pre><code>&gt; sbank-list-allocations ... -f \"+ FIELD[:WIDTH]...FIELD[:WIDTH]\", only one + symbol is needed.\n</code></pre></p> <p>The fields will be displayed in table format and in the order entered in the command line. You can specify the field width, where WIDTH can be positive or negative value. Left alignment use -, right alignment use + or nothing.</p> <p>For -w option:</p> <p>FIELD:WIDTH, if the field is displayed it will change the width for the specified field. </p> <p>NOTE: This will not add the field as in -f option, only change the width. To get available fields you can also use -w? or -w \"?\" as in -f option.</p> <p>For -S, -E, --created, --queued, --last-updated, --history-date-range options:</p> <p>These are the date filter options. All dates are treated as UTC. </p> <p>You can use any reasonable date string that resembles a date Ambiguous dates will be parsed with the following parsing precedence: **YEAR then MONTH then DAY **</p> <p>For example, 10-11-12 or 101112 will be the following date: Oct. 11, 2012 Not: Nov. 12, 2010 or Nov. 10, 2012 </p> <p>Or you can specify a single date as follows:  <pre><code>\"[OPER]UTC_DATE\" You can specify a date range as follows: \n\"[OPER1]UTC_DATE1...[OPER2]UTC_DATE2\" Where OPER can be one of the following operators: \"==\", \"&gt;=\", \"&lt;=\", \"&gt;\", \"&lt;\" or \"eq\", \"ge\", \"le\", \"gt\", \"lt\" \n</code></pre></p> <p>Note: The following defaults for OPER, OPER1, OPER2 for the following options:  <pre><code>Options OPER OPER1 OPER2 ------------------------- ---- ----- ----- -E, &lt; &gt;= &lt; -S, &gt;= &gt;= &lt; --at &gt;= &gt;= &lt; --created &gt;= &gt;= &lt; --eligible &gt;= &gt;= &lt; --last-updated &gt;= &gt;= &lt; --queued &gt;= &gt;= &lt; \n</code></pre></p> <p>You can also use the following key letters \"n\", \"t\", \"d\", \"w\", \"y\" as follows:  <pre><code>KEY SYNTAX DEFINITIONS ---------- ----------- n[ow] now, where \"now\" is current-date current-time UTC t[oday] today, where \"today\" is current-date 00:00:00 UTC [+/-]d specified \"number\" of +/- days from \"today\" in UTC [+/-]w specified \"number\" of +/- weeks from \"today\" in UTC [+/-]y specified \"number\" of +/- years from \"today\" in UTC\n</code></pre></p> <p>For -T option:</p> <p>Transaction type option. The following are the valid transaction types and their explanation: CHARGE filter on job charges PULLBACK filter on allocation pullbacks DEPOSIT filter on allocation deposits REFUND filter on job refunds VOID filter on void transactions</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#invocation","title":"INVOCATION","text":"<p>sbank sbank sbank sbank-detail sbank detail sbank d sbank-detail-allocations sbank detail allocations sbank d a sbank-detail-jobs sbank detail jobs sbank d j sbank-detail-projects sbank detail project sbank d p sbank-detail-transactions sbank detail transactions sbank d t sbank-detail-users sbank detail users sbank d u sbank-list sbank list sbank l sbank-list-allocations sbank list allocations sbank l a sbank-list-jobs sbank list jobs sbank l j sbank-list-projects sbank list projects sbank l p sbank-list-transactions sbank list transactions sbank l t sbank-list-users sbank list users sbank l u</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#environment-variables","title":"ENVIRONMENT VARIABLES","text":"<p>Command line default options: Define the following environment variables as you would in the command line. Once the environment variable is defined, it will be used as the default options and arguments for the specific command. Command line options will take precedence.</p> <p>sbank_DETAIL_ALLOCATIONS_ARGS</p> <p>Default arguments and options for sbank-detail-allocations.</p> <p>sbank_DETAIL_CATEGORIES_ARGS</p> <p>Default arguments and options for sbank-detail-categories.</p> <p>sbank_DETAIL_NAMES_ARGS</p> <p>Default arguments and options for sbank-detail-names.</p> <p>sbank_DETAIL_MESSAGES_ARGS</p> <p>Default arguments and options for sbank-detail-messages.</p> <p>sbank_DETAIL_JOBS_ARGS</p> <p>Default arguments and options for sbank-detail-jobs.</p> <p>sbank_DETAIL_PROJECTS_ARGS</p> <p>Default arguments and options for sbank-detail-projects.</p> <p>sbank_DETAIL_TRANSACTIONS_ARGS</p> <p>Default arguments and options for sbank-detail-transactions.</p> <p>sbank_DETAIL_USERS_ARGS</p> <p>Default arguments and options for sbank-detail-users.</p> <p>sbank_LIST_ALLOCATIONS_ARGS</p> <p>Default arguments and options for sbank-list-allocations.</p> <p>sbank_LIST_JOBS_ARGS</p> <p>Default arguments and options for sbank-list-jobs.</p> <p>sbank_LIST_PROJECTS_ARGS</p> <p>Default arguments and options for sbank-list-projects.</p> <p>sbank_LIST_TRANSACTIONS_ARGS</p> <p>Default arguments and options for sbank-list-transactions.</p> <p>sbank_LIST_USERS_ARGS</p> <p>Default arguments and options for sbank-list-users.</p>"},{"location":"account-project-management/allocation-management/not_in_nav/sbank-manpage/#examples","title":"EXAMPLES <p>Example 1: -f, --field <pre><code>&gt; sbank-list-transactions ... -f field1:-20 -f field2:20 -f field3 or &gt; sbank-list-transactions ... -f \"field1:-20 field2:20 field3\" \n</code></pre> Explanation: Fields will be displayed in order of appearance, where field1:-20 means 20 characters long, left align; where field2:20 means 20 characters long, right align; where field3 uses default sizes. Number fields default to right aligned. Text fields default to left aligned.</p> <p>Example 2: -S, -E, --created, --queued, --last-updated, --history-start, --history-end</p> <p>Single date-string examples: </p> <ul> <li>  <p>sbank-list-allocations -S \"&gt;=Oct 11, 2014\" start dates that are &gt;= \"2014-10-11 00:00:00\" </p>  </li> <li>  <p>sbank-list-allocations -S \"&lt;=2014-11-10\" start dates that are &lt;= \"2014-11-10 00:00:00\" </p>  </li> <li>  <p>sbank-list-allocations -E \"&lt;20141110\" end dates that are &lt; \"2014-11-10 00:00:00\" </p>  </li> <li>  <p>sbank-list-allocations -E \"22:30:10\" end dates that are &lt; \" 22:30:10\"    <li>  <p>sbank-list-allocations -S \"&gt;today\" start dates that are &gt; \" 00:00:00\"    <li>  <p>sbank-list-allocations -E t end dates that are &lt; \" 00:00:00\"    <li>  <p>sbank-list-allocations -S gtnow start dates that are &gt; \" \"    <li>  <p>sbank-list-allocations -E len end dates that are &lt;= \" \"    <li>  <p>sbank-list-allocations -S \"1d\" start dates that are &gt;= \"today +1 day\" </p>  </li> <li>  <p>sbank-list-allocations -E \"-2w\" end dates that are &lt; \"today -2 weeks\" </p>  </li> <li>  <p>sbank-list-allocations -S \"&gt;=1y\" start dates that are &gt;= \"today +1 year\" </p>  </li> <li>  <p>sbank-list-allocations -S \"&gt;2012\" start dates that are &gt; \"2012-- 00:00:00\"     <p>Range date-string examples: </p> <ul> <li>  <p>sbank-list-allocations -S \"2013-01-01...2014-01-01\" \"2013-01-01\" &lt;= DATES &lt; \"2014-01-01\" </p>  </li> <li>  <p>sbank-list-allocations -S \"-1y...t\" \"today -1 year\" &lt;= DATES &lt; \"today\" </p>  </li> <li>  <p>sbank-list-allocations -E \"2013...t\"\" \"2013--\" &lt;= DATES &lt; \"today\"    <li>  <p>sbank-list-allocations -E \"&gt;2013...&lt;=t\"\" \"2013--\" &lt; DATES &lt;= \"today\"    <p>Example 3: Command invocation examples</p> <ul> <li>  <p>sbank-list-projects list projects full command invocation </p>  </li> <li>  <p>sbank list projects list projects meta command invocation</p>  </li> <li>  <p>sbank s p list projects partial meta command invocation </p>  </li> <li>  <p>sbank p list projects where \"list\" is the default</p>  </li> <li>  <p>sbank list allocations is the default </p>  </li> <li>  <p>sbank a list allocations \"list\" is the default </p>  </li> <li>  <p>sbank s a list allocations partial meta command invocation</p>  </li> </ul> <p>Example 4: -h, --help</p> <ul> <li>  <p>sbank -h will give you help summary on all of sbank </p>  </li> <li>  <p>sbank list --help will give you help on all the \"list\" commands </p>  </li> <li>  <p>sbank list allocations -h will give you help on the \"list allocations\" command</p>  </li> <li>  <p>sbank-list-allocations -h will give you help on the \"list allocations\" command </p>  </li> <li>  <p>sbank l a --help will give you help on the \"list allocations\" command</p>  </li> </ul>","text":""},{"location":"account-project-management/project-management/project-reports/","title":"Quarterly and Year-End Reporting","text":"<p>The Argonne Leadership Computing Facility (ALCF) is required to report the progress and scientific accomplishments of all peer-reviwed projects. </p> <p>PIs of INCITE, ALCC, and ADSP projects are required to complete quarterly reports and a final end-of-project (EOY/EOP) report.</p>"},{"location":"account-project-management/project-management/project-reports/#due-dates","title":"Due dates","text":""},{"location":"account-project-management/project-management/project-reports/#due-dates-for-the-2024-incite-quarterly-eoy-and-the-eop-reports","title":"Due dates for the 2024 INCITE quarterly, EOY, and the EOP reports:","text":"<ul> <li>April 1, 2024 (CY2024 - Q1)</li> <li>July 1, 2024 (CY2024 - Q2)</li> <li>October 1, 2024 (CY2024 - Q3)</li> <li>January 1, 2025 (CY2025 - EOY) or February 15, 2025 (entire allocation period - EOP)</li> </ul>"},{"location":"account-project-management/project-management/project-reports/#due-dates-for-the-2023-2024-alcc-quarterly-and-the-eop-reports","title":"Due dates for the 2023-2024 ALCC quarterly and the EOP reports:","text":"<ul> <li>October 1, 2023 (CY2023 - Q3)</li> <li>January 1, 2024 (CY2024 - Q4)</li> <li>April 1, 2024 (CY2024 - Q1)</li> <li>August 15, 2024 (CY2024 - EOP)</li> </ul>"},{"location":"account-project-management/project-management/project-reports/#penalties","title":"Penalties","text":"<p>If a quarterly report is more than 30 days late: - The ability to submit jobs for the PI and users of the late project will be disabled.</p> <p>If a quarterly report is more than 90 days late: - The PI and users of the late project will have their accounts disabled.</p> <p>These penalties will be removed within three business days after the late quarterly or EOY report is submitted.</p>"},{"location":"account-project-management/project-management/project-reports/#alcc-specific-penalties","title":"ALCC Specific Penalties:","text":"<p>A similar penalty will also be applied to new ALCC projects with the same PI or co-PIs that have failed to submit the EOP report for a previous ALCC project. If the EOP report is more than 15 days late:</p> <ul> <li>The new ALCC project will be blocked. For a currently active ALCC project, the ability to submit jobs will be disabled for the project and all sub-projects. For a project that has not been created yet, the process for new project creation will be halted.</li> </ul>"},{"location":"account-project-management/project-management/project-reports/#appeals","title":"Appeals","text":"<p>A PI or user may appeal a project or account suspension to the ALCF Director by a request to support at alcf.anl.gov.</p>"},{"location":"account-project-management/project-management/project-reports/#report-templates","title":"Report Templates","text":"<p>Templates for the quarterly and the EOY reports can be found at the links on the bottom of this page.</p> <p>Please modify the filename to replace PINAME with the last name of the PI of the INCITE/ALCC project, ALLOCATION to INCITE/ALCC, and YEAR to the corresponding calendar year.  For quarterly reports, please replace the X in the filename with the quarter number.</p> <p>For example, for a project with PI 'Joe Smith' that is submitting the quarterly report for the first quarter in 2023-2024 cycle for ALCC, the filename will be Smith_ALCC_Q1.docx.</p> <p>For an EOY report, replace YEARS with the years associated with your allocation. For example, an ALCC 2023-2024 project with PI 'Joe Smith' would have a filename of Smith_ALCC_2023-2024_EOY.docx.</p>"},{"location":"account-project-management/project-management/project-reports/#templates-for-incite-and-alcc","title":"Templates for INCITE and ALCC:","text":"<ul> <li>Quarterly Report Template</li> <li>End of Project Report Template</li> <li>End of Year Report Template</li> </ul>"},{"location":"account-project-management/project-management/starting-alcf-award/","title":"Starting Your ALCF Award","text":"<p>The following guide is for PIs and Proxies to get insight into managing projects and teams for ALCF awards. Please submit for questions or trouble tickets to support@alcf.anl.gov.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#get-started-with-alcfs-systems","title":"Get Started with ALCF\u2019s Systems","text":"<p>To get started using our resources, please visit:  Connect &amp; Login</p> <p>We also encourage you to take full advantage of ALCF's training programs and user services. Some useful introductory materials and videos are listed below:</p> <ul> <li>Running on Polaris </li> <li>Lustre File Striping Basics</li> <li>Community Data Sharing with ACDC (using Eagle)</li> </ul>"},{"location":"account-project-management/project-management/starting-alcf-award/#project-terminology","title":"Project Terminology","text":"<p>Before your project begins, you will receive an email with the following project information:</p> <ul> <li>Project Short Name: The assigned, shortened name for your project. This will be the name that you\u2019ll use to access your project on the systems.</li> <li>Project Proxies: Project members designated by PIs that are authorized to add or renew project members on your behalf.</li> <li>Allocation System(s) and Allocation Amount: The approved system(s) and amount of your award in node hours.</li> <li>Approved Quota: The approved amount of disk space for your project directory.</li> <li>File System: The file system where your project directory will reside. For information on the Grand and Eagle file systems, see Storage and Networking.</li> <li>Assigned Catalyst: INCITE projects will have ALCF staff members that are assigned to the projects who are available to assist the team throughout the duration of the INCITE allocation.</li> <li>Allocation Start Date: The start date of your award.</li> <li>Allocation End Date: The end date of your award.</li> </ul>"},{"location":"account-project-management/project-management/starting-alcf-award/#account-setup","title":"Account Setup","text":"<p>If you do not have an ALCF account: You will need to request one at https://accounts.alcf.anl.gov/accountRequest. When prompted for project name, please select the project short name you were given in your award email from support@alcf.anl.gov.</p> <p>If you have an active ALCF account: Submit a request to join the newly awarded project at https://accounts.alcf.anl.gov/#!/joinProject.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#information-for-foreign-national-access","title":"Information for Foreign National Access","text":"<p>The U.S. Department of Energy has guidelines and requirements for foreign nationals who access its facilities and sites. This guidance is issued in DOE Order 142.3, which is part of Argonne's contract; therefore, all foreign nationals (non-U.S. Citizens) must obtain authorization prior to using ALCF resources.</p> <p>If you are a foreign national and do not have current authorization credentials, you are required to submit a ANL-593 (Foreign National Access Request) form. It is critical that identity documentation requests sent by ALCF staff are completed as early as possible to facilitate timely processing for your account approval.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#user-agreement-for-incite-alcc-and-adsp","title":"User Agreement for INCITE, ALCC, and ADSP","text":"<p>Note: This does not apply to Director's Discretionary awards.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#insitution-master-agreement-for-incite-alcc-and-adsp","title":"Insitution Master Agreement for INCITE, ALCC, and ADSP","text":"<p>If you are not an employee of Argonne National Laboratory, a user agreement must be signed by your home institution to perform research at Argonne\u2019s user facilities. This policy applies to every member of the project team who will be conducting research on ALCF resources.</p> <p>A list of home institutions that have master agreements in place is located on this webpage: https://www.aps.anl.gov/Users-Information/Legal-Financial/Argonne-User-Facility-Agreements</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#alcf-user-agreement-for-incite-alcc-and-adsp","title":"ALCF User Agreement for INCITE, ALCC, and ADSP","text":"<p>Note: This does not apply to Director's Discretionary awards.</p> <p>Every project team member who requests an ALCF account must sign and return an acknowledgment form, stating that they agree to the terms in the user agreement.</p> <p>The form is located at: https://www.alcf.anl.gov/files/Acknowledgement_Form.pdf. Please print, sign, scan and email it to accounts@alcf.anl.gov.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#managing-project-team-membership","title":"Managing Project Team Membership","text":"<p>As a PI, you can add members to your project. You can assign proxies who are project members authorized to add or renew project members on your behalf.</p> <p>A project PI or proxy has the authority to:</p> <ul> <li>Approve and renew accounts</li> <li>Add and delete users to/from the project</li> <li>Approve Foreign Assignment/Visit Request form renewals for project members who are foreign nationals</li> </ul> <p>During your project setup, the ALCF Support Team will request the following information to establish your project members:</p> <ul> <li>The names, email addresses, and/or ALCF usernames (if already existing) of up to two proxies and all project members.</li> </ul>"},{"location":"account-project-management/project-management/starting-alcf-award/#about-project-and-unix-group-membership","title":"About Project and UNIX Group Membership","text":"<p>All project members have the ability to run jobs against your allocation. There is no limit to the number of project members you may authorize. Project members are automatically added to the project UNIX group giving them the ability to write to the project directory and to access project data. When a project member is added or removed from a project, this automatically be reflected in the project UNIX group membership. </p>"},{"location":"account-project-management/project-management/starting-alcf-award/#adding-project-members","title":"Adding Project Members","text":"<p>The PI or a proxy must approve each team member to access ALCF resources and run jobs on their project. PI/proxies can respond to emails from ALCF for account access approval with a \"yes\" or \"no\".</p> <p>PI/proxies with active ALCF accounts can also approve new account requests, project membership requests, account reactivation requests, add existing active ALCF users to the project by logging into the ALCF Account and Project Management application.  </p> <p>Note: If PI/proxies need to request an ALCF account, see the section below for instructions on \"how to apply\" for an account.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#accounts-and-access-for-your-project-members","title":"Accounts and Access for your Project Members","text":"<p>All project members will need an ALCF user account to access project data and to run jobs on ALCF systems.</p> <p>Members that do not have an ALCF account should request one at: https://accounts.alcf.anl.gov/#/accountRequest. When prompted for project name, they should select your project short name.</p> <p>Members with ALCF accounts that are no longer active should submit a reactivation request here: https://accounts.alcf.anl.gov/#/accountReactivate. When prompted for project name, they should select your project short name.</p> <p>Members with active ALCF accounts but have not been added to your project should submit a request to join your project by going to this page: https://accounts.alcf.anl.gov/#!/joinProject. They should search for your project and click the \"Request Membership\" button for that project.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#moving-your-data","title":"Moving Your Data","text":"<p>We encourage you to use Globus to move your project data to your ALCF project directory before your allocation begins. For details, see Using Globus.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#project-status-reports-for-incite-alcc-and-adsp","title":"Project Status Reports for INCITE, ALCC, and ADSP","text":"<p>Note: PIs that are awarded a Director's Discretionary will not receive weekly status project reports.</p> <p>Shortly after your allocation begins, we will begin sending you a weekly project status report via support@alcf.anl.gov to keep you informed or your award progress.</p> <p>Look for an email from us with the subject line: ALCF [ALLOCATION PROGRAM] Project Status Report for [PROJECT SHORT NAME]</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#reporting-requirements-for-incite-alcc-and-adsp","title":"Reporting Requirements for INCITE, ALCC, and ADSP","text":"<p>Note: PIs that are awarded a Director's Discretionary allocations are not required to submit project reports.</p> <p>If you receieved INCITE, ALCC, or ADSP allocation award, quarterly reporting is required to keep DOE informed of progress related to your allocation. </p> <p>The ALCF will send you a report template at the end of each quarter. Please complete the report promptly and submit it via email to support@alcf.anl.gov. For more information see the Quarterly Report webpage.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#policies","title":"Policies","text":""},{"location":"account-project-management/project-management/starting-alcf-award/#pullback-policy","title":"Pullback Policy","text":"<p>Please be aware that we will periodically monitor, and could potentially adjust, your project allocation if a large portion of it goes unused. You may view: Pullback Policy</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#allocation-overburn-policy","title":"Allocation Overburn Policy","text":"<p>Please see this page for overburn/overuse eligibility for INCITE projects that have exhausted their allocation in the first 11 months of its allocation year: Allocation Overburn</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#acknowledgment-in-publications","title":"Acknowledgment In Publications","text":"<p>Please follow the guidelines provided on the ALCF Acknowledgement Policy page to properly acknowledge the use of ALCF resources in all of your publications, both online and print.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#facility-policies","title":"Facility Policies","text":"<p>Facility policies have been established to provide consistent and reliable services. Please read about our [ALCF Facility Policies] (../policies/facility-policies.md).</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#useful-allocation-and-quota-commands","title":"Useful Allocation and Quota Commands","text":"<p>We have an allocation management tool called sbank, and below are a few helpful sbank commands.  </p> <ul> <li>myprojectquotas: log into Polaris and type this command to view the project directory quotas for all your projects</li> <li>myquota: log into Polaris and type this command to view your home directory quota</li> </ul> <p>You can use the following command to check your project balance on Polaris: - sbank-list-allocations -p  -r  <p>For more command examples and details, see sbank.</p>"},{"location":"account-project-management/project-management/starting-alcf-award/#how-can-we-help","title":"How Can We Help?","text":"<p>We can also help resolve any issues or needs that may be delaying the start of your scientific campaign. - Are you in need of high-throughput software? - Are you having difficulty compiling your application? - Does your code have limited restart capabilities?</p> <p>If your project allocation usage is being held back for reasons due to one of our systems, please contact us for assistance by emailing support@alcf.anl.gov.</p>"},{"location":"account-project-management/project-management/team-management/","title":"Managing Your Team Members","text":"<p>The PI or Proxy must approve each member of the team to gain access and to run project jobs on the ALCF's resources. If you have an active ALCF account, you can manage your project team by logging into the ALCF account and project management website and navigating to https://accounts.alcf.anl.gov/#/manageProjects</p> <p>Project members will need to have an active ALCF user account to access project data and to run jobs on ALCF systems. See [Accounts and Access for your Project Members]{https://docs.alcf.anl.gov/account-project-management/project-management/starting-alcf-award/#accounts-and-access-for-your-project-members) for information on how team members can get an account, reactivate an account, or request to join your project.</p>"},{"location":"account-project-management/project-management/team-management/#accessing-your-projects","title":"Accessing your project(s)","text":"<ol> <li>Log in at https://accounts.alcf.anl.gov/#/manageProjects using your credentials (ALCF username and Physical/Mobile token passcode one-time passcode).</li> <li>You will see a list of projects of which you are the Primary Investigator (PI).</li> <li>Click on the desired project to view information and management options for the selected project.</li> </ol>"},{"location":"account-project-management/project-management/team-management/#modifying-project-information","title":"Modifying project information","text":"<p>Some project information cannot be modified, but as the PI, you can modify the following: project title, institutions, and associated funding.</p> <p>Your project can be associated with multiple institutions, but you must specify a primary institution.</p>"},{"location":"account-project-management/project-management/team-management/#managing-project-members-with-an-existing-alcf-account","title":"Managing project members with an Existing ALCF Account","text":"<ol> <li>You can manage the membership for your project by clicking on the desired project from the Project Management screen.</li> <li>Add and/or remove proxies and team members by clicking on the red \"Remove\" button to the right of each member or clicking on \"Add new user.\"</li> <li>You can view account information for each user as it relates to the project:</li> <li>Account Status</li> <li>Project Role</li> <li>Proxy Permissions</li> <li> <p>Membership Status</p> </li> <li> <p>Proxies are individuals authorized to add or renew user accounts for the project PI. You have the ability to upgrade a user from a member to a Proxy, by clicking on the \"Proxy\" radio button that corresponds with the desired member.</p> </li> </ol>"},{"location":"ai-testbed/getting-started/","title":"ALCF AI Testbed","text":"<p>The ALCF AI Testbed houses some of the most advanced AI accelerators for scientific research. </p> <p>The goal of the testbed is to enable explorations into next-generation machine learning applications and workloads, enabling the ALCF and its user community to help define the role of AI accelerators in scientific computing and how to best integrate such technologies with supercomputing resources.</p> <p>The AI accelerators complement the ALCF's current and next-generation supercomputers to provide a state-of-the-art computing environment that supports pioneering research at the intersection of AI, big data, and high performance computing (HPC). </p> <p>The platforms are equipped with architectural features that support AI and data-centric workloads, making them well suited for research tasks involving the growing deluge of scientific data produced by powerful tools, such as supercomputers, light sources, telescopes, particle accelerators, and sensors. In addition, the testbed will allow researchers to explore novel workflows that combine AI methods with simulation and experimental science to accelerate the pace of discovery.</p>"},{"location":"ai-testbed/getting-started/#how-to-get-access","title":"How to Get Access","text":"<p>Researchers interested in using the AI Testbed\u2019s <code>Cerebras CS-2</code>, <code>SambaNova DataScale SN30</code>, <code>Graphcore Bow Pod64</code> and <code>GroqRack</code> platforms can now submit project proposals via the ALCF\u2019s Director\u2019s Discretionary program. Access to additional testbed resources, including <code>Habana</code> accelerators, will be announced at a later date. </p> <p>Submit your proposal requests at: Allocation Request Page</p>"},{"location":"ai-testbed/getting-started/#getting-started","title":"Getting Started","text":"<ol> <li> <p>Request a Director's Discretionary project on SambaNova/Cerebras/Graphcore/Groq.</p> </li> <li> <p>Apply for an ALCF account after the project request is approved. Choose the SambaNova/Cerebras/Graphcore/Groq project that your PI has created at ALCF. If you have an active ALCF account, request to join the project after your project is approved.</p> </li> <li> <p>Transfer data to ALCF using Globus after your account has been created.</p> <p>a. The endpoint for your data in ALCF is <code>alcf#ai_testbed_projects</code> with the path to your project being  <code>/&lt;project name&gt;</code>. </p> <p>b. The endpoint for your home directory on the AI Testbeds in ALCF is <code>alcf#ai_testbed_home</code>.</p> </li> <li> <p>Add/invite team members to your ALCF project on SambaNova/Cerebras/Graphcore/Groq. </p> </li> </ol>"},{"location":"ai-testbed/getting-started/#how-to-contribute-to-documentation","title":"How to Contribute to Documentation","text":"<p>The documentation is based on MkDocs and source files are on GitHub. You can contribute to the documentation by creating a pull request. </p> <p>Learn more on how to contribute to documentation.</p>"},{"location":"ai-testbed/cerebras/customizing-environment/","title":"Customizing Environments","text":""},{"location":"ai-testbed/cerebras/customizing-environment/#using-virtual-python-environments","title":"Using virtual Python environments","text":""},{"location":"ai-testbed/cerebras/customizing-environment/#to-make-a-pytorch-virtual-environment-for-cerebras","title":"To make a PyTorch virtual environment for Cerebras","text":"<pre><code>mkdir ~/R_2.2.0\ncd ~/R_2.2.0\n# Note: \"deactivate\" does not actually work in scripts.\ndeactivate\nrm -r venv_cerebras_pt\n/software/cerebras/python3.8/bin/python3.8 -m venv venv_cerebras_pt\nsource venv_cerebras_pt/bin/activate\npip install --upgrade pip\npip install cerebras_pytorch==2.2.0\n# Patch to fix a mount problem; a future release will not need this.\ncp /software/cerebras/venvs/R_2_2.0_patch/appliance_manager.py ~/R_2.2.0/venv_cerebras_pt/lib/python3.8/site-packages/cerebras/appliance/appliance_manager.py\n</code></pre>"},{"location":"ai-testbed/cerebras/customizing-environment/#activation-and-deactivation","title":"Activation and deactivation","text":"<p>To activate a virtual environments</p> <pre><code>source ~/R_2.2.0/venv_cerebras_pt/bin/activate\n</code></pre> <p>To deactivate a virtual environment,</p> <pre><code>deactivate\n</code></pre>"},{"location":"ai-testbed/cerebras/example-programs/","title":"Example Programs","text":""},{"location":"ai-testbed/cerebras/example-programs/#use-a-local-copy-of-the-model-zoo","title":"Use a local copy of the model zoo","text":"<p>Make a working directory and a local copy of the Cerebras modelzoo and anl_shared repository, if not previously done, as follows.</p> <pre><code>mkdir ~/R_2.2.0\ncd ~/R_2.2.0\ngit clone https://github.com/Cerebras/modelzoo.git\ncd modelzoo\ngit tag\ngit checkout Release_2.2.0\n</code></pre>"},{"location":"ai-testbed/cerebras/example-programs/#bert-pytorch","title":"BERT - PyTorch","text":"<p>The modelzoo/modelzoo/transformers/pytorch/bert directory is a PyTorch implementation of BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding This BERT-large msl128 example uses a single sample dataset for both training and evaluation. See the README.md in the source directory for details on how to build a dataset from text input. First, source a Cerebras PyTorch virtual environment and make sure that the requirements are installed:</p> <pre><code>source ~/R_2.2.0/venv_cerebras_pt/bin/activate\npip install -r ~/R_2.2.0/modelzoo/requirements.txt\n</code></pre> <p>Then</p> <p><pre><code>cd ~/R_2.2.0/modelzoo/src/cerebras/modelzoo/models/nlp/bert\ncp /software/cerebras/dataset/bert_large/bert_large_MSL128_sampleds.yaml configs/bert_large_MSL128_sampleds.yaml\nexport MODEL_DIR=model_dir_bert_large_pytorch\nif [ -d \"$MODEL_DIR\" ]; then rm -Rf $MODEL_DIR; fi\npython run.py CSX --job_labels name=bert_pt --params configs/bert_large_MSL128_sampleds.yaml --num_workers_per_csx=1 --mode train --model_dir $MODEL_DIR --mount_dirs /home/ /software/ --python_paths /home/$(whoami)/R_2.2.0/modelzoo/src --compile_dir $(whoami) |&amp; tee mytest.log\n</code></pre> Note: the vocabulary file referenced in <code>/software/cerebras/dataset/bert_large/bert_large_MSL128_sampleds.yaml</code> is the same as the one at <code>/home/$(whoami)/R_2.2.0/modelzoo/modelzoo/transformers/vocab/google_research_uncased_L-12_H-768_A-12.txt</code>. </p> <p>The last parts of the output should resemble the following, with messages about cuda that should be ignored and are not shown.</p> <pre><code>2023-11-29 20:07:49,284 INFO:   Beginning appliance run\n2023-11-29 20:08:14,365 INFO:   | Train Device=CSX, Step=100, Loss=9.50000, Rate=4088.28 samples/sec, GlobalRate=4088.26 samples/sec\n2023-11-29 20:08:39,820 INFO:   | Train Device=CSX, Step=200, Loss=8.37500, Rate=4048.91 samples/sec, GlobalRate=4055.21 samples/sec\n2023-11-29 20:09:05,356 INFO:   | Train Device=CSX, Step=300, Loss=7.96875, Rate=4025.61 samples/sec, GlobalRate=4040.05 samples/sec\n2023-11-29 20:09:30,626 INFO:   | Train Device=CSX, Step=400, Loss=7.56250, Rate=4041.61 samples/sec, GlobalRate=4043.10 samples/sec\n2023-11-29 20:09:56,022 INFO:   | Train Device=CSX, Step=500, Loss=7.50000, Rate=4035.92 samples/sec, GlobalRate=4040.90 samples/sec\n2023-11-29 20:10:21,410 INFO:   | Train Device=CSX, Step=600, Loss=7.37500, Rate=4034.41 samples/sec, GlobalRate=4039.65 samples/sec\n2023-11-29 20:10:46,690 INFO:   | Train Device=CSX, Step=700, Loss=7.37500, Rate=4044.10 samples/sec, GlobalRate=4041.20 samples/sec\n2023-11-29 20:11:12,004 INFO:   | Train Device=CSX, Step=800, Loss=7.25000, Rate=4044.75 samples/sec, GlobalRate=4041.70 samples/sec\n2023-11-29 20:11:37,196 INFO:   | Train Device=CSX, Step=900, Loss=7.21875, Rate=4056.77 samples/sec, GlobalRate=4044.25 samples/sec\n2023-11-29 20:12:02,285 INFO:   | Train Device=CSX, Step=1000, Loss=7.12500, Rate=4071.60 samples/sec, GlobalRate=4047.95 samples/sec\n2023-11-29 20:12:02,286 INFO:   Saving checkpoint at step 1000\n2023-11-29 20:12:37,079 INFO:   Saved checkpoint model_dir_bert_large_pytorch/checkpoint_1000.mdl\n2023-11-29 20:13:25,683 INFO:   Heartbeat thread stopped for wsjob-gfi2baioyfduozkmgsc6a7.\n2023-11-29 20:13:25,691 INFO:   Training completed successfully!\n2023-11-29 20:13:25,691 INFO:   Processed 1024000 sample(s) in 336.373620536 seconds.\n</code></pre>"},{"location":"ai-testbed/cerebras/example-programs/#gpt-j-pytorch","title":"GPT-J PyTorch","text":"<p>GPT-J [github] is an auto-regressive language model created by EleutherAI. This PyTorch GPT-J 6B parameter pretraining sample uses 2 CS2s.</p> <p>First, source a Cerebras PyTorch virtual environment and make sure that the requirements are installed:</p> <pre><code>source ~/R_2.2.0/venv_cerebras_pt/bin/activate\npip install -r ~/R_2.2.0/modelzoo/requirements.txt\n</code></pre> <p>Then</p> <pre><code>cd ~/R_2.2.0/modelzoo/src/cerebras/modelzoo/models/nlp/gptj\ncp /software/cerebras/dataset/gptj/params_gptj_6B_sampleds.yaml configs/params_gptj_6B_sampleds.yaml\nexport MODEL_DIR=model_dir_gptj\nif [ -d \"$MODEL_DIR\" ]; then rm -Rf $MODEL_DIR; fi\npython run.py CSX --job_labels name=gptj_pt --params configs/params_gptj_6B_sampleds.yaml --num_csx=2 --mode train --model_dir $MODEL_DIR --mount_dirs /home/ /software --python_paths /home/$(whoami)/R_2.2.0/modelzoo/src --compile_dir $(whoami) |&amp; tee mytest.log\n</code></pre> <p>The last parts of the output should resemble the following:</p> <pre><code>2023-11-29 20:59:19,223 INFO:   Beginning appliance run\n2023-11-29 21:03:53,875 INFO:   | Train Device=CSX, Step=100, Loss=8.43750, Rate=43.70 samples/sec, GlobalRate=43.70 samples/sec\n2023-11-29 21:08:28,779 INFO:   | Train Device=CSX, Step=200, Loss=8.12500, Rate=43.67 samples/sec, GlobalRate=43.67 samples/sec\n2023-11-29 21:08:28,781 INFO:   Saving checkpoint at step 200\n2023-11-29 21:13:56,695 INFO:   Saved checkpoint model_dir_gptj/checkpoint_200.mdl\n2023-11-29 21:14:30,135 INFO:   Heartbeat thread stopped for wsjob-kd4olqkhu6ya8qqzt88utd.\n2023-11-29 21:14:30,142 INFO:   Training completed successfully!\n2023-11-29 21:14:30,142 INFO:   Processed 24000 sample(s) in 910.883781998 seconds.\n</code></pre>"},{"location":"ai-testbed/cerebras/example-programs/#llama-7b","title":"Llama-7B","text":"<p>The Cerebras llama7B model implementation can be found at modelzoo/modelzoo/transformers/pytorch/llama and it's overview at https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/pytorch/llama#configs-included-for-this-model. This set up will use a subset of pile data (preprocessed at path /software/datasets/llama_data_32K/) to train with a 32K vocab size. </p> <p>First, source a Cerebras PyTorch virtual environment and make sure that the requirements are installed: <pre><code>source ~/R_2.2.0/venv_cerebras_pt/bin/activate\npip install -r ~/R_2.2.0/modelzoo/requirements.txt\n</code></pre> Instructions for training: <pre><code>cd ~/R_2.2.0/modelzoo/src/cerebras/modelzoo/models/nlp/llama\ncp /software/cerebras/dataset/params_llama_7b.yaml configs/params_llama_7b.yaml\nexport MODEL_DIR=model_dir_llamma\nif [ -d \"$MODEL_DIR\" ]; then rm -Rf $MODEL_DIR; fi\npython run.py CSX --job_labels name=llama_7b --params configs/params_llama_7b.yaml --num_csx=1 --mode train --model_dir $MODEL_DIR --mount_dirs /projects /home/ /software --python_paths /home/$(whoami)/R_2.2.0/modelzoo/src  --compile_dir $(whoami) |&amp; tee mytest.log\n</code></pre></p> <p>Please find a sample output <pre><code>2024-03-21 14:40:57,949 INFO:   Effective batch size is 99.\n2024-03-21 14:40:57,970 INFO:   Checkpoint autoloading is enabled. Looking for latest checkpoint in \"/srv/projects/datascience/vsastry/model_dir_llama/\" directory with the following naming convention: `checkpoint_(step)(_timestamp)?.mdl`.\n2024-03-21 14:40:57,971 INFO:   No checkpoints were found in \"/srv/projects/datascience/vsastry/model_dir_llama/\".\n2024-03-21 14:40:57,971 INFO:   No checkpoint was provided. Using randomly initialized model parameters.\n2024-03-21 14:40:59,419 INFO:   Saving checkpoint at step 0\n2024-03-21 14:48:46,988 INFO:   Saved checkpoint /srv/projects/datascience/vsastry/model_dir_llama/checkpoint_0.mdl\n2024-03-21 14:49:05,547 INFO:   Compiling the model. This may take a few minutes.\n2024-03-21 14:49:05,550 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.\n2024-03-21 14:49:06,819 INFO:   Initiating a new image build job against the cluster server.\n2024-03-21 14:49:06,898 INFO:   Custom worker image build is disabled from server.\n2024-03-21 14:49:06,911 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.\n2024-03-21 14:49:07,143 INFO:   Initiating a new compile wsjob against the cluster server.\n2024-03-21 14:49:07,226 INFO:   compile job id: wsjob-pg4gslxvgsalvh6ppdvydb, remote log path: /n1/wsjob/workdir/job-operator/wsjob-pg4gslxvgsalvh6ppdvydb\n2024-03-21 14:49:17,259 INFO:   Poll ingress status: Waiting for job running, current job status: Queueing, msg: job is queueing. Job queue status: current job is top of queue but likely blocked by running jobs, 1 compile job(s) running using 67Gi memory. For more information, please run 'csctl get jobs'.\n2024-03-21 15:02:07,673 INFO:   Poll ingress status: Waiting for job running, current job status: Queueing, msg: job is queueing. Job queue status: current job is top of queue but likely blocked by running jobs, 1 execute job(s) running using 1 system(s), 1 compile job(s) running using 67Gi memory. For more information, please run 'csctl get jobs'.\n2024-03-21 15:02:17,683 INFO:   Poll ingress status: Waiting for job service readiness.\n2024-03-21 15:02:47,717 INFO:   Ingress is ready: Job ingress ready, poll ingress success.\n2024-03-21 15:02:58,509 INFO:   Pre-optimization transforms...\n2024-03-21 15:03:14,815 INFO:   Optimizing layouts and memory usage...\n2024-03-21 15:03:14,839 INFO:   Gradient accumulation enabled\n2024-03-21 15:03:14,840 WARNING:   Gradient accumulation will search for an optimal micro batch size based on internal performance models, which can lead to an increased compile time. Specify `micro_batch_size` option in the 'train_input/eval_input' section of your .yaml parameter file to set the gradient accumulation microbatch size, if an optimal microbatch size is known.\n\n2024-03-21 15:03:14,842 INFO:   Gradient accumulation trying sub-batch size 3...\n2024-03-21 15:03:21,632 INFO:   Exploring floorplans\n2024-03-21 15:03:30,198 INFO:   Exploring data layouts\n2024-03-21 15:03:50,589 INFO:   Optimizing memory usage\n2024-03-21 15:05:23,008 INFO:   Gradient accumulation trying sub-batch size 33...\n2024-03-21 15:05:30,532 INFO:   Exploring floorplans\n2024-03-21 15:05:37,304 INFO:   Exploring data layouts\n2024-03-21 15:06:11,327 INFO:   Optimizing memory usage\n2024-03-21 15:11:37,204 INFO:   Gradient accumulation trying sub-batch size 9...\n2024-03-21 15:11:44,383 INFO:   Exploring floorplans\n2024-03-21 15:11:50,639 INFO:   Exploring data layouts\n2024-03-21 15:12:16,120 INFO:   Optimizing memory usage\n2024-03-21 15:15:59,788 INFO:   Gradient accumulation trying sub-batch size 11...\n2024-03-21 15:16:06,314 INFO:   Exploring floorplans\n2024-03-21 15:16:12,563 INFO:   Exploring data layouts\n2024-03-21 15:16:40,965 INFO:   Optimizing memory usage\n2024-03-21 15:21:03,938 INFO:   Exploring floorplans\n2024-03-21 15:21:10,918 INFO:   Exploring data layouts\n2024-03-21 15:22:03,953 INFO:   Optimizing memory usage\n2024-03-21 15:30:35,456 INFO:   No benefit from gradient accumulation expected. Compile will proceed at original per-box batch size 99 with 9 lanes\n\n2024-03-21 15:30:35,540 INFO:   Post-layout optimizations...\n2024-03-21 15:32:11,639 INFO:   Allocating buffers...\n2024-03-21 15:32:18,023 INFO:   Code generation...\n2024-03-21 15:32:53,573 INFO:   Compiling image...\n2024-03-21 15:32:53,578 INFO:   Compiling kernels\n2024-03-21 15:34:39,222 INFO:   Compiling final image\n2024-03-21 15:36:54,995 INFO:   Compile artifacts successfully written to remote compile directory. Compile hash is: cs_2599085507768189065\n2024-03-21 15:36:55,146 INFO:   Heartbeat thread stopped for wsjob-pg4gslxvgsalvh6ppdvydb.\n2024-03-21 15:36:55,160 INFO:   Compile was successful!\n2024-03-21 15:36:55,171 INFO:   Programming Cerebras Wafer Scale Cluster for execution. This may take a few minutes.\n2024-03-21 15:36:56,403 INFO:   Defaulted to use the job-operator namespace as the usernode config /opt/cerebras/config_v2 only has access to that namespace.\n2024-03-21 15:36:56,659 INFO:   Initiating a new execute wsjob against the cluster server.\n2024-03-21 15:36:56,758 INFO:   execute job id: wsjob-bdcvvsrwely3kbfwduefqx, remote log path: /n1/wsjob/workdir/job-operator/wsjob-bdcvvsrwely3kbfwduefqx\n2024-03-21 15:37:06,789 INFO:   Poll ingress status: Waiting for job running, current job status: Scheduled, msg: job is scheduled. \n2024-03-21 15:37:16,793 INFO:   Poll ingress status: Waiting for job service readiness.\n2024-03-21 15:37:36,838 INFO:   Poll ingress status: Waiting for job ingress readiness.\n2024-03-21 15:37:46,861 INFO:   Ingress is ready: Job ingress ready, poll ingress success.\n2024-03-21 15:37:47,052 INFO:   Preparing to execute using 1 CSX\n2024-03-21 15:38:33,999 INFO:   About to send initial weights\n2024-03-21 15:40:01,150 INFO:   Finished sending initial weights\n2024-03-21 15:40:01,154 INFO:   Finalizing appliance staging for the run\n2024-03-21 15:40:01,203 INFO:   Waiting for device programming to complete\n2024-03-21 15:41:26,576 INFO:   Device programming is complete\n2024-03-21 15:41:27,888 INFO:   Using network type: ROCE\n2024-03-21 15:41:27,890 INFO:   Waiting for input workers to prime the data pipeline and begin streaming ...\n2024-03-21 15:41:27,942 INFO:   Input workers have begun streaming input data\n2024-03-21 15:41:45,009 INFO:   Appliance staging is complete\n2024-03-21 15:41:45,021 INFO:   Beginning appliance run\n2024-03-21 15:49:45,474 INFO:   | Train Device=CSX, Step=100, Loss=9.84375, Rate=20.61 samples/sec, GlobalRate=20.61 samples/sec\n2024-03-21 15:57:49,616 INFO:   | Train Device=CSX, Step=200, Loss=8.35938, Rate=20.51 samples/sec, GlobalRate=20.53 samples/sec\n2024-03-21 16:05:53,769 INFO:   | Train Device=CSX, Step=300, Loss=8.26562, Rate=20.47 samples/sec, GlobalRate=20.50 samples/sec\n2024-03-21 16:13:58,078 INFO:   | Train Device=CSX, Step=400, Loss=7.02344, Rate=20.45 samples/sec, GlobalRate=20.49 samples/sec\n2024-03-21 16:22:02,644 INFO:   | Train Device=CSX, Step=500, Loss=7.07812, Rate=20.44 samples/sec, GlobalRate=20.48 samples/sec\n2024-03-21 16:30:06,513 INFO:   | Train Device=CSX, Step=600, Loss=7.34375, Rate=20.45 samples/sec, GlobalRate=20.47 samples/sec\n2024-03-21 16:38:10,737 INFO:   | Train Device=CSX, Step=700, Loss=7.19531, Rate=20.45 samples/sec, GlobalRate=20.47 samples/sec\n2024-03-21 16:46:15,052 INFO:   | Train Device=CSX, Step=800, Loss=6.52344, Rate=20.44 samples/sec, GlobalRate=20.47 samples/sec\n2024-03-21 16:54:19,448 INFO:   | Train Device=CSX, Step=900, Loss=6.46875, Rate=20.44 samples/sec, GlobalRate=20.46 samples/sec\n2024-03-21 17:02:24,111 INFO:   | Train Device=CSX, Step=1000, Loss=5.98438, Rate=20.43 samples/sec, GlobalRate=20.46 samples/sec\n2024-03-21 17:10:28,632 INFO:   | Train Device=CSX, Step=1100, Loss=6.17188, Rate=20.43 samples/sec, GlobalRate=20.46 samples/sec\n2024-03-21 17:18:32,943 INFO:   | Train Device=CSX, Step=1200, Loss=6.04688, Rate=20.44 samples/sec, GlobalRate=20.46 samples/sec\n2024-03-21 17:26:37,241 INFO:   | Train Device=CSX, Step=1300, Loss=5.54688, Rate=20.44 samples/sec, GlobalRate=20.45 samples/sec\n2024-03-21 17:34:41,491 INFO:   | Train Device=CSX, Step=1400, Loss=5.92188, Rate=20.44 samples/sec, GlobalRate=20.45 samples/sec\n2024-03-21 17:42:45,646 INFO:   | Train Device=CSX, Step=1500, Loss=5.68750, Rate=20.45 samples/sec, GlobalRate=20.45 samples/sec\n2024-03-21 17:50:50,110 INFO:   | Train Device=CSX, Step=1600, Loss=5.85938, Rate=20.44 samples/sec, GlobalRate=20.45 samples/sec\n</code></pre></p>"},{"location":"ai-testbed/cerebras/getting-started/","title":"Getting Started","text":""},{"location":"ai-testbed/cerebras/getting-started/#getting-started","title":"Getting Started","text":""},{"location":"ai-testbed/cerebras/getting-started/#connection-to-a-cs-2-node","title":"Connection to a CS-2 node","text":"<p> Connection to one of the CS-2 cluster login nodes requires an MFA passcode for authentication - either an 8-digit passcode generated by an app on your mobile device (e.g. MobilePASS+) or a CRYPTOCard-generated passcode prefixed by a 4-digit pin. This is the same passcode used to authenticate into other ALCF systems, such as Polaris. In the examples below, replace ALCFUserID with your ALCF user id. To connect to a CS-2 login:</p> <ol> <li>ssh to a desired login node:     <pre><code>ssh ALCFUserID@cer-login-01.ai.alcf.anl.gov\n</code></pre>     or     <pre><code>ssh ALCFUserID@cer-login-02.ai.alcf.anl.gov\n</code></pre>     or     <pre><code>ssh ALCFUserID@cer-login-03.ai.alcf.anl.gov\n</code></pre></li> <li>Alternatively, ssh randomly to one of the above three login nodes:     <pre><code>ssh ALCFUserID@cerebras.ai.alcf.anl.gov\n</code></pre></li> </ol>"},{"location":"ai-testbed/cerebras/job-queuing-and-submission/","title":"Job Queuing and Submission","text":"<p>The CS-2 cluster has its own Kubernetes-based system for job submission and queuing.</p> <p>Jobs are started automatically through the Python framework in modelzoo.common.pytorch.run_utils Continuous job status for a job is output to stdout/stderr; redirect the output, or consider using a persistent session started with screen, or tmux, or both.</p> <p>Jobs that have not yet completed can be listed as shown. Note: this command can take over a minute to complete.</p> <p><pre><code>(venv_cerebras_pt) $ csctl get jobs\nNAME                          AGE  DURATION  PHASE    SYSTEMS     USER     LABELS        DASHBOARD\nwsjob-thjj8zticwsylhppkbmjqe  13s  1s        RUNNING  cer-cs2-01  username name=unet_pt  https://grafana.cerebras1.lab.alcf.anl.gov/d/WebHNShVz/wsjob-dashboard?orgId=1&amp;var-wsjob=wsjob-thjj8zticwsylhppkbmjqe&amp;from=1691705374000&amp;to=now\n(venv_cerebras_pt) $\n</code></pre> To view the grafana databoard for a job, follow the instructions at Grafana WsJob Dashboard for Cerebras jobs</p> <p>Jobs can be canceled as shown:</p> <pre><code>(venv_cerebras_pt) $ csctl cancel job wsjob-eyjapwgnycahq9tus4w7id\nJob canceled successfully\n(venv_cerebras_pt) $\n</code></pre> <p>Jobs can be labeled in the command line that launches them, if they are written with Cerebras's Python framework for running appliance jobs, by adding a command line option of this form: <pre><code> --job_labels labelname=labelvalue\n</code></pre></p> <p>Jobs can also be labeled after they have been started as shown: <pre><code>(venv_cerebras_pt) $ csctl label job wsjob-ez6dyfronnsg2rz7f7fqw4 testlabel=test\njob/wsjob-ez6dyfronnsg2rz7f7fqw4 was patched\n(venv_cerebras_pt) $\n</code></pre></p> <p>Jobs with a particular label/label value can be listed as shown: <pre><code>(venv_cerebras_pt) $ csctl get jobs | grep \"testlabel=test\"\nwsjob-ez6dyfronnsg2rz7f7fqw4  19m SUCCEEDED  cer-cs2-02 username testlabel=test,user=username\n(venv_cerebras_pt) $\n</code></pre></p> <p>See <code>csctl -h</code> for more options. Add <code>-h</code> to a command for help for that command, e.g. <code>csctl get -h</code> or <code>csctl cancel -h</code>. </p> <pre><code>$ csctl -h\nCerebras cluster command line tool.\n\nUsage:\n  csctl [command]\n\nAvailable Commands:\n  cancel             Cancel job\n  clear-worker-cache Clear the worker cache\n  config             View csctl config files\n  get                Get resources\n  label              Label resources\n  log-export         Gather and download logs.\n  types              Display resource types\n\nFlags:\n  -d, --debug int          higher debug values will display more fields in output objects\n  -h, --help               help for csctl\n      --namespace string   configure csctl to talk to different user namespaces\n  -v, --version            version for csctl\n\nUse \"csctl [command] --help\" for more information about a command.\n</code></pre>"},{"location":"ai-testbed/cerebras/miscellaneous/","title":"Miscellaneous","text":""},{"location":"ai-testbed/cerebras/miscellaneous/#porting-applications-to-the-cs-2","title":"Porting applications to the CS-2","text":"<p>Cerebras documentation for porting code to run on a Cerebras CS-2 system: Ways to port your model</p>"},{"location":"ai-testbed/cerebras/miscellaneous/#grafana-wsjob-dashboard-for-cerebras-jobs","title":"Grafana WsJob Dashboard for Cerebras jobs","text":"<p>A Grafana dashboard provides support for visualizing, querying, and exploring the CS2 system\u2019s metrics and enables to access system logs and traces. See the Cerebras documentation for the Job Information Dashboard</p> <p>Here is a summary (tested to work on Ubuntu and MacOS)</p> <p>On your work machine with a web browser, e.g. your laptop, edit /etc/hosts, using your editor of choice <pre><code>sudo nano /etc/hosts\n</code></pre> Add this line <pre><code>127.0.0.1   grafana.cerebras1.lab.alcf.anl.gov\n</code></pre> Save, and exit the editor</p> <p>Download the Grafana certificate present on the Cerebras node at /opt/cerebras/certs/grafana_tls.crt to your local machine. To add this certificate to your browser keychain, </p> <ol> <li>On chrome, go to Settings-&gt;Privacy and security-&gt;Security-&gt;Manage device certificates</li> <li>Select System under \"System Keychains\" on the left hand side of your screen. Also select the \"Certificate\" tab.</li> <li>Drag and drop the downloaded certificate. Once it is added, it is visible as \"lab.alcf.anl.gov\"    </li> <li>Select the certificate, and ensure that the \"Trust\" section is set to \"Always Trust\"    </li> </ol> <p>On your work machine with a web browser, e.g. your laptop, tunnel the grafana https port on the cerebras grafana host through to localhost <pre><code>ssh -L 8443:grafana.cerebras1.lab.alcf.anl.gov:443 arnoldw@cer-login-03.ai.alcf.anl.gov\n</code></pre></p> <p>Point a browser at grafana. (Tested with Firefox and Chrome/Brave) Open browser to a job grafana url shown in csctl get jobs, adding :8443 to hostname, e.g. <pre><code>https://grafana.cerebras1.lab.alcf.anl.gov:8443/d/WebHNShVz/wsjob-dashboard?orgId=1&amp;var-wsjob=wsjob-49b7uuojdelvtrcxu3cwbw&amp;from=1684859330000&amp;to=noww\n</code></pre></p> <p>Login to the dashboard with user admin, and password prom-operator</p>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/","title":"Running a Model/Program","text":""},{"location":"ai-testbed/cerebras/running-a-model-or-program/#getting-started","title":"Getting Started","text":""},{"location":"ai-testbed/cerebras/running-a-model-or-program/#job-submission-and-queuing","title":"Job submission and queuing","text":"<p>Cerebras jobs are initiated and tracked automatically within the Python framework in modelzoo.common.pytorch.run_utils. This framework interacts with the Cerebras cluster management node.</p>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/#login-nodes","title":"Login nodes","text":"<p>Jobs are launched from login nodes. If you expect a loss of an internet connection for any reason, for long-running jobs we suggest logging into a specific login node and using either screen or tmux to create persistent command line sessions.  For details use:2</p> <pre><code>man screen\n# or\nman tmux\n</code></pre>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/#running-jobs-on-the-wafer","title":"Running jobs on the wafer","text":"<p>Follow these instructions to compile and train the <code>fc_mnist</code> PyTorch sample. This models is a couple of fully connected layers plus dropout and RELU. </p>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/#cerebras-virtual-environments","title":"Cerebras virtual environments","text":"<p>First, make a virtual environment for Cerebras for PyTorch. See Customizing Environments for the procedures for making PyTorch virtual environments for Cerebras. If an environment is made in <code>~/R_2.2.0/</code>, it would be activated as follows: <pre><code>source ~/R_2.2.0/venv_cerebras_pt/bin/activate\n</code></pre></p>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/#clone-the-cerebras-modelzoo","title":"Clone the Cerebras modelzoo","text":"<pre><code>mkdir ~/R_2.2.0\ncd ~/R_2.2.0\ngit clone https://github.com/Cerebras/modelzoo.git\ncd modelzoo\ngit tag\ngit checkout Release_2.2.0\n</code></pre>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/#running-a-pytorch-sample","title":"Running a Pytorch sample","text":""},{"location":"ai-testbed/cerebras/running-a-model-or-program/#activate-your-pytorch-virtual-environment-install-modelzoo-requirements-and-change-to-the-working-directory","title":"Activate your PyTorch virtual environment, install modelzoo requirements, and change to the working directory","text":"<pre><code>source ~/R_2.2.0/venv_cerebras_pt/bin/activate\npip install -r ~/R_2.2.0/modelzoo/requirements.txt\ncd ~/R_2.2.0/modelzoo/src/cerebras/modelzoo/fc_mnist/pytorch\n</code></pre> <p>Next, edit configs/params.yaml, making the following changes:</p> <pre><code> train_input:\n-    data_dir: \"./mnist\"\n+    data_dir: \"/software/cerebras/dataset/fc_mnist/data/mnist/train\"\n</code></pre> <p>and</p> <pre><code> eval_input:\n-    data_dir: \"./mnist\"\n+    data_dir: \"/software/cerebras/dataset/fc_mnist/data/mnist/train\"\n</code></pre> <p>If you want to have the sample download the dataset, you will need to specify absolute paths for the \"data_dir\"s.</p>"},{"location":"ai-testbed/cerebras/running-a-model-or-program/#running-a-sample-pytorch-training-job","title":"Running a sample PyTorch training job","text":"<p>To run the sample:</p> <pre><code>export MODEL_DIR=model_dir\n# deletion of the model_dir is only needed if sample has been previously run\nif [ -d \"$MODEL_DIR\" ]; then rm -Rf $MODEL_DIR; fi\npython run.py CSX --job_labels name=fc_mnist --params configs/params.yaml --num_csx=1 --mode train --model_dir $MODEL_DIR --mount_dirs /home/$(whoami)/ /software --python_paths /home/$(whoami)/R_2.2.0/modelzoo/src --compile_dir /$(whoami) |&amp; tee mytest.log\n</code></pre> <p>A successful fc_mnist PyTorch training run should finish with output resembling the following:</p> <pre><code>2023-11-29 18:13:13,048 INFO:   | Train Device=CSX, Step=1950, Loss=2.28834, Rate=397.31 samples/sec, GlobalRate=433.98 samples/sec\n2023-11-29 18:13:13,555 INFO:   | Train Device=CSX, Step=2000, Loss=2.34778, Rate=395.69 samples/sec, GlobalRate=431.83 samples/sec\n2023-11-29 18:13:13,555 INFO:   Saving checkpoint at step 2000\n2023-11-29 18:13:17,242 INFO:   Saved checkpoint model_dir/checkpoint_2000.mdl\n2023-11-29 18:13:55,517 INFO:   Heartbeat thread stopped for wsjob-fpwqt7maq8a5mxvblwwzbu.\n2023-11-29 18:13:55,523 INFO:   Training completed successfully!\n2023-11-29 18:13:55,523 INFO:   Processed 4000 sample(s) in 51.230697212 seconds.\n</code></pre>"},{"location":"ai-testbed/cerebras/system-overview/","title":"System Overview","text":"<p>The Cerebras CS-2 is a wafer-scale deep learning accelerator comprising 850,000 processing cores, each providing 48KB of dedicated SRAM memory for an on-chip total of 40GB and interconnected to optimize bandwidth and latency. Its software platform integrates the popular machine learning framework PyTorch.</p> <p>The ALCF CS-2 systems are configured as a Cerebras Wafer-Scale Cluster, designed to support large-scale models (up to and well beyond 1 billion parameters) and large-scale inputs. The cluster contains two CS-2 systems and can distribute jobs across one or both CS-2 systems in a data-parallel framework. The supporting CPU cluster consists of MemoryX, SwarmX, management, and input worker nodes. The Cerebras Wafer-Scale cluster is run as an appliance: a user submits a job to the appliance, and the appliance manages preprocessing and streaming of the data, IO, and device orchestration within the appliance. It provides programming via PyTorch, with data-parallel distribution when using more than one CS-2. This installation supports both Pipelined execution for models up to 1 billion parameters and Weight Streaming execution for models up to and above 1 billion parameters.</p> <p>The public Cerebras documentation is available here.</p> <p>A typical Cerebras Wafer-Scale Cluster is shown in the figure. Users connect (ssh) to one of the three login nodes. Either ssh to <code>cerebras.ai.alcf.anl.gov</code>, which randomly resolves to one of cer-login-0[1-3].ai.alcf.anl.gov, or ssh to a specific node, <code>cer-login-01.ai.alcf.anl.gov</code>, <code>cer-login-02.ai.alcf.anl.gov</code>, <code>cer-login-03.ai.alcf.anl.gov</code>. The rest of the nodes in the cluster infrastructure are not directly accessible, except by admins. The trees <code>/home</code>, <code>/projects</code>, and <code>/software</code> are shared across all three login nodes, the relevant cluster infrastructure nodes, and all ALCF AI testbed platforms.</p> <p> </p> CS-2 cluster figure <p>(Figure from https://docs.cerebras.net/en/latest/wsc/cerebras-basics/how-cerebras-works.html)</p> <p>As indicated in the figures, the CS-2 nodes on the right are responsible only for running and accelerating the computations for training and predictions with the model. The other work, including compilation, is performed by input nodes, and by MemoryX nodes, which are used for weight storage and broadcast, and SwarmX nodes, which are used for gradient accumulation. Some model verification work can be done on login nodes.</p>"},{"location":"ai-testbed/cerebras/tunneling-and-forwarding-ports/","title":"Tunneling and Forwarding Ports","text":"<p>See ALCF's Jupyter Instructions, and Tunneling and forwarding ports. The Cerebras login nodes are direct login; tunneling and port forwarding do not involve jump hosts.</p>"},{"location":"ai-testbed/data-management/data-management-overview/","title":"Data Management for the AI Testbed","text":""},{"location":"ai-testbed/data-management/data-management-overview/#home-file-system-space","title":"Home File System Space","text":"<p>Users have a shared home filesystem <code>/home</code> shared across the ALCF AI testbed systems, including the login and compute nodes. Default user quota is <code>1 TB</code> storage and <code>1,000,000 files</code>. This space is backed up. </p>"},{"location":"ai-testbed/data-management/data-management-overview/#project-file-system-space","title":"Project File System Space","text":"<p>The team project/campaign file system <code>/projects</code> is intended to facilitate project collaboration and is accessible to the team members of your project that have an ALCF account.  Default group storage quota is <code>2 TB</code> and <code>2,000,000 files</code>. Please note that this space isn't backed up.  Our policy is that data will be purged from disk 6 months after project completion.</p>"},{"location":"ai-testbed/data-management/data-management-overview/#data-transfer","title":"Data Transfer","text":"<p>Users can transfer data to and from the AI testbed using <code>Globus</code> or tools such as <code>scp</code> or <code>rsync</code>.</p>"},{"location":"ai-testbed/data-management/data-management-overview/#using-globus","title":"Using Globus","text":"<p>We have a Globus endpoint each to move data to and from the <code>/projects</code> and <code>/home</code> filesystem respectively.</p> <ul> <li>Use <code>alcf#ai_testbed_projects</code> for the <code>/projects</code> file system</li> <li>Use <code>alcf#ai_testbed_home</code> for the <code>/home</code> files system </li> </ul> <p>Relevant information regarding using globus can be found here</p>"},{"location":"ai-testbed/data-management/data-management-overview/#alcf-storage-policies","title":"ALCF Storage Policies","text":"<p>ALCF data policies is available here </p> <p>Please Note: The basic level of protection provided is UNIX file level permissions; it is the user's responsibility to ensure that file permissions and umasks are set to match their needs.</p>"},{"location":"ai-testbed/files/notes/","title":"Notes","text":"<pre><code>git submodule init; git submodule update\n</code></pre>"},{"location":"ai-testbed/files/todo/","title":"TODO","text":""},{"location":"ai-testbed/files/todo/#cosmictagger-v1x","title":"CosmicTagger v1.x","text":"<p>Note: Conversion of CT to the various machines is meant to be a tutorial as to how to convert a model.</p>"},{"location":"ai-testbed/files/todo/#cerebras-ct","title":"Cerebras CT","text":"<p>Cerebras cannot support CT and UNets in general as of 4/25/23.</p>"},{"location":"ai-testbed/files/todo/#graphcore-ct","title":"Graphcore CT","text":"<p>Alex has been very busy with conferences, etc.</p> <p>He ran CT but, it ran on the CPU.  He has stated that it may need to be completely written using, I can't remember which, Poplar or PopArt.  If that is necessary, Venkat should make the call.</p>"},{"location":"ai-testbed/files/todo/#groq-ct","title":"Groq CT","text":""},{"location":"ai-testbed/files/todo/#habana-ct","title":"Habana CT","text":"<p>Repo:    https://github.com/argonne-lcf/user-guides.git Branch:  feature/Habana002-DNP File:    docs/ai-testbed/habana/CosmicTagger-Conversion.md</p>"},{"location":"ai-testbed/files/todo/#sambanova-ct","title":"SambaNova CT","text":"<p>SN has a highly-engineered version of CT.</p> <p>They are working to support CT OOB, Out-Of-Box.</p>"},{"location":"ai-testbed/files/todo/#cerebras","title":"Cerebras","text":"<p>Repo:    https://github.com/argonne-lcf/user-guides.git Branch:  Talk to Bill.</p>"},{"location":"ai-testbed/files/todo/#graphcore","title":"Graphcore","text":"<p>Repo:  https://github.com/argonne-lcf/user-guides.git</p> <p>When you change back to 3.2, use virtual-environments.md from the commit a4ce3b5598f4d6feee7ca58accde1a6a0ea84244 \"virtual-environments.md with 3.2 edits.\"</p>"},{"location":"ai-testbed/files/todo/#groq","title":"Groq","text":"<p>Repo:   https://github.com/argonne-lcf/user-guides.git Branch: feature/Groq001-DNP</p>"},{"location":"ai-testbed/files/todo/#habana","title":"Habana","text":"<p>Repo:  https://github.com/argonne-lcf/user-guides.git Branch:  feature/Habana002-DNP</p>"},{"location":"ai-testbed/files/todo/#sambanova","title":"SambaNova","text":"<p>Repo:  https://github.com/argonne-lcf/user-guides.git</p>"},{"location":"ai-testbed/graphcore/documentation/","title":"Documentation links","text":"<p>Poplar SDK PyTorch for the IPU: User Guide Targetting the IPU from Tensorflow 2 IPU programming guide Examples Examples Github Repo POD systems POD64 specs </p>"},{"location":"ai-testbed/graphcore/example-programs/","title":"Example Programs","text":"<p>Graphcore provides examples of some well-known AI applications in their repository at https://github.com/graphcore/examples.git. Clone the examples repository to your personal directory structure: <pre><code>mkdir ~/graphcore\ncd ~/graphcore\ngit clone https://github.com/graphcore/examples.git\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#mnist-poptorch","title":"MNIST - PopTorch","text":""},{"location":"ai-testbed/graphcore/example-programs/#activate-poptorch-environment","title":"Activate PopTorch Environment","text":"<pre><code>source ~/venvs/graphcore/poptorch33_env/bin/activate\n</code></pre>"},{"location":"ai-testbed/graphcore/example-programs/#install-requirements","title":"Install Requirements","text":"<p>Change directory: <pre><code>cd ~/graphcore/examples/tutorials/simple_applications/pytorch/mnist\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#run-mnist","title":"Run MNIST","text":"<p>Execute the command: <pre><code>/opt/slurm/bin/srun --ipus=1 python mnist_poptorch.py\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#output","title":"Output","text":"<p>The expected output will resemble the following:</p> <pre><code>srun: job 10671 queued and waiting for resources\nsrun: job 10671 has been allocated resources\nTrainingModelWithLoss(\n  (model): Network(\n    (layer1): Block(\n      (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (relu): ReLU()\n    )\n    (layer2): Block(\n      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (relu): ReLU()\n    )\n    (layer3): Linear(in_features=1600, out_features=128, bias=True)\n    (layer3_act): ReLU()\n    (layer3_dropout): Dropout(p=0.5, inplace=False)\n    (layer4): Linear(in_features=128, out_features=10, bias=True)\n    (softmax): Softmax(dim=1)\n  )\n  (loss): CrossEntropyLoss()\n)\nEpochs:   0%|          | 0/10 [00:00&lt;?,[23:27:06.753] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 10\nGraph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00]\nEpochs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:17&lt;00:00,  7.71s/it]\nGraph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00]                          \nAccuracy on test set: 96.85%\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00]\n</code></pre>"},{"location":"ai-testbed/graphcore/example-programs/#mnist-tensorflow2","title":"MNIST - Tensorflow2","text":""},{"location":"ai-testbed/graphcore/example-programs/#activate-tensorflow2-environment","title":"Activate Tensorflow2 Environment","text":"<p>Create a TensorFlow2 environment as explained in the tensorflow-2-environment-setup and activate the same. <pre><code>source ~/venvs/graphcore/tensorflow2_33_env/bin/activate\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#install-requirements_1","title":"Install Requirements","text":"<p>Change directory: <pre><code>cd ~/graphcore/examples/tutorials/simple_applications/tensorflow2/mnist/\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#run-mnist-tensorflow","title":"Run MNIST - TensorFlow","text":"<p>Execute the command:</p> <pre><code>/opt/slurm/bin/srun --ipus=1 python mnist.py\n</code></pre>"},{"location":"ai-testbed/graphcore/example-programs/#output_1","title":"Output","text":"<p>The expected output will resemble the following:</p> <pre><code>srun: job 10672 queued and waiting for resources\nsrun: job 10672 has been allocated resources\n2023-08-22 23:35:02.925033: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 3.3.0 (de1f8de2a7) Poplar package: b67b751185\n2023-08-22 23:35:06.119772: I tensorflow/compiler/plugin/poplar/driver/poplar_executor.cc:1619] TensorFlow device /device:IPU:0 attached to 1 IPU with Poplar device ID: 0\n2023-08-22 23:35:07.087287: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2023-08-22 23:35:07.351132: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n2023-08-22T23:35:09.469066Z PL:POPOPS    3545299.3545299 W: createOutputForElementWiseOp 'while/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits/fusion.3/Op/Equal/Out' ({32,10}): No suitable input found, creating new variable with linear tile mapping\n2023-08-22 23:35:18.532415: I tensorflow/compiler/jit/xla_compilation_cache.cc:376] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nEpoch 1/4\n2000/2000 [==============================] - 13s 6ms/step - loss: 0.6220\nEpoch 2/4\n2000/2000 [==============================] - 1s 262us/step - loss: 0.3265\nEpoch 3/4\n2000/2000 [==============================] - 1s 273us/step - loss: 0.2781\nEpoch 4/4\n2000/2000 [==============================] - 1s 289us/step - loss: 0.2482\n</code></pre>"},{"location":"ai-testbed/graphcore/example-programs/#resnet50","title":"ResNet50","text":""},{"location":"ai-testbed/graphcore/example-programs/#activate-poptorch-environment_1","title":"Activate PopTorch Environment","text":"<p>Create and activate a fresh PopTorch environment <code>poptorch33_resnet50_env</code> as outlined in the virtual environment section, then activate it. <pre><code>source ~/venvs/graphcore/poptorch33_resnet50_env/bin/activate\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#install-requirements_2","title":"Install Requirements","text":"<p>Change directory <pre><code>cd ~/graphcore/examples/vision/cnns/pytorch\nmake install \nmake install-turbojpeg\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#update-configsyml","title":"Update configs.yml","text":"<p>Change directory: <pre><code>cd ~/graphcore/examples/vision/cnns/pytorch/train\n</code></pre> Open configs.yml with your favorite editor. Find in the resnet50 section <pre><code>use_bbox_info: true\n</code></pre> and change it to: <pre><code>use_bbox_info: false\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#run-resnet50","title":"Run ResNet50","text":"<p>The scripts to train a ResNet50 PyTorch model on Pod4 is located at https://github.com/graphcore/examples/tree/master/vision/cnns/pytorch/train</p> <p>Set the following environmental variables. <pre><code>mkdir -p ~/graphcore/tmp/pt_cache/\nexport PYTORCH_CACHE_DIR=~/graphcore/tmp/pt_cache/\n</code></pre> To run 4 replicas (a total for 4 IPUs) of the ResNet50 model: Make a script with the following contents, called poprun_unet.sh This script tells poprun to use the partition id of the partition created for the slurm job used to run the script. <pre><code>#!/bin/bash\npoprun -vv --vipu-partition=slurm_${SLURM_JOBID} --num-instances=1 --num-replicas=4 --executable-cache-path=$PYTORCH_CACHE_DIR python3 /home/$USER/graphcore/examples/vision/cnns/pytorch/train/train.py --config resnet50-pod4 --imagenet-data-path /mnt/localdata/datasets/imagenet-raw-dataset --epoch 2 --validation-mode none --dataloader-worker 14 --dataloader-rebatch-size 256\n</code></pre> Then <pre><code>chmod +x poprun_unet.sh\n/opt/slurm/bin/srun --ipus=4 poprun_unet.sh\n</code></pre></p> <p>This model is run with the imagenet dataset.</p>"},{"location":"ai-testbed/graphcore/example-programs/#output_2","title":"Output","text":"<p>The expected output starts with this: <pre><code>srun: job 10675 queued and waiting for resources\nsrun: job 10675 has been allocated resources\n23:48:29.160 3555537 POPRUN [I] V-IPU server address picked up from 'vipu': 10.1.3.101:8090\n23:48:29.160 3555537 POPRUN [D] Connecting to 10.1.3.101:8090\n23:48:29.162 3555537 POPRUN [D] Status for partition slurm_10673: OK (error 0)\n23:48:29.162 3555537 POPRUN [I] Partition slurm_10673 already exists and is in state: PS_ACTIVE\n23:48:29.163 3555537 POPRUN [D] The reconfigurable partition slurm_10673 is OK\n ===========================\n|      poprun topology      |\n|===========================|\n| hosts     | gc-poplar-02  |\n|-----------|---------------|\n| ILDs      |       0       |\n|-----------|---------------|\n| instances |       0       |\n|-----------|---------------|\n| replicas  | 0 | 1 | 2 | 3 |\n ---------------------------\n23:48:29.163 3555537 POPRUN [D] Target options from environment: {}\n23:48:29.163 3555537 POPRUN [D] Target options from V-IPU partition: {\"ipuLinkDomainSize\":\"4\",\"ipuLinkConfiguration\":\"default\",\"ipuLinkTopology\":\"mesh\",\"gatewayMode\":\"true\",\"instanceSize\":\"4\"}\n23:48:29.207 3555537 POPRUN [D] Found 1 devices with 4 IPUs\n23:48:29.777 3555537 POPRUN [D] Attached to device 6\n23:48:29.777 3555537 POPRUN [I] Preparing parent device 6\n23:48:29.777 3555537 POPRUN [D] Device 6 ipuLinkDomainSize=64, ipuLinkConfiguration=Default, ipuLinkTopology=Mesh, gatewayMode=true, instanceSize=4\n23:48:33.631 3555537 POPRUN [D] Target options from Poplar device: {\"ipuLinkDomainSize\":\"64\",\"ipuLinkConfiguration\":\"default\",\"ipuLinkTopology\":\"mesh\",\"gatewayMode\":\"true\",\"instanceSize\":\"4\"}\n23:48:33.631 3555537 POPRUN [D] Using target options: {\"ipuLinkDomainSize\":\"4\",\"ipuLinkConfiguration\":\"default\",\"ipuLinkTopology\":\"mesh\",\"gatewayMode\":\"true\",\"instanceSize\":\"4\"}\n</code></pre> Expected output ends with this: <pre><code>Graph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04&lt;00:00][1,0]&lt;stderr&gt;:2023-08-22T23:49:40.103248Z PO:ENGINE   3556102.3556102 W: WARNING: The compile time engine option debug.branchRecordTile is set to \"5887\" when creating the Engine. (At compile time it was set to 1471)\n[1,0]&lt;stderr&gt;:\nLoss:6.7539 [1,0]&lt;stdout&gt;:[INFO] Epoch 1\u2588\u2588\u2588\u2588\u258c| 75/78 [02:42&lt;00:06,  2.05s/it][1,0]&lt;stderr&gt;:\n[1,0]&lt;stdout&gt;:[INFO] loss: 6.7462,\n[1,0]&lt;stdout&gt;:[INFO] accuracy: 0.62 %\n[1,0]&lt;stdout&gt;:[INFO] throughput: 7599.7 samples/sec\n[1,0]&lt;stdout&gt;:[INFO] Epoch 2/2\nLoss:6.7462 | Accuracy:0.62%: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [02:48&lt;00:00,  2.16s/it][1,0]&lt;stderr&gt;:\nLoss:6.2821 | Accuracy:2.42%:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 75/7[1,0]&lt;stdout&gt;:[INFO] Epoch 2,0]&lt;stderr&gt;:\n[1,0]&lt;stdout&gt;:[INFO] loss: 6.2720,\n[1,0]&lt;stdout&gt;:[INFO] accuracy: 2.48 %\n[1,0]&lt;stdout&gt;:[INFO] throughput: 8125.8 samples/sec\n[1,0]&lt;stdout&gt;:[INFO] Finished training. Time: 2023-08-22 23:54:57.853508. It took: 0:05:26.090631\nLoss:6.2720 | Accuracy:2.48%: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78/78 [02:37&lt;00:00,  2.02s/it][1,0]&lt;stderr&gt;:\n[1,0]&lt;stderr&gt;:/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 14 leaked semaphore objects to clean up at shutdown\n[1,0]&lt;stderr&gt;:  warnings.warn('resource_tracker: There appear to be %d '\n23:55:02.722 3555537 POPRUN [I] mpirun (PID 3556098) terminated with exit code 0\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#gpt-2-pytorch-pod16-run","title":"GPT-2 PyTorch - POD16 run","text":"<p>The scripts to train a GPT-2 pytorch model on the POD16 are located at https://github.com/graphcore/examples/tree/master/nlp/gpt2/pytorch</p> <p>In order to run the GPT-2 Pytorch model, create a new popTorch virtual environment poptorch33_gpt2 as described in the virtual environment section and activate it.</p> <pre><code>source ~/venvs/graphcore/poptorch33_gpt2/bin/activate\n</code></pre>"},{"location":"ai-testbed/graphcore/example-programs/#install-requirements_3","title":"Install Requirements","text":"<p>Change directory: <pre><code>cd ~/graphcore/examples/nlp/gpt2/pytorch\npip3 install -r requirements.txt\n</code></pre></p>"},{"location":"ai-testbed/graphcore/example-programs/#run-gpt2-on-16-ipus","title":"Run GPT2 on 16 IPUs","text":"<p>The command for the GPT2 model is as follows is as follows. <pre><code>/opt/slurm/bin/srun --ipus=16 python /home/$USER/graphcore/examples/nlp/gpt2/pytorch/train_gpt2.py --model gpt2 --ipus-per-replica 4 --replication-factor 4 --gradient-accumulation 2048 --device-iterations 8 --batch-size 1 --layers-per-ipu 0 4 4 4 --matmul-proportion 0.15 0.15 0.15 0.15 --max-len 1024 --optimizer AdamW --learning-rate 0.00015 --lr-schedule cosine --lr-warmup 0.01 --remap-logit True --enable-sequence-serialized True --embedding-serialization-factor 4 --recompute-checkpoint-every-layer True --enable-half-partials True --replicated-tensor-sharding True --dataset 'generated' --epochs 1\n</code></pre> It runs a <code>gpt2</code> model that fits on 4 IPUS indicated by <code>--ipus-per-replica</code>. The <code>--replication-factor</code> indicates how many times the model is replicated in a data parallel manner (4 in the above example). Hence the total number of IPUs used in this example is 16.</p> <p>The effective global batch size in this example is (micro)batch-size * gradient-accumulation * replication-factor = 1 x 2048 x 4 = 8192.  The device iterations indicates the total number samples loaded in 1 training step = global batch size * device iterations = 8192*8 = 65536. To learn more about these parameters and in general batching of IPUs refer IPU batching .</p> <p>The above example is running with <code>generated</code> or <code>synthetic data</code>. To use the same example with a real world dataset, refer to data setup.</p>"},{"location":"ai-testbed/graphcore/example-programs/#output_3","title":"Output","text":"<p>Expected output starts with the following: <pre><code>srun: job 10697 queued and waiting for resources\nsrun: job 10697 has been allocated resources\nBuilding (if necessary) and loading remap_tensor_ce.\nFailed to find compiled extension; rebuilding.\nBuilding (if necessary) and loading residual_add_inplace_pattern.\nModel initializing\n-------------------- Device Allocation --------------------\nEmbedding  --&gt; IPU 0\nLayer 0  --&gt; IPU 1\nLayer 1  --&gt; IPU 1\nLayer 2  --&gt; IPU 1\nLayer 3  --&gt; IPU 1\nLayer 4  --&gt; IPU 2\nLayer 5  --&gt; IPU 2\nLayer 6  --&gt; IPU 2\nLayer 7  --&gt; IPU 2\nLayer 8  --&gt; IPU 3\nLayer 9  --&gt; IPU 3\nLayer 10 --&gt; IPU 3\nLayer 11 --&gt; IPU 3\nLM_head --&gt; IPU 0\n</code></pre> Expected output ends with the following: <pre><code>step 0 of epoch 0, loss: 10.913220405578613, acc: 2.0071864128112793e-05, lr: 0.00012803300858899104, throughput: 646.8439205981404 samples/sec\nstep 1 of epoch 0, loss: 10.836345672607422, acc: 1.9788742065429688e-05, lr: 7.5e-05, throughput: 1058.0979097185766 samples/sec\nstep 2 of epoch 0, loss: 10.831247329711914, acc: 2.0518898963928223e-05, lr: 2.1966991411008938e-05, throughput: 1058.7595523807183 samples/sec\nstep 3 of epoch 0, loss: 10.829034805297852, acc: 1.990795135498047e-05, lr: 0.0, throughput: 1059.6762623043378 samples/sec\n</code></pre></p> <p>Note: The graph compilation for a large model like GPT-2 takes about half an hour. </p>"},{"location":"ai-testbed/graphcore/getting-started/","title":"Getting Started","text":"<p>Connection to a Graphcore node is a two-step process.</p> <p>The first step is to ssh from a local machine to the login node.</p> <p>The second step is to log in to a Graphcore node from the login node.</p> <p></p>"},{"location":"ai-testbed/graphcore/getting-started/#log-in-to-login-node","title":"Log in to Login Node","text":"<p>Login to the Graphcore login node from your local machine using the below command. This uses the ALCF account ID that uses the password generated from the MobilePASS+.</p> <p>Note:  In the examples below, replace ALCFUserID with your ALCF user id.</p> <pre><code>ssh ALCFUserID@gc-login-01.ai.alcf.anl.gov\n# or\nssh ALCFUserID@gc-login-02.ai.alcf.anl.gov\n</code></pre> <p>Note: Use the ssh \"-v\" option in order to debug any ssh problems.</p>"},{"location":"ai-testbed/graphcore/getting-started/#log-in-to-a-graphcore-node","title":"Log in to a Graphcore Node","text":"<p>Once you are on the login node, ssh to one of the Graphcore nodes.</p> <pre><code>ssh gc-poplar-02.ai.alcf.anl.gov\n# or\nssh gc-poplar-03.ai.alcf.anl.gov\n# or\nssh gc-poplar-04.ai.alcf.anl.gov\n</code></pre> <p>**Note: <code>ssh gc-poplar-01.ai.alcf.anl.gov</code> is not accessible to users. However, its IPU resources are assigned by the slurm tasks.</p>"},{"location":"ai-testbed/graphcore/job-queuing-and-submission/","title":"Job Queueing and Submission","text":""},{"location":"ai-testbed/graphcore/job-queuing-and-submission/#introduction","title":"Introduction","text":"<p>ALCF's Graphcore POD64 system uses Slurm for job submission and queueing. Below are some of the important commands for using Slurm. For more information refer to Slurm Documentation.</p> <p>NOTE: Jobs that require IPUs will fail unless launched with <code>srun</code> or <code>sbatch</code>. NOTE: There is a single Slurm scheduler for the Graphcore POD64.</p>"},{"location":"ai-testbed/graphcore/job-queuing-and-submission/#srun","title":"SRun","text":"<p>The Slurm command <code>srun</code> can be used to run individual Python scripts (or other programs) in parallel with other scripts on a cluster managed by Slurm. An example of <code>srun</code> usage is shown below. Use the <code>--ipus=</code> option to specify the number of IPUs required for the run.</p> <p>Example:</p> <pre><code>srun --ipus=1 python mnist_poptorch.py\n</code></pre>"},{"location":"ai-testbed/graphcore/job-queuing-and-submission/#sbatch","title":"SBatch","text":"<p>Alternatively, these jobs can be submitted to the Slurm workload manager through a batch script by using the <code>sbatch</code> command. To do this, create a bash script (submit-mnist-poptorch-job.sh here as an example) with the commands that you want to execute.</p> <pre><code>#!/bin/sh\n\npython mnist_poptorch.py\n</code></pre> <p>Then pass the bash script as an input to the <code>sbatch</code> command as shown below, requesting the number of IPUs required:</p> <pre><code>sbatch --ipus=1 --output=mnist-poptorch-output.log submit-mnist-poptorch-job.sh\n</code></pre>"},{"location":"ai-testbed/graphcore/job-queuing-and-submission/#squeue","title":"SQueue","text":"<p>The <code>squeue</code> command provides information about jobs located in the Slurm scheduling queue.</p> <pre><code>$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n              2572       p64 Graphcor username  R       1:12      1 gc-poplar-02\n</code></pre>"},{"location":"ai-testbed/graphcore/job-queuing-and-submission/#sinfo","title":"SInfo","text":"<p>SInfo is used to view partition and node information for a system running Slurm.</p> <pre><code>$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\np64*         up   infinite      3   idle gc-poplar-[02-04]\n</code></pre> <p>For more information, see SInfo.</p>"},{"location":"ai-testbed/graphcore/job-queuing-and-submission/#scancel","title":"SCancel","text":"<p>SCancel is used to signal or cancel jobs, job arrays, or job steps.</p> <pre><code>scancel job_id\n</code></pre>"},{"location":"ai-testbed/graphcore/miscellaneous/","title":"Miscellaneous","text":""},{"location":"ai-testbed/graphcore/miscellaneous/#status","title":"Status","text":""},{"location":"ai-testbed/graphcore/miscellaneous/#gc-monitor","title":"GC-Monitor","text":"<p>The command <code>gc-monitor</code> is Graphcore's device usage monitor. Run it as follows for ordinary monitoring. See <code>gc-monitor --help</code> for other options.</p> <p><pre><code>export IPUOF_VIPU_API_HOST=10.1.3.101\ngc-monitor --no-card-info --all-partitions\n# or watch gc-monitor --no-card-info --all-partitions\n</code></pre> The IPUOF_VIPU_API_HOST environment variable can conflict with the running of poptorch programs.  The graphcore nodes have a convenience script that temporarily sets this environment variable. <pre><code>wrapped_gc_monitor.sh --no-card-info --all-partitions\n</code></pre></p> <p>Note: if there are no partitions active, gc-monitor will core dump: <code>Segmentation fault (core dumped)</code></p> <p>The output will look something like:</p> <pre><code>+--------------------------------------------------------------+-----------------------+\n|      IPUs in slurm_2616 attached from other namespaces       |         Board         |\n+----+------------------------------+--------------+-----------+-----------+-----------+\n| ID |       Application host       |    Clock     |   Temp    |   Temp    |   Power   |\n+----+------------------------------+--------------+-----------+-----------+-----------+\n| 0  |         gc-poplar-02         |   1850MHz    |  24.2 C   |  21.1 C   |  92.3 W   |\n+----+------------------------------+--------------+-----------+-----------+-----------+\n</code></pre>"},{"location":"ai-testbed/graphcore/miscellaneous/#gc-info","title":"GC-Info","text":"<p>The command <code>gc-info</code> is used to display device information. See <code>gc-info --help</code> for more options.</p> <p>To list devices,  <pre><code>gc-info -l\n</code></pre></p> <p>The command <code>gc-info</code> lists the partition and different IPU Id's along with the multi-IPU configuration IDs.</p> <pre><code>-+- Id:  [0], target: [Fabric], IPU-M host:  [10.1.5.1], IPU#: [3]\n-+- Id:  [1], target: [Fabric], IPU-M host:  [10.1.5.1], IPU#: [2]\n-+- Id:  [2], target: [Fabric], IPU-M host:  [10.1.5.1], IPU#: [1]\n</code></pre> <p>One may also display detailed information for a specific device.  The devices are numbered 0-63.  For example,</p> <pre><code>gc-info --device-id 0 --device-info\n</code></pre> <p>See <code>gc-info --help</code> for more information.</p>"},{"location":"ai-testbed/graphcore/miscellaneous/#how-busy-is-the-system","title":"How busy is the system?","text":"<p>Use one of</p> <pre><code>top\nhtop\n</code></pre>"},{"location":"ai-testbed/graphcore/running-a-model-or-program/","title":"Steps to Run a Model/Program","text":"<p>Note:  Please be mindful of how you are using the system. For example, consider running larger jobs in the evening or on weekends.</p> <p>Running of any model or application includes graph compilation of the model that is then deployed on the IPUs. Below is the description of training a neural network for classification on the MNIST dataset using the PopTorch (pytorch framework optimized for IPU).</p>"},{"location":"ai-testbed/graphcore/running-a-model-or-program/#examples-repo","title":"Examples Repo","text":"<p>Graphcore provides examples of some well-known AI applications in their repository at https://github.com/graphcore/examples.git.</p> <p>Clone the examples repository to your personal directory structure, and checkout the v3.3.0 release:</p> <pre><code>mkdir ~/graphcore\ncd ~/graphcore\ngit clone https://github.com/graphcore/examples.git\ncd examples\n</code></pre>"},{"location":"ai-testbed/graphcore/running-a-model-or-program/#mnist","title":"MNIST","text":""},{"location":"ai-testbed/graphcore/running-a-model-or-program/#activate-poptorch-environment","title":"Activate PopTorch Environment","text":"<p>Follows the steps at Poptorch environment setup to enable the Poplar SDK.</p> <pre><code>source ~/venvs/graphcore/poptorch33_env/bin/activate\n</code></pre>"},{"location":"ai-testbed/graphcore/running-a-model-or-program/#install-requirements","title":"Install Requirements","text":"<p>Change directory and install packages specific to the MNIST model:</p> <pre><code>cd ~/graphcore/examples/tutorials/simple_applications/pytorch/mnist\n</code></pre>"},{"location":"ai-testbed/graphcore/running-a-model-or-program/#run-mnist","title":"Run MNIST","text":"<p>Execute the command:</p> <pre><code>/opt/slurm/bin/srun --ipus=1 python mnist_poptorch.py\n</code></pre> <p>All models are run using Slurm, with the <code>--ipus</code> indicating how many IPUs are need to be allocated for the model being run. This example uses a batchsize of 8, and run for 10 epochs. It also set the device iteration to 50 which is the number of iterations the device should run over the data before returning to the user.  The dataset used in the example is derived from the TorchVision and the PopTorch dataloader is used to load the data required for the 50 device iterations from the host to the device in a single step.</p> <p>The model used here is a simple CNN based model with an output from a classifier (softmax layer). A simple Pytorch model is translated to a PopTorch model using <code>poptorch.Options()</code>. <code>poptorch.trainingModel</code> is the model wrapping function on the Pytorch model. The first call to <code>trainingModel</code> will compile the model for the IPU. You can observe the compilation process as part of output of the above command.</p> <pre><code>Graph compilation:   3%|\u258e         | 3/100 [00:00&lt;00:03]2023-04-26T16:53:21.225944Z PL:POPLIN    3680893.3680893 W: poplin::preplanMatMuls() is deprecated! Use poplin::preplan() instead\nGraph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:20&lt;00:00]2023-04-26T16:53:38.241395Z popart:session 3680893.3680893\n</code></pre> <p>The artifacts from the graph compilations is cached in the location set by the flag <code>POPTORCH_CACHE_DIR</code>, where the <code>.popef</code> file corresponding to the model under consideration is cached.</p>"},{"location":"ai-testbed/graphcore/running-a-model-or-program/#output","title":"Output","text":"<p>The expected output will start with downloads followed by and we can observe the model used by the model, the progress bar of the compilation process, and the training progress bar.</p> <pre><code>srun: job 10671 queued and waiting for resources\nsrun: job 10671 has been allocated resources\nTrainingModelWithLoss(\n  (model): Network(\n    (layer1): Block(\n      (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (relu): ReLU()\n    )\n    (layer2): Block(\n      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (relu): ReLU()\n    )\n    (layer3): Linear(in_features=1600, out_features=128, bias=True)\n    (layer3_act): ReLU()\n    (layer3_dropout): Dropout(p=0.5, inplace=False)\n    (layer4): Linear(in_features=128, out_features=10, bias=True)\n    (softmax): Softmax(dim=1)\n  )\n  (loss): CrossEntropyLoss()\n)\nEpochs:   0%|          | 0/10 [00:00&lt;?,[23:27:06.753] [poptorch:cpp] [warning] [DISPATCHER] Type coerced from Long to Int for tensor id 10\nGraph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00]\nEpochs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:17&lt;00:00,  7.71s/it]\nGraph compilation: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00]                          \nAccuracy on test set: 96.85%\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00]\n</code></pre> <p>Refer to the script to learn more about this example.</p> <p>Example Programs lists the different example applications with corresponding commands for each of the above steps.</p>"},{"location":"ai-testbed/graphcore/system-overview/","title":"System Overview","text":"<p>The Graphcore Bow-Pod64 system is the latest-generation AI accelerator from Graphcore. This is a one-rack system consisting of 64 Bow-class Intelligence Processing Units (IPU) with a custom interconnect. The system provides for an aggregate 22 Petaflops/s of performance in half precision. It has a total of 57.6 GB In-Processor-Memory with a total of 94,208 IPU cores. The system consists of four servers for data-processing. </p> <p>For more details refer to the POD64 spec </p> <p> (Figure from https://www.graphcore.ai/products/poplar)</p> <p>The Graphcore software stack includes support for TensorFlow and PyTorch using the Poplar SDK. The Poplar\u00ae SDK is t is the toolchain specifically designed for creating graph software for ML applications.  It integrates with the traditional ML frameworks like PyTorch and TensorFlow allowing users to port their existing code to the IPU hardware-specific code. The various components of the poplar SDK stack are shown in the figure. It includes the PopTorch framework which is a wrapper over the PyTorch framework optimized to the IPU hardware. It also enlists the different PopLibs libraries supported, which enables to construct graphs, define tensor data and control how the code and data are mapped onto the IPU for execution.  </p>"},{"location":"ai-testbed/graphcore/virtual-environments/","title":"Virtual Environments","text":""},{"location":"ai-testbed/graphcore/virtual-environments/#poplar-sdk-setup","title":"Poplar SDK Setup","text":"<p>The Poplar SDK is downloaded onto the graphcore systems at the <code>/software/graphcore/poplar_sdk/</code> location. The default poplar version (3.3.0) is enabled automatically upon logging into a graphcore node.</p> <p>Check if Poplar is setup correctly:</p> <pre><code>popc --version\n</code></pre> <p>One should see:</p> <pre><code>POPLAR version 3.3.0 (de1f8de2a7)\nclang version 16.0.0 (2fce0648f3c328b23a6cbc664fc0dd0630122212)\n</code></pre> <p>If the Poplar SDK is not enabled, it can be enabled with <pre><code>source /software/graphcore/poplar_sdk/3.3.0/enable\n</code></pre></p> <p>To disable the current Poplar SDK, e.g. if one wants to use a different Poplar SDK, follow the steps below. (Otherwise, skip to section Miscellaneous Environment Variables.) This example assumes that the current installed SDK is 3.1.0 and you want to move to 3.3.0</p> <ol> <li>Check the current version    <pre><code> $ popc --version\n POPLAR version 3.1.0 (e12d5f9f01)\n clang version 15.0.0 (bab932b4fc4cdb58bb009370384b2c41579bd9d9)\n</code></pre></li> <li>Unset the current version    <pre><code>unset POPLAR_SDK_ENABLED\n</code></pre></li> <li>Enable poplar and popart    <pre><code>source /software/graphcore/poplar_sdk/3.3.0/poplar-ubuntu_20_04-3.3.0+7857-b67b751185/enable.sh \nsource /software/graphcore/poplar_sdk/3.3.0/popart-ubuntu_20_04-3.3.0+7857-b67b751185/enable.sh \n</code></pre></li> <li>Recheck for the new version.    <pre><code>$popc --version\nPOPLAR version 3.3.0 (de1f8de2a7)\nclang version 16.0.0 (2fce0648f3c328b23a6cbc664fc0dd0630122212)\n</code></pre></li> <li> <p>Set SDK env variable    <pre><code>POPLAR_SDK_ROOT=/software/graphcore/poplar_sdk/3.3.0/\nexport POPLAR_SDK_ROOT=$POPLAR_SDK_ROOT\n</code></pre></p> </li> <li> <p>Create a new virtual environment with this SDK and install popTorch and or other frameworks as needed.    <pre><code>virtualenv ~/Graphcore/workspace/poptorch33_env\nsource ~/Graphcore/workspace/poptorch33_env/bin/activate\npip install $POPLAR_SDK_ROOT/poptorch-3.3.0+113432_960e9c294b_ubuntu_20_04-cp38-cp38-linux_x86_64.whl\nexport PYTHONPATH=$POPLAR_SDK_ROOT/python:$PYTHONPATH\n</code></pre></p> </li> </ol>"},{"location":"ai-testbed/graphcore/virtual-environments/#miscellaneous-environment-variables","title":"Miscellaneous Environment Variables","text":"<pre><code>mkdir ~/tmp\nexport TF_POPLAR_FLAGS=--executable_cache_path=~/tmp\nexport POPTORCH_CACHE_DIR=~/tmp\n\nexport POPART_LOG_LEVEL=WARN\nexport POPLAR_LOG_LEVEL=WARN\nexport POPLIBS_LOG_LEVEL=WARN\n\nexport PYTHONPATH=/software/graphcore/poplar_sdk/3.3.0/poplar-ubuntu_20_04-3.3.0+7857-b67b751185/python:$PYTHONPATH\n</code></pre>"},{"location":"ai-testbed/graphcore/virtual-environments/#poptorch-environment-setup","title":"PopTorch Environment Setup","text":"<p>PopTorch is an extension of the Pytorch framework that is optimized for the IPU specific functionality. To activate the PopTorch environment, first create a virtual environment and activate it.</p> <pre><code>mkdir -p ~/venvs/graphcore\nvirtualenv ~/venvs/graphcore/poptorch33_env\nsource ~/venvs/graphcore/poptorch33_env/bin/activate\n</code></pre> <p>Use the following commands to install the PopTorch environment.</p> <pre><code>POPLAR_SDK_ROOT=/software/graphcore/poplar_sdk/3.3.0\nexport POPLAR_SDK_ROOT=$POPLAR_SDK_ROOT\npip install $POPLAR_SDK_ROOT/poptorch-3.3.0+113432_960e9c294b_ubuntu_20_04-cp38-cp38-linux_x86_64.whl\n</code></pre>"},{"location":"ai-testbed/graphcore/virtual-environments/#tensorflow-2-environment-setup","title":"TensorFlow 2 Environment Setup","text":"<p>The Poplar SDK provides TensorFlow and Keras wheels built on 2.6 that includes the IPU specific functionality and optimized for the AMD processors. It can be installed as follows.</p> <p>Create virtual environment.</p> <pre><code>virtualenv ~/venvs/graphcore/tensorflow2_33_env\nsource ~/venvs/graphcore/tensorflow2_33_env/bin/activate\n</code></pre> <p>Install the TensorFlow and Keras wheels.</p> <pre><code>POPLAR_SDK_ROOT=/software/graphcore/poplar_sdk/3.3.0\nexport POPLAR_SDK_ROOT=$POPLAR_SDK_ROOT\npip install $POPLAR_SDK_ROOT/tensorflow-2.6.3+gc3.3.0+251580+08d96978c7f+amd_znver1-cp38-cp38-linux_x86_64.whl\npip install $POPLAR_SDK_ROOT/keras-2.6.0+gc3.3.0+251582+a3785372-py2.py3-none-any.whl\n</code></pre>"},{"location":"ai-testbed/graphcore/virtual-environments/#verify-installation","title":"Verify Installation","text":"<pre><code>python -c \"from tensorflow.python import ipu\"\n</code></pre> <p>You should see:</p> <pre><code>2023-08-22 21:53:26.109934: I tensorflow/compiler/plugin/poplar/driver/poplar_platform.cc:43] Poplar version: 3.3.0 (de1f8de2a7) Poplar package: b67b751185\n</code></pre>"},{"location":"ai-testbed/graphcore/virtual-environments/#installing-packages","title":"Installing Packages","text":"<p>Install packages in the normal manner such as:</p> <pre><code>python3 -m pip install \"some_package\"\n</code></pre> <p>For more details see Use pip for installing.</p> <p>To install a different version of a package that is already installed in one's environment, one can use:</p> <pre><code>pip install --ignore-installed  ... # or -I\n</code></pre> <p>Note: Conda is not supported on the Graphcore system.</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/","title":"Scaling ResNet50","text":"<p>Follow all the instructions in Getting Started to log into a Graphcore node.</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#examples-repo","title":"Examples Repo","text":"<p>Graphcore provides examples of some well-known AI applications in their repository at https://github.com/graphcore/examples.git.</p> <p>Clone the examples repository to your personal directory structure:</p> <pre><code>mkdir ~/graphcore\ncd ~/graphcore\ngit clone https://github.com/graphcore/examples.git\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#environment-setup","title":"Environment Setup","text":"<p>Establish a virtual environment.</p> <pre><code>mkdir -p ~/venvs/graphcore\nrm -rf ~/venvs/graphcore/poptorch31_rn50_env\nvirtualenv ~/venvs/graphcore/poptorch31_rn50_env\nsource ~/venvs/graphcore/poptorch31_rn50_env/bin/activate\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#install-poptorch","title":"Install PopTorch","text":"<p>Install PopTorch.</p> <pre><code>POPLAR_SDK_ROOT=/software/graphcore/poplar_sdk/3.1.0\nexport POPLAR_SDK_ROOT=$POPLAR_SDK_ROOT\npip install $POPLAR_SDK_ROOT/poptorch-3.1.0+98660_0a383de63f_ubuntu_20_04-cp38-cp38-linux_x86_64.whl\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#environment-variables","title":"Environment Variables","text":"<p>Establish the following environment variables.</p> <pre><code>mkdir ${HOME}/tmp\nexport TF_POPLAR_FLAGS=--executable_cache_path=${HOME}/tmp\nexport POPTORCH_CACHE_DIR=${HOME}/tmp\nexport POPART_LOG_LEVEL=WARN\nexport POPLAR_LOG_LEVEL=WARN\nexport POPLIBS_LOG_LEVEL=WARN\nexport PYTHONPATH=/software/graphcore/poplar_sdk/3.1.0/poplar-ubuntu_20_04-3.1.0+6824-9c103dc348/python:$PYTHONPATH\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#install-requirements","title":"Install Requirements","text":"<pre><code>cd ${HOME}/graphcore/examples/vision/cnns/pytorch/\nmake install\nmake install-turbojpeg\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#one-time-per-user-ssh-key-set-up","title":"One-time per user ssh key set up","text":"<p>Set up the ssh key on gc-poplar-01.</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#gc-poplar-01","title":"Gc-poplar-01","text":"<p>On gc-poplar-01:</p> <pre><code>mkdir ~/.ssh\ncd ~/.ssh\nssh-keygen -t rsa -b 4096\n#Accecpt default filename of id_rsa\n#Enter passphrase (empty for no passphrase):\n#Enter same passphrase again:\ncat id_rsa.pub &gt;&gt; authorized_keys\n</code></pre> <pre><code>ssh-keyscan -H gc-poplar-01 &gt;&gt; ~/.ssh/known_hosts\n</code></pre> <p>You should see:</p> <pre><code># gc-poplar-01:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-01:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-01:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-01:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-01:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n</code></pre> <pre><code>ssh-keyscan -H gc-poplar-02 &gt;&gt; ~/.ssh/known_hosts\n</code></pre> <p>You should see:</p> <pre><code># gc-poplar-02:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-02:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-02:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-02:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-02:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n</code></pre> <pre><code>ssh-keyscan -H gc-poplar-03 &gt;&gt; ~/.ssh/known_hosts\n</code></pre> <p>You should see:</p> <pre><code># gc-poplar-03:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-03:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-03:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-03:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-03:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n</code></pre> <pre><code>ssh-keyscan -H gc-poplar-04 &gt;&gt; ~/.ssh/known_hosts\n</code></pre> <p>You should see:</p> <pre><code># gc-poplar-04:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-04:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-04:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-04:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n# gc-poplar-04:22 SSH-2.0-OpenSSH_8.2p1 Ubuntu-4ubuntu0.5\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#benchmarksyml","title":"<code>benchmarks.yml</code>","text":"<p>Update ${HOME}/graphcore/examples/vision/cnns/pytorch/train/benchmarks.yml with your favorite editor to match benchmarks.yml.</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#configsyml","title":"<code>configs.yml</code>","text":"<p>Update ${HOME}/graphcore/examples/vision/cnns/pytorch/train/configs.yml with your favorite editor.  At about line 30, change use_bbox_info: true to use_bbox_info: false.</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#scale-resnet50","title":"Scale ResNet50","text":"<p>Scale and benchmark ResNet50.</p> <p>Note: The number at the end of each line indicates the number of IPUs.</p> <p>Note: Use screen because every run is long.</p> <p>\"PopRun exposes this control with the --process-placement flag and provides multiple pre-defined strategies. By default (and with --process-placement spreadnuma), PopRun is designed to be NUMA-aware. On each host, all the available NUMA nodes are divided among the instances. This means that each instance is bound to execute on and allocate memory from its assigned NUMA nodes, ensuring memory access locality. This strategy maximises memory bandwidth and is likely to yield optimal performance for most of the data loading workloads in machine learning.\" [Multi-Instance Multi-Host(https://docs.graphcore.ai/projects/poprun-user-guide/en/latest/launching.html#multi-instance-multi-host)</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#setup","title":"Setup","text":"<p>Move to the correct directory and establish the datasets directory.</p> <pre><code>cd ${HOME}/graphcore/examples/vision/cnns/pytorch/train\nexport DATASETS_DIR=/mnt/localdata/datasets/\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#scaling-to-16-ipus","title":"Scaling to 16 IPUs","text":"<p>One may use any of the following commands to run ResNet50 on one to sixteen IPUs.</p> <pre><code>python3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_1\npython3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_2\npython3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_4\npython3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_8\npython3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_pod16\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#scaling-to-64-ipus","title":"Scaling to 64 IPUs","text":"<p>Note: One must complete the instructions on Multi-node Setup before running this example.</p>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#establish-environment-variables","title":"Establish Environment Variables","text":"<pre><code>HOST1=`ifconfig eno1 | grep \"inet \" | grep -o '[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}' | head -1`\nOCT123=`echo \"$HOST1\" | cut -d \".\" -f 1,2,3`\nOCT4=`echo \"$HOST1\" | cut -d \".\" -f 4`\nHOST2=$OCT123.`expr $OCT4 + 1`\nHOST3=$OCT123.`expr $OCT4 + 2`\nHOST4=$OCT123.`expr $OCT4 + 3`\nexport HOSTS=$HOST1,$HOST2,$HOST3,$HOST4\nexport CLUSTER=c16\nexport IPUOF_VIPU_API_PARTITION_ID=p64\nexport TCP_IF_INCLUDE=$OCT123.0/8\nexport IPUOF_VIPU_API_HOST=$HOST1\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#64-ipu-run","title":"64 IPU Run","text":"<p>This runs to convergence.  It uses all 64 IPUs for more than 12 hours.</p> <p>Note: This should only be used if absolutely required.</p> <p>Execute:</p> <pre><code>python3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_pod64\npython3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_pod64_conv\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#benchmark-results","title":"Benchmark Results","text":""},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#one-ipu","title":"One IPU","text":"<pre><code>[INFO] 2022-12-16 17:07:32: Total runtime: 3956.836479 seconds\n[INFO] 2022-12-16 17:07:32:    throughput = '7527.626315789474'\n[INFO] 2022-12-16 17:07:32:    accuracy = '57.41'\n[INFO] 2022-12-16 17:07:32:    loss = '2.8153'\n[INFO] 2022-12-16 17:07:33:    Total compile time: 429.59 seconds\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#two-ipus","title":"Two IPUs","text":"<pre><code>[INFO] 2022-12-16 15:56:23: Total runtime: 5866.494071 seconds\n[INFO] 2022-12-16 15:56:23:    throughput = '4798.778947368421'\n[INFO] 2022-12-16 15:56:23:    accuracy = '68.23'\n[INFO] 2022-12-16 15:56:23:    loss = '2.3148'\n[INFO] 2022-12-16 15:56:24:    Total compile time: 418.75 seconds\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#four-ipus","title":"Four IPUs","text":"<pre><code>[INFO] 2022-12-16 04:05:28: Total runtime: 3070.994553 seconds\n[INFO] 2022-12-16 04:05:28:    throughput = '9959.821052631578'\n[INFO] 2022-12-16 04:05:28:    accuracy = '67.76'\n[INFO] 2022-12-16 04:05:28:    loss = '2.338'\n[INFO] 2022-12-16 04:05:29:    Total compile time: 377.4 seconds\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#eight-ipus","title":"Eight IPUs","text":"<pre><code>[INFO] 2022-12-16 02:46:45: Total runtime: 1831.437598 seconds\n[INFO] 2022-12-16 02:46:45:    throughput = '19865.263157894733'\n[INFO] 2022-12-16 02:46:45:    accuracy = '64.94'\n[INFO] 2022-12-16 02:46:45:    loss = '2.4649'\n[INFO] 2022-12-16 02:46:46:    Total compile time: 386.27 seconds\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#sixteen-ipus","title":"Sixteen IPUs","text":"<p>Epochs: 20</p> <pre><code>[INFO] 2022-12-15 22:01:14: Total runtime: 1297.274336 seconds\n[INFO] 2022-62:01:14:    throughput = '39057.447368421046'\n[INFO] 2022-12-15 22:01:14:    accuracy = '57.43'\n[INFO] 2022-12-15 22:01:14:    loss = '2.8162'\n[INFO] 2022-12-15 22:01:16:    Total compile time: 397.08 seconds\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/Scaling-ResNet50/#sixty-four-ipus","title":"Sixty-Four IPUs","text":"<pre><code>[1,0]&lt;stdout&gt;:[INFO] loss: 4.8367,\n[1,0]&lt;stdout&gt;:[INFO] accuracy: 18.83 %\n[1,0]&lt;stdout&gt;:[INFO] throughput: 51368.5 samples/sec\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/","title":"CosmicTagger Conversion","text":"<p>The intent of this page is to show conceptually how to convert a model to run on the Graphcore system. It is not necessary to convert CosmicTagger because it has already been converted and is located at CosmicTagger on the Graphcore branch. The original is located at CosmicTagger.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#run-model-on-cpu","title":"Run Model on CPU","text":"<p>The first step to converting a model is to verify that it runs on the CPU.  This step has been verified for CosmicTagger.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#configpy","title":"Config.py","text":"<p>CosmicTagger can run on multiple machines.  As such, it is necessary to specify the architecture that one is using.  For example, CPU or GPU.  The architecture is stored in the ComputeMode class.</p> <p>Edit src/config/config.py.  Add IPU to the ComputeMode class.</p> <pre><code>class ComputeMode(Enum):\n    CPU   = 0\n    #...\n    IPU   = 5\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#trainerpy","title":"Trainer.py","text":"<p>Edit src/utils/torch/trainer.py.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#import-poptorch","title":"Import PopTorch","text":"<p>PopTorch is Graphcore's extension of PyTorch.</p> <p>Import poptorch at the top of the file.</p> <pre><code>import poptorch\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#wrap-model","title":"Wrap Model","text":"<p>Wrap the model using poptorch.trainingModel() so that it may be ran on IPUs for training.</p> <p>Wrap the model using poptorch.inferenceModel() when not training.</p> <p>Find the following code around line 90 in the init_network method.</p> <pre><code>        # Foregoing any fusions as to not disturb the existing ingestion pipeline\n        if self.is_training() and self.args.mode.quantization_aware:\n            self._raw_net.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n            self._net = torch.quantization.prepare_qat(self._raw_net)\n        else:\n            self._net = self._raw_net\n</code></pre> <p>After the above code, add:</p> <pre><code>        if self.args.run.compute_mode == ComputeMode.IPU:\n            if self.is_training():\n                opts = poptorch.Options()\n                self._net = poptorch.trainingModel(self._net, opts, optimizer=torch.optim.SGD(self._net.parameters(), lr=1e-3))\n            else:\n                self._net = poptorch.inferenceModel(self._net)\n</code></pre> <p>See poptorch.trainingModel() and poptorch.inferenceModel() for more information.</p> <p>There is also a Build the Model tutorial.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#update-optimizer","title":"Update Optimizer","text":"<p>Update init_optimizer() to use the poptorch class instead of the torch class as needed.</p> <p>Change:</p> <pre><code>        if self.args.mode.optimizer.name == OptimizerKind.rmsprop:\n            self._opt = torch.optim.RMSprop(self._net.parameters(), 1.0, eps=1e-4)\n        else:\n            self._opt = torch.optim.Adam(self._net.parameters(), 1.0)\n</code></pre> <p>to:</p> <pre><code>        if self.args.mode.optimizer.name == OptimizerKind.rmsprop:\n            if self.args.run.compute_mode == ComputeMode.IPU:\n                self._opt = poptorch.optim.RMSprop(self._net.parameters(), 1.0, eps=1e-4)\n            else:\n                self._opt = torch.optim.RMSprop(self._net.parameters(), 1.0, eps=1e-4)\n        else:\n            if self.args.run.compute_mode == ComputeMode.IPU:\n                self._opt = poptorch.optim.Adam(self._net.parameters(), 1.0)\n            else:\n                self._opt = torch.optim.Adam(self._net.parameters(), 1.0)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#update-the-forward-pass","title":"Update the Forward Pass","text":"<p>Putting the loss calculation in forward_pass() allows the loss computation to be performed on the IPUs. This will be faster because the data will not need to be transfered round-trip to the CPU.</p> <p>Change forward_pass():</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#original","title":"Original","text":"<pre><code>            if net is None:\n                logits_image = self._net(minibatch_data['image'])\n            else:\n                logits_image = net(minibatch_data['image'])\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#updated","title":"Updated","text":"<p>The following code changes are to account for the loss function, i.e., self.loss_calculator, and the image labels, i.e., labels_image, to be passed to the model's forward_pass method.  Additionally, the calculated loss is returned from the forward_pass method.</p> <pre><code>            if net is None:\n                if self.args.run.compute_mode == ComputeMode.IPU:\n                    logits_image, labels_image, loss = self._net(minibatch_data['image'], self.loss_calculator, labels_image)\n                    return logits_image, labels_image, loss\n                else:\n                    logits_image = self._net(minibatch_data['image'])\n            else:\n                if self.args.run.compute_mode == ComputeMode.IPU and self.args.mode.name != ModeKind.inference:\n                    logits_image, labels_image, loss = net(minibatch_data['image'], self.loss_calculator, labels_image)\n                    return logits_image, labels_image, loss\n                else:\n                    logits_image = net(minibatch_data['image'])\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#update-the-training-step","title":"Update the Training Step","text":"<p>Receive the extra loss variable from the forward_pass method.</p> <p>Update the train_step method.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#original-training-step","title":"Original Training Step","text":"<pre><code>                    with self.timing_context(\"forward\"):\n                        if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\n                            with torch.cuda.amp.autocast():\n                                logits_image, labels_image = self.forward_pass(minibatch_data)\n                        else:\n                            logits_image, labels_image = self.forward_pass(minibatch_data)\n\n                    verbose = False\n\n                    # Compute the loss based on the logits\n                    with self.timing_context(\"loss\"):\n                        loss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#updated-training-step","title":"Updated Training Step","text":"<p>The forward_pass() method was changed to return the extra variable loss in the previous section.  It is now received conditionally when using an IPU(s).</p> <p>In the with self.timing_context(\"loss\"): section, only calculate loss if not using an IPU(s).</p> <pre><code>                    with self.timing_context(\"forward\"):\n                        if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\n                            with torch.cuda.amp.autocast():\n                                logits_image, labels_image = self.forward_pass(minibatch_data)\n                        else:\n                            if self.args.run.compute_mode == ComputeMode.IPU:\n                                logits_image, labels_image, loss = self.forward_pass(minibatch_data)\n                            else:\n                                logits_image, labels_image = self.forward_pass(minibatch_data)\n\n                    verbose = False\n\n\n                    # Compute the loss based on the logits\n                    with self.timing_context(\"loss\"):\n                        if self.args.run.compute_mode == ComputeMode.IPU:\n                            loss = loss\n                        else:\n                            loss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#update-validation-step","title":"Update Validation Step","text":"<p>Update the val_step method.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#original-validation-step-code","title":"Original Validation Step Code","text":"<p>Find this code.</p> <pre><code>            if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\n                with torch.cuda.amp.autocast():\n                    logits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n            else:\n                logits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n\n            # Compute the loss based on the logits\n            loss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#updated-validation-step-code","title":"Updated Validation Step Code","text":"<p>Change the code to the following.</p> <pre><code>            if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\n                with torch.cuda.amp.autocast():\n                    logits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n\n                    # Compute the loss based on the logits\n                    loss = self.loss_calculator(labels_image, logits_image)\n            else:\n                if self.args.run.compute_mode == ComputeMode.IPU:\n                    logits_image, labels_image, loss = self.forward_pass(minibatch_data, net=val_net)\n                else:\n                    logits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n\n                    # Compute the loss based on the logits\n                    loss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#uresnet2d-model","title":"UResNet2D Model","text":""},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#update-model","title":"Update Model","text":"<p>The Graphcore system is more computationally efficient if the loss function is on the IPU.  This is accomplished by using the loss function within the model's forward method.</p> <p>Edit src/networks/torch/uresnet2D.py.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#update-the-forward-declaration","title":"Update the Forward Declaration","text":"<p>Find the forward method.</p> <pre><code>def forward(self, input_tensor):\n</code></pre> <p>Update the argument list to include the loss function, i.e., loss_calculator and the image labels, i.e., labels_image.</p> <pre><code>def forward(self, input_tensor, loss_calculator=None, labels_image=None):\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#add-loss-calculation","title":"Add Loss Calculation","text":"<p>Add the loss calculation just before the forward method returns.</p> <pre><code>        if loss_calculator is not None:\n\n            labels_image = labels_image.long()\n            labels_image = torch.chunk(labels_image, chunks=3, dim=1)\n            shape =  labels_image[0].shape\n            labels_image = [ _label.view([shape[0], shape[-2], shape[-1]]) for _label in labels_image ]\n\n            loss = loss_calculator(labels_image, x)\n            import poptorch\n            loss = poptorch.identity_loss(loss , reduction=\"mean\")\n            return x, labels_image, loss\n\n        # This return already exists.\n        return x\n</code></pre> <p>The poptorch.identity_loss method takes a single PyTorch tensor and will backpropagate a gradient of ones through it.  You may find an example at here</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-conversion/#binexecpy","title":"bin/exec.py","text":"<p>The following is included for completeness.  One will not likely find this in other code.</p> <p>Open bin/exec.py in your favorite editor.  Change:</p> <pre><code>@hydra.main(version_base=None, config_path=\"../src/config\", config_name=\"config\")\n</code></pre> <p>to</p> <pre><code>@hydra.main(config_path=\"../src/config\", config_name=\"config\")\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/","title":"CosmicTagger Conversion","text":"<p>The intent of this page is to show conceptually how to convert a Graphcore model to run on Distributed Data Parallel using PopDist. It is not necessary to convert CosmicTagger because it has already been converted and is located at CosmicTagger on the GraphcoreDDP branch. The original is located at CosmicTagger.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#run-model-on-cpu","title":"Run Model on CPU","text":"<p>The first step to converting a model is to verify that it runs on the CPU.  This step has been verified for CosmicTagger.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#starter-code","title":"Starter Code","text":"<p>You may use the code at CosmicTagger on the Graphcore branch.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#trainerpy","title":"Trainer.py","text":"<p>Edit src/utils/torch/trainer.py.</p>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#import-poplar-packages","title":"Import Poplar Packages","text":"<p>PopTorch is Graphcore's extension of PyTorch.</p> <p>PopDist is Graphcore's distributed processing package.</p> <p>Import poptorch and popdist at the top of the file.</p> <pre><code>try:\n    import poptorch\n    import popdist\n    import popdist.poptorch\nexcept:\n    pass\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#initialization","title":"Initialization","text":"<p>Initialize popdist for distributed computing.</p> <p>Establish a class variable name instance.  This is used to differentiate between different model instances that will be saved.</p> <p>Add the following line at the bottom of init().</p> <pre><code>        if self.args.run.compute_mode == ComputeMode.IPU and popdist.isPopdistEnvSet():\n            popdist.init()\n            self._instance = popdist.getInstanceIndex()\n        else:\n            self._instance = 0\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#use-instance-variable","title":"Use Instance Variable","text":"<p>Use the instance variable for the model file name.</p> <p>Find def get_model_filepath.</p> <p>Change:</p> <pre><code>        name = file_path + 'model-{}.ckpt'.format(self._global_step)\n</code></pre> <p>To:</p> <pre><code>        name = file_path + f'model-{self._global_step}-{self._instance}.ckpt'\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#establish-logging-method","title":"Establish Logging Method","text":"<p>Add a helper function to log data at the bottom of the file.</p> <pre><code>    def log_in_single_instance(self, string):\n        if self.args.run.compute_mode == ComputeMode.IPU:\n            if not popdist.isPopdistEnvSet() or popdist.getInstanceIndex() == 0:\n                logging.info(string)\n        else:\n            logging.info(string)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#update-init_network","title":"Update Init_network()","text":"<p>PopTorch has an Option() method which returns values that get passed to poptorch.trainingModel. The returned values are stored in opts in this example.</p> <p>Find:</p> <pre><code>        if self.args.run.compute_mode == ComputeMode.IPU:\n            if self.is_training():\n                opts = poptorch.Options()\n                self._net = poptorch.trainingModel(self._net, opts, optimizer=torch.optim.SGD(self._net.parameters(), lr=1e-3))\n            else:\n                self._net = poptorch.inferenceModel(self._net)\n</code></pre> <p>Replace it with:</p> <pre><code>        if self.args.run.compute_mode == ComputeMode.IPU:\n            if popdist.isPopdistEnvSet():\n                opts = popdist.poptorch.Options()\n                # When using the dataloader with 'auto_distributed_partitioning=True'\n                # and 'shuffle=True' we must set the random seed to ensure that tensors\n                # are in the same order in all processes.\n                opts.randomSeed(42)\n                # Replication factor is already set via PopRun so\n                # we ignore 'args.num_replicas'.\n                logging.info(f\"Num of local replicas: {popdist.getNumLocalReplicas()}\")\n            else:\n                opts = poptorch.Options()\n                opts.replicationFactor(self.args.num_replicas)\n\n            if self.is_training():\n                self._net = poptorch.trainingModel(self._net, opts, optimizer=torch.optim.SGD(self._net.parameters(), lr=1e-3))\n            else:\n                self._net = poptorch.inferenceModel(self._net)\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/cosmictagger-ddp/#run-the-code","title":"Run The Code","text":"<p>See instructions in README_GRAPHCORE.md.</p>"},{"location":"ai-testbed/graphcore/unused/multi-node-setup/","title":"Multi-node Setup","text":"<p>These steps only need to be executed once per user.</p> <p>Running on multiple nodes is a three step process.</p> <ol> <li> <p>Create a Key</p> <pre><code>cd ~/.ssh\nssh-keygen -t rsa -b 4096\n</code></pre> </li> <li> <p>Put Key into Authorized_keys File</p> <pre><code>cat id_rsa.pub &gt;&gt; authorized_keys\n</code></pre> </li> <li> <p>Add Node IP Addresses to Known_hosts File</p> <pre><code>ssh-keyscan -H 10.1.3.101 &gt;&gt; ~/.ssh/known_hosts\nssh-keyscan -H 10.1.3.102 &gt;&gt; ~/.ssh/known_hosts\nssh-keyscan -H 10.1.3.103 &gt;&gt; ~/.ssh/known_hosts\nssh-keyscan -H 10.1.3.104 &gt;&gt; ~/.ssh/known_hosts\n</code></pre> </li> </ol>"},{"location":"ai-testbed/graphcore/unused/profiling-mnist/","title":"Profiling MNIST","text":"<p>Follow all the instructions in Getting Started to log into a Graphcore node.</p> <p>Follow the instructions in Virtual Environments up to and including PopART Environment Setup.</p> <p>Following the instructions in Example Programs up to and including MNIST, Install Requirements.</p>"},{"location":"ai-testbed/graphcore/unused/profiling-mnist/#change-directory","title":"Change Directory","text":"<pre><code>cd ~/graphcore/tutorials/simple_applications/pytorch/mnist\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling-mnist/#set-poplar-options","title":"Set Poplar Options","text":"<p>Set the option to generate all reports, i.e., \"autoReport.all\":\"true\".</p> <p>Set the reports directory, i.e., \"autoReport.directory\":\"./reports\".</p> <p>Do so by running the following commands:</p> <pre><code>export POPLAR_ENGINE_OPTIONS='{\"autoReport.all\":\"true\", \"autoReport.directory\":\"./reports\"}'\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling-mnist/#run-mnist","title":"Run MNIST","text":"<p>Do so by running the following command:</p> <pre><code>python mnist_poptorch.py\n</code></pre> <p>When MNIST has finished running, see Profiling to use Graph Analyser.</p>"},{"location":"ai-testbed/graphcore/unused/profiling-resnet50/","title":"Profiling ResNet50","text":"<p>Follow all the instructions in Getting Started to log into a Graphcore node.</p> <p>Follow the instructions in Virtual Environments up to and including PopART Environment Setup.</p>"},{"location":"ai-testbed/graphcore/unused/profiling-resnet50/#examples-repo","title":"Examples Repo","text":"<p>Graphcore provides examples of some well-known AI applications in their repository at https://github.com/graphcore/examples.git.</p> <p>Clone the examples repository to your personal directory structure:</p> <pre><code>mkdir ~/graphcore\ncd ~/graphcore\ngit clone https://github.com/graphcore/examples.git\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling-resnet50/#install-requirements","title":"Install Requirements","text":"<p>Change directory</p> <pre><code>cd ~/graphcore/examples/vision/cnns/pytorch\npython -m pip install -r requirements.txt\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling-resnet50/#export-variables","title":"Export Variables","text":"<p>Export the datasets directory.</p> <pre><code>export POPLAR_ENGINE_OPTIONS='{\"autoReport.all\":\"true\", \"autoReport.directory\":\"./reports\"}'\nexport DATASETS_DIR=/software/datasets\nHOST1=`ifconfig eno1 | grep \"inet \" | grep -o '[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}' | head -1`\nOCT123=`echo \"$HOST1\" | cut -d \".\" -f 1,2,3`\nOCT4=`echo \"$HOST1\" | cut -d \".\" -f 4`\nHOST2=$OCT123.`expr $OCT4 + 1`\nHOST3=$OCT123.`expr $OCT4 + 2`\nHOST4=$OCT123.`expr $OCT4 + 3`\nexport HOSTS=$HOST1,$HOST2,$HOST3,$HOST4\nexport CLUSTER=c16\nVIPU_SERVER=${VIPU_SERVER:=$HOST1}\nFIRST_PARTITION=`vipu-admin list partitions --api-host $VIPU_SERVER| grep ACTIVE | cut -d '|' -f 3 | cut -d ' ' -f 2 | head -1`\nPARTITON=${PARTITION:=$FIRST_PARTITION}\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling-resnet50/#profile-resnet50","title":"Profile ResNet50","text":"<p>Profile ResNet50.</p> <p>Note: Use screen because every run is long.</p> <pre><code>cd train\npython3 -m examples_utils benchmark --spec benchmarks.yml --benchmark pytorch_resnet50_train_real_pod16\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling-resnet50/#profile-results","title":"Profile Results","text":"<p>When ResNet50 has finished running, see Profiling to use Graph Analyser.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/","title":"Profiling","text":"<p>This is an adaptation of Capturing IPU Reports.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/#reports","title":"Reports","text":""},{"location":"ai-testbed/graphcore/unused/profiling/#capturing-ipu-reports","title":"Capturing IPU Reports","text":"<p>See Capturing IPU Reports for more information.</p> <p>This section describes how to generate the files that the Graph Analyser can analyze. The Graph Analyser uses report files generated during compilation and execution by the Poplar SDK.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/#ipu-memory-overhead","title":"IPU Memory Overhead","text":"<p>Because of all these extra memory requirements, a model with high memory consumption may go out of memory when profiling is enabled. Depending on the model, you can adjust its parameters to leave space for the instrumentation. For example, you can try decreasing the batch size. In TensorFlow BERT you can adjust the micro batch-size.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/#host-computing-overhead","title":"Host Computing Overhead","text":"<p>It is essential that you also try to reduce the iterations on each run. For instance, by reducing the number of steps or the number of batches per step you can get a lighter execution profile. This will not only reduce the host computation overhead but will also speed up visualization in the Graph Analyser.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/#download-popvision","title":"Download PopVision","text":"<ol> <li> <p>Download PopVision Tools.</p> </li> <li> <p>Click Download Now button.</p> </li> <li> <p>In the Graph Analyser section, select you operating system.</p> </li> <li> <p>Install per selected operating system.</p> </li> </ol>"},{"location":"ai-testbed/graphcore/unused/profiling/#create-ssh-session","title":"Create SSH Session","text":"<p>Use ssh from your development system.</p> <p>The ssh command will use a jumphost and port forwarding.  The format is as follows:</p> <pre><code>ssh -J ALCFUserID@gc-login-dd.ai.alcf.anl.gov ALCFUserID@gc-poplar-DD -L 8090:127.0.0.1:22\nssh -J wilsonb@gc-login-01.ai.alcf.anl.gov wilsonb@gc-poplar-02.ai.alcf.anl.gov -L 8090:127.0.0.1:22\n</code></pre> <p>Where:</p> Argument Help ALCFUserID Is your ALCF user identification. dd Is the Graphcore login node to use, i.e., 01 or 02 DD Is the Graphcore node to use, i.e., 01, 02, 03, or 04. 8090 Is the port on your local machine. 127.0.0.1:22 Is the local IP address and port on the remote machine. <p>You will receive a prompt.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/#launch-graph-analyser","title":"Launch Graph Analyser","text":"<p>Continue on your development machine.</p>"},{"location":"ai-testbed/graphcore/unused/profiling/#operating-system","title":"Operating System","text":""},{"location":"ai-testbed/graphcore/unused/profiling/#ubuntu","title":"Ubuntu","text":"<pre><code>cd /path/to/graph/analyser/directory\n./popvision-graph-analyser-3.11.6.AppImage\n</code></pre>"},{"location":"ai-testbed/graphcore/unused/profiling/#user-interface","title":"User Interface","text":"<ol> <li>Click Open a report...;</li> <li>Click the remote tab;</li> <li>Enter your ALCFUserID for remote machine;</li> <li>Enter the Hostname of your local machine, i.e., 127.0.0.1;</li> <li>Enter your Port address used in the ssh command, e.g., 8090;</li> <li>Click Connect;</li> <li>Navigate to your reports directory;</li> <li>Select the training directory;</li> <li>Select archive.a file; and</li> <li>Click Open button.</li> </ol> <p>The Summary Report will be displayed.</p>"},{"location":"ai-testbed/groq/getting-started/","title":"Getting Started","text":""},{"location":"ai-testbed/groq/getting-started/#allocations","title":"Allocations","text":"<p>If you do not already have an allocation, you will need to request one here: Discretionary Allocation Request (New &amp; Renewal)</p>"},{"location":"ai-testbed/groq/getting-started/#accounts","title":"Accounts","text":"<p>If you do not have an ALCF account (but have an allocation), request one here: ALCF Account and Project Management</p>"},{"location":"ai-testbed/groq/getting-started/#setup","title":"Setup","text":"<p>Connection to a GroqRack node is a two-step process.</p> <p>The first step is to ssh from a local machine to a login node. The second, optional step is to ssh from a login node to a GroqRack node. Jobs may also be started and tracked from login nodes.</p> <p></p>"},{"location":"ai-testbed/groq/getting-started/#log-in-to-a-login-node","title":"Log in to a login node","text":"<p>Connect to a groq login node, editing this command line to use your ALCF user id. You will be prompted for a password; use the 8-digit code provided by  MobilePASS+.  <pre><code>ssh ALCFUserID@groq.ai.alcf.anl.gov\n</code></pre> This randomly selects one of the login nodes, namely <code>groq-login-01.ai.alcf.anl.gov</code> or <code>groq-login-02.ai.alcf.anl.gov</code>. You can alternatively ssh to the specific login nodes directly. </p>"},{"location":"ai-testbed/groq/getting-started/#log-in-to-a-groqrack-node","title":"Log in to a GroqRack node","text":"<p>Once you are on a login node, optionally ssh to one of the GroqRack nodes, which are numbered 1-9.</p> <pre><code>ssh groq-r01-gn-01.ai.alcf.anl.gov\n# or\nssh groq-r01-gn-09.ai.alcf.anl.gov\n# or any node with hostname of form groq-r01-gn-0[1-9].ai.alcf.anl.gov\n</code></pre>"},{"location":"ai-testbed/groq/groqview/","title":"GroqView profiler and visualizer tool","text":"<p>This section covers how to remotely use the GroqView profiler and visualizer tool.</p>"},{"location":"ai-testbed/groq/groqview/#groqview-sample","title":"GroqView sample","text":"<p>Groq compiles produce an accurate and detailed model of the performance of a model's execution on groq cards. There is no need to run a model on groqcards to use GroqView. The GroqView example adds the \"groqview=True\" parameter to the <code>groqit</code> call, then calls the <code>groqview()</code> method on the model returned by <code>groqit</code>. This is the relevant code when using GroqFlow. It tries to retrieve the compiled model from the cache, compiles the model on a cache miss, then calls <code>groqview()</code>. From <code>groqflow/examples/pytorch/groqview.py</code>:  <pre><code># Build model\ngmodel = groqit(pytorch_model, inputs, groqview=True)\n# Open GroqView\ngmodel.groqview()\n</code></pre></p>"},{"location":"ai-testbed/groq/groqview/#run-the-sample","title":"Run the sample","text":"<p>On a groq node, run the groqview.py sample (or any script that includes similar code). Note the port number chosen by GroqView. <pre><code>conda activate groqflow\ncd ~/groqflow/examples/pytorch\npython groqview.py\n# You will see something like the following.\n# The port number may be different.\n...\nOpen your web browser:\n    http://localhost:8439\n</code></pre></p>"},{"location":"ai-testbed/groq/groqview/#forward-the-port-to-your-machine-with-a-browser","title":"Forward the port to your machine with a browser","text":"<p>On your laptop/user machine with a display, set up a 2-hop ssh tunnel. Set <code>$GN_HOSTNAME</code> to the name of the host where job is running <pre><code>export GN_HOSTNAME=groq-r01-gn-09\n# Modify the port number if GroqView has chosen a different port.\n# This might happen if another user is also using GroqView.\n# Also, another user may be using the port on the login host.\n# `groq-login-01.ai.alcf.anl.gov` can be used as well.\nssh -L 8439:localhost:8439 arnoldw@groq-login-02.ai.alcf.anl.gov -t ssh -L 8439:localhost:8439 -N $GN_HOSTNAME\n# When complete, \"ctrl-c\" or equivalent in the console where the ssh tunnel\n# was started will terminate both parts of a ssh tunnel set up this way.\n</code></pre></p>"},{"location":"ai-testbed/groq/groqview/#access-the-groqview-server-for-your-application","title":"Access the GroqView server for your application:","text":"<p>Point a Google Chrome-family web browser at this url, adjusting the port number if necessary. (Chrome, Brave, Vivaldi, Opera tested.) <pre><code>http://localhost:8439\n</code></pre></p>"},{"location":"ai-testbed/groq/job-queuing-and-submission/","title":"Job Queueing and Submission","text":"<p>Groq jobs in the AI Testbed's groqrack are managed by the PBS job scheduler. Overview: PBS For additional information, see  https://docs.alcf.anl.gov/running-jobs/job-and-queue-scheduling/ Man pages are available. These are the key commands: <pre><code># qsub - to submit a batch job using a script\nman qsub\n# qstat - to display queue information\nman qstat\n# qdel - to delete (cancel) a job:\nman qdel\n# qhold - to hold a job\nman qhold\n</code></pre></p>"},{"location":"ai-testbed/groq/running-a-model-or-program/","title":"Running a Model/Program","text":"<p>Jobs are launched from any GroqRack node, or from login nodes.  If you expect a loss of an internet connection for any reason, for long-running jobs we suggest logging into a specific node and using either screen or tmux to create persistent command line sessions.  For details use:</p> <p><pre><code>man screen\n# or\nman tmux\n</code></pre> or online man pages: screen, tmux</p>"},{"location":"ai-testbed/groq/running-a-model-or-program/#running-jobs-on-groq-nodes","title":"Running jobs on Groq nodes","text":""},{"location":"ai-testbed/groq/running-a-model-or-program/#groqflow","title":"GroqFlow","text":"<p>GroqFlow is the simplest way to port applications running inference to groq. The groqflow github repo includes many sample applications. See GroqFlow.</p>"},{"location":"ai-testbed/groq/running-a-model-or-program/#clone-the-groqflow-github-repo","title":"Clone the GroqFlow github repo","text":"<p>Clone the groqflow github repo and change current directory to the clone: <pre><code>cd ~/\ngit clone https://github.com/groq/groqflow.git\ncd groqflow\n</code></pre></p>"},{"location":"ai-testbed/groq/running-a-model-or-program/#groqflow-conda-environments","title":"GroqFlow conda environments","text":"<p>Create a groqflow conda environment, and activate it. Follow the instructions in the Virtual Environments  section. Note: Similar install instructions are in <code>~/groqflow/docs/install.md</code> or GroqFlow\u2122 Installation Guide The conda enviroment should be reinstalled whenever new groqflow code is pulled from the groqflow github; with a groqflow conda environment activated, redo just the pip install steps.</p>"},{"location":"ai-testbed/groq/running-a-model-or-program/#running-a-groqflow-sample","title":"Running a groqflow sample","text":"<p>Each groqflow sample directory in the <code>~/groqflow/proof_points</code> tree has a README.md describing the sample and how to run it.</p>"},{"location":"ai-testbed/groq/running-a-model-or-program/#optionally-activate-your-groqflow-conda-environment","title":"Optionally activate your GroqFlow conda environment","text":"<pre><code>conda activate groqflow\n</code></pre>"},{"location":"ai-testbed/groq/running-a-model-or-program/#run-a-sample-using-pbs-in-batch-mode","title":"Run a sample using PBS in batch mode","text":"<p>See Job Queueing and Submission for more information about the PBS job scheduler.</p> <p>Create a script <code>run_minilmv2.sh</code> with the following contents. It assumes that conda was installed in the default location. The conda initialize section can also be copied from your .bashrc if the conda installer was allowed to add it. <pre><code>#!/bin/bash\n# &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$(${HOME}'/miniconda3/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"${HOME}/miniconda3/etc/profile.d/conda.sh\" ]; then\n        . \"${HOME}/miniconda3/etc/profile.d/conda.sh\"\n    else\n        export PATH=\"${HOME}/miniconda3/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\nconda activate groqflow\ncd ~/groqflow/proof_points/natural_language_processing/minilm\npip install -r requirements.txt\npython minilmv2.py\n</code></pre></p> <p>Then run the script as a batch job with PBS: <pre><code>qsub -l groq_accelerator=1 run_minilmv2.sh\n</code></pre></p> <p>Note: the number of chips used by a model can be found in the compile cache dir for the model after it is compiled. E.g. <pre><code>$ grep num_chips_used ~/.cache/groqflow/minilmv2/minilmv2_state.yaml\nnum_chips_used: 1\n</code></pre> The groqflow proofpoints models use 1, 2 or 4 chips. </p> <p>If your <code>~/.bashrc</code> initializes conda, an alternative to copying the conda initilization script into your execution scripts is to comment out this section in your \"~/.bashrc\": <pre><code># If not running interactively, don't do anything\ncase $- in\n    *i*) ;;\n      *) return;;\nesac\n</code></pre> to <pre><code>## If not running interactively, don't do anything\n#case $- in\n#    *i*) ;;\n#      *) return;;\n#esac\n</code></pre> Then the execution script becomes: <pre><code>#!/bin/bash\nconda activate groqflow\ncd ~/groqflow/proof_points/natural_language_processing/minilm\npip install -r requirements.txt\npython minilmv2.py\n</code></pre> Job status can  be tracked with qstat: <pre><code>$ qstat\nJob id            Name             User              Time Use S Queue\n----------------  ---------------- ----------------  -------- - -----\n3084.groq-r01-co* run_minilmv2     user              0 R workq           \n$ \n</code></pre></p> <p>Output will by default go to two files with names like the following, where the suffix is the job id. One standard output for the job. The other is the standard error for the job. <pre><code>$ ls -la run_minilmv2.sh.*\n-rw------- 1 user users   448 Oct 16 18:40 run_minilmv2.sh.e3082\n-rw------- 1 user users 50473 Oct 16 18:42 run_minilmv2.sh.o3082\n</code></pre></p>"},{"location":"ai-testbed/groq/running-a-model-or-program/#run-a-sample-using-pbs-in-interactive-mode","title":"Run a sample using PBS in interactive mode","text":"<p>An alternative is to use an interactive PBS job. This may be useful when debugging new or changed code. Here is an example that starts a 24 hour interactive job. <pre><code>qsub -IV -l walltime=24:00:00 -l groq_accelerator=2\n</code></pre> Then activate your groqflow environment, and run python scripts with <pre><code>conda activate groqflow\npython scriptname.py\n</code></pre></p>"},{"location":"ai-testbed/groq/system-overview/","title":"System Overview","text":"<p>ALCF's Groq system consists of a single <code>GroqRackTM compute cluster</code> that provides an extensible accelerator network consisting of 9 <code>GroqNodeTM</code> [ groq-r01-gn-01 through groq-r01-gn-09 ] nodes with a rotational multi-node network topology. Each of these GroqNodes consists of 8 GroqCardTM accelerators in them with integrated chip-to-chip connections with a dragonfly multi-chip topology.</p> <p><code>GroqCardTM accelerator</code> is a dual-width, full-height, three-quarter length PCI-Express Gen4 x16 adapter that includes a single <code>GroqChipTM processor</code> with 230 MB of on-chip memory. Based on the proprietary Tensor Streaming Processor (TSP) architecture, the GroqChip processor is a low latency and high throughput single core SIMD compute engine capable of 750 TOPS (INT8) and 188 TFLOPS (FP16) @ 900 MHz that includes advanced vector and matrix mathematical acceleration units.  The GroqChip processor is deterministic, providing predictable and repeatable performance. </p> <p>The <code>GroqWare suite SDK</code> uses a API based programming model and enables users to develop, compile, and run models on the GroqCard accelerator in a host server system. The SDK uses a ONNX/MLIR enabled DAG compiler and it consists of Groq Compiler, Groq API, and utility tools like GroqView\u2122 profiler and groq-runtime. </p> <pre><code>\n</code></pre> <p>For more information refer to the following links:</p> <p>GroqRack spec sheet GroqNode spec sheet GroqCard spec sheet GroqChip spec sheet (via)</p>"},{"location":"ai-testbed/groq/virtual-environments/","title":"Virtual Environments","text":""},{"location":"ai-testbed/groq/virtual-environments/#install-conda","title":"Install conda","text":"<p>If conda is not already installed: <pre><code>rm Miniconda3-latest-Linux-x86_64.sh*\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\n# answer y/yes to all prompts\n# exit ssh session, then start a new ssh session\nexit\n</code></pre></p>"},{"location":"ai-testbed/groq/virtual-environments/#groqflow-conda-environment-setup","title":"GroqFlow conda environment setup","text":""},{"location":"ai-testbed/groq/virtual-environments/#create-and-activate-a-groqflow-conda-environment","title":"Create and activate a groqflow conda environment","text":"<p>Create a groqflow conda environment and activate it <pre><code>export PYTHON_VERSION=3.10.12\nconda create -n groqflow python=$PYTHON_VERSION -y\nconda activate groqflow\n</code></pre></p>"},{"location":"ai-testbed/groq/virtual-environments/#install-groqflow-into-the-groqflow-conda-environment","title":"Install groqflow into the groqflow conda environment","text":"<p>Execute the following commands to install groqflow into the activated groqflow conda environment</p> <pre><code># Alter this if you have cloned groqflow to some other location.\ncd ~/groqflow\nif [ -d \"groqflow.egg-info\" ]; then rm -r groqflow.egg-info; fi\npip install --upgrade pip\npip list --format=freeze &gt; frozen.txt\npip install -r frozen.txt -e .\npushd . \ncd demo_helpers\nif [ -d \"groqflow_demo_helpers.egg-info\" ]; then rm -r groqflow_demo_helpers.egg-info; fi\npip install -e .\npopd\npip install soundfile\n</code></pre> <p>Note: if you encounter problems trying to update an existing groqflow conda environment, consider removing the existing environment with the following command, and recreating it. Make sure you deactivate the environment before removing it.      <pre><code>  conda remove --name groqflow --all -y\n</code></pre></p>"},{"location":"ai-testbed/groq/virtual-environments/#use-groqflow","title":"Use Groqflow","text":"<p>To use groqfloq, <pre><code>conda activate groqflow\n</code></pre> Note: Always use a personal conda environment when installing packages on groq nodes; otherwise they can get installed into <code>~/.local</code> and can cause problems when your shared home directory is used on other systems. If you encounter mysterious package dependency/version issues, check your <code>~/.local/lib</code> and <code>~/.local/bin</code> for mistakenly installed packages.</p> <p>Note: The conda enviroment should be reinstalled whenever new groqflow code is pulled from the groqflow github; with a groqflow conda environment activated, redo just the pip install steps, including the removal of the egg-info directories.</p>"},{"location":"ai-testbed/sambanova/TODO/","title":"TODO","text":"<ul> <li> docs/ai-testbed/sambanova_gen2/example-multi-node-programs.md</li> <li> docs/ai-testbed/sambanova_gen2/ GPT2 example</li> </ul> <p>Using /data/ANL/results/sn30-r1-h1/wilsonb/032223.18/GPT1.5B.out for output Using /data/ANL/results/sn30-r2-h1/wilsonb/032223.19/GPT1.5B.out for output</p> <p>Using /data/ANL/results/sn30-r2-h1/wilsonb/032223.19/BertLarge.out for output</p>"},{"location":"ai-testbed/sambanova/documentation/","title":"Documentation","text":"<p>The SambaNova documentation is now available online SambaNova Documentation.</p> <p>The documentation for the SambaTune (a profiling and performance tuning tool for SambaNova systems) is now available at SambaTune Documentation.</p>"},{"location":"ai-testbed/sambanova/example-multi-node-programs/","title":"Example Multi-Node Programs","text":"<p>In this section we will learn how to extend the UNet2d and Gpt1.5B applications scripts that we introduced in the Example Programs to compile and run multiple instances of the model in a data parallel fashion across multiple tiles or across multiple nodes.</p>"},{"location":"ai-testbed/sambanova/example-multi-node-programs/#unet2d","title":"UNet2d","text":""},{"location":"ai-testbed/sambanova/example-multi-node-programs/#set-up","title":"Set Up","text":"<p>Create the following directory and change to it if you have not already done so.</p> <pre><code>mkdir -p ~/apps/image/unet\ncd ~/apps/image/unet\n</code></pre>"},{"location":"ai-testbed/sambanova/example-multi-node-programs/#create-unet2dsh-and-unet_batchsh","title":"Create Unet2d.sh and unet_batch.sh","text":"<p>Create the file Unet2d.sh and unet_batch.sh in the current directory using your favorite editor. Copy and paste the contents of Unet2d.sh and unet_batch.sh to files with the same name into the current directory using your favorite editor.</p> <pre><code>chmod +x Unet2d.sh\nchmod +x unet_batch.sh\n</code></pre>"},{"location":"ai-testbed/sambanova/example-multi-node-programs/#compile-and-run","title":"Compile and run","text":"<p>Run these commands for training (compile + train): The compile and run scripts have the following input arguments.</p> <ol> <li> <p>image size:  The images are square.  Valid sizes include 256, 512, and 1024.</p> </li> <li> <p>Batch size: local batch size.  The global batch size is local batch size * Num of instances.</p> </li> <li> <p>num of instances: Total number of instances of Unet2d run in data parallel framework.</p> </li> <li> <p>RunID: A unique Id for the compile or run process.</p> </li> </ol> <p>The script uses the arguments <code>pcompile</code> and <code>prun</code> for the data parallel compile and run.</p> <pre><code>./Unet2d.sh pcompile &lt;image size&gt; &lt;batch_size&gt; &lt;num of instances&gt; &lt;RunID&gt;\n./Unet2d.sh prun &lt;image size&gt; &lt;batch_size&gt; &lt;num of instances&gt; &lt;RunID&gt;\n</code></pre> <p>For a image size of 256x256 and local batch size of 256 when running 8 instance, the commands are provided as follows.</p> <pre><code>./Unet2d.sh pcompile 256 256 8 unet2d_8inst_pcompile\n./Unet2d.sh prun 256 256 8 unet2d_8inst_prun\n</code></pre> <p>The above commands displays the file that contains the output for the execution of the above scripts, usually <code>/data/ANL/results/&lt;hostname&gt;/&lt;userId&gt;/&lt;RunID&gt;/Unet2d.out</code></p> <p>You can inspect the compile command that contains <code>--data-parallel -ws 2</code> arguments to ensure that the <code>pef</code> file is compatible for data parallel runs. The pef generated from the compilation process for the above compile command is placed under out/Unet2d/unet_train_256_256_NP_4 inside the current working directory.</p> <pre><code>python /opt/sambaflow/apps/image/segmentation/compile.py compile --mac-v2 --in-channels=3 --in-width=${2} --in-height=${2} --batch-size=${BS} --enable-conv-tiling --num-tiles=4 --pef-name=unet_train_${BS}_${2}_NP_${NUM_TILES}  --data-parallel -ws 2 --output-folder=${OUTDIR}\n</code></pre> <p>Once the model is compiled, sbatch is used to launch the multiple instances. The below example shows that a total of 8 tasks or instances are launched over the host on which the script is launched.</p> <pre><code>sbatch --gres=rdu:1 --tasks-per-node ${NP} --nodes 1 --nodelist $(hostname) --cpus-per-task=${cpus} $(pwd)/unet_batch.sh ${NP} ${NUM_WORKERS} ${BS} ${2} ${5}\n</code></pre> <p>The <code>run</code> command has <code>--data-parallel --reduce-on-rdu</code> arguments that is compatible with data parallel run.</p> <pre><code>srun --mpi=pmi2 python /opt/sambaflow/apps/image/segmentation//hook.py run --data-cache=${CACHE_DIR}  --data-in-memory --num-workers=${NUM_WORKERS} --enable-tiling  --min-throughput 395 --in-channels=3 --in-width=${IM} --in-height=${IM} --init-features 32 --batch-size=${BS} --epochs 10 --data-dir ${DS} --log-dir log_dir_unet_${IM}_${BS}_${NP} --data-parallel --reduce-on-rdu --pef=${OUTDIR}/unet_train_${BS}_${IM}_NP_4/unet_train_${BS}_${IM}_NP_4.pef\n</code></pre> <p>The throughput is calculated by averaging the <code>e2e samples_per_sec</code> over the different instances.</p> <pre><code>inner train loop time : 36.314290046691895 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 563.9653143065\ninner train loop time : 33.36756229400635 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 613.7697389922524\ninner train loop time : 33.94625234603882 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 603.3066563941279\ninner train loop time : 32.309499979019165 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 633.8692958200872\ninner train loop time : 31.418426036834717 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 651.8467849404489\ninner train loop time : 28.164129495620728 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 727.1660927132315\ninner train loop time : 30.29698896408081 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 675.9747651583616\ninner train loop time : 25.332663536071777 for 10 epochs, number of global steps: 10, e2e samples_per_sec: 808.442427336472\n</code></pre>"},{"location":"ai-testbed/sambanova/example-multi-node-programs/#gpt-15b","title":"Gpt 1.5B","text":""},{"location":"ai-testbed/sambanova/example-multi-node-programs/#set-up_1","title":"Set up","text":"<pre><code>mkdir ~/nlp-multiNodetest\ncd ~/nlp-multiNodetest\n</code></pre>"},{"location":"ai-testbed/sambanova/example-multi-node-programs/#create-and-run-gpt15b_compilesh-and-gpt15b_runsh","title":"Create and run Gpt1.5B_compile.sh and Gpt1.5B_run.sh","text":"<p>Create the files Gpt1.5B_compile.sh and Gpt1.5B_run.sh in the current directory. Copy the contents of Gpt1.5B_compile.sh and Gpt1.5B_run.sh. Alternatively, the files can be accessed at <code>/data/ANL/scripts/Gpt1.5B_compile.sh</code> and <code>/data/ANL/scripts/Gpt1.5B_run.sh</code> on any of the compute node and can be copied over to the working directory.</p>"},{"location":"ai-testbed/sambanova/example-multi-node-programs/#compile-and-run_1","title":"Compile and Run","text":"<p>This script consists of commands to <code>compile</code> and <code>run</code> multiple instances of Gpt1.5B model across multiple nodes. Run the Gpt1.5B_compile.sh to first compile and generate the <code>pef</code> file for the model and it in turn launches the <code>Gpt1.5B_run.sh</code> script to run multiple instances of the model over the different nodes.</p> <pre><code>chmod +x Gpt1.5B_compile.sh\nchmod +x Gpt1.5B_run.sh\n./Gpt1.5B_compile.sh\n</code></pre> <p>You can see the log file path displayed on the screen as seen in the example below. You can use the <code>tail</code> command to check the progress of the run.</p> <pre><code>vsastry@sn30-r1-h1:~/nlp-multiNodetest$ ./Gpt1.5B_compile.sh\nUsing /data/ANL/results/sn30-r1-h1/vsastry/041823.19/GPT1.5B.out for output\n</code></pre> <p>The artifacts of the compile process is produced in the path : <code>/data/scratch/&lt;userId&gt;</code>.</p> <p>Inspect the <code>compile</code> command in the script to see that it includes additional arguments <code>--data-parallel</code> and <code>-ws 2</code> to generate a <code>pef</code> that is compatible for data parallel runs.</p> <pre><code>python /opt/sambaflow/apps/nlp/transformers_on_rdu/transformers_hook.py compile --module_name gpt2_pretrain --task_name clm --max_seq_length 1024 -b 16 --output_dir=${OUTDIR}/hf_output --overwrite_output_dir --do_train  --per_device_train_batch_size 16 --cache ${OUTDIR}/cache/ --tokenizer_name gpt2 --model_name gpt2 --mac-v2 --non_split_head --mac-human-decision /opt/sambaflow/apps/nlp/transformers_on_rdu/human_decisions_gm/mac_v2_overrides/gpt2_48_enc_full_recompute_training_spatialmapping_tiling16_clmerge_gm_nonpardp_lnsd.json --compiler-configs-file /opt/sambaflow/apps/nlp/transformers_on_rdu/human_decisions_gm/compiler_configs/compiler_configs_gpt2_sc_recompute_spatialmapping_tiling16_clsmerge_withcls_nonpardp_norc_e2e.json --skip_broadcast_patch --config_name /opt/sambaflow/apps/nlp/transformers_on_rdu/customer_specific/mv/configs/gpt2_config_xl_50260.json --no_index_select_patch --data-parallel -ws 2 --weight_decay 0.1  --max_grad_norm_clip 1.0 --num-tiles 4 --pef-name=gpt15 --output-folder=${OUTDIR}\n</code></pre> <p>Once the model is compiled, <code>sbatch</code> is used to launch the multiple instances across the nodes. The below example shows that a total of <code>32 tasks</code> or instances are launched over <code>2 nodes</code> with each node having a maximum of <code>16 tasks</code>. Slurm allocates any 2 of the available nodes in this example.</p> <pre><code>/usr/local/bin/sbatch --output=${HOME}/slurm-%A.out --ntasks 32 --gres=rdu:1 --ntasks-per-node 16  --nodes 2 --cpus-per-task=8  Gpt1.5B_run.sh ${1} &gt;&gt; ${OUTPUT_PATH} 2&gt;&amp;1\n</code></pre> <p>The <code>run</code> command for each of this instance is present in the <code>Gpt1.5B_run.sh</code> script. You can inspect the command in the script to see that <code>--data-parallel --reduce-on-rdu</code> arguments are present to ensure that the model is run in a data parallel fashion and that the gradient accumulation takes place on the RDU.</p> <pre><code>/usr/local/bin/srun --mpi=pmi2 python /opt/sambaflow/apps/nlp/transformers_on_rdu/transformers_hook.py run  -b 16  --module_name gpt2_pretrain --task_name clm --max_seq_length 1024  --overwrite_output_dir --do_train  --per_device_train_batch_size 16 --cache ${OUTDIR}/cache/  --tokenizer_name gpt2 --model_name gpt2 --non_split_head --skip_broadcast_patch --no_index_select_patch --output_dir=${OUTDIR}/hf_output --config_name /opt/sambaflow/apps/nlp/transformers_on_rdu/customer_specific/mv/configs/gpt2_config_xl_50260.json --max_grad_norm_clip 1.0 --skip_checkpoint --data-parallel --reduce-on-rdu --data_dir /data/ANL/ss1024 --data_dir /data/ANL/ss1024  --logging_steps 1 --max_steps 900000 --learning_rate 0.00025 --steps_this_run 800 --min_throughput 299000 --max_throughput 600000 --pef=${OUTDIR}/gpt15/gpt15.pef &gt;&gt; ${OUTPUT_PATH} 2&gt;&amp;1\n</code></pre> <p><code>squeue</code> shows that the model is run on 2 nodes <code>sn30-r1-h1</code> and <code>sn30-r2-h2</code>.</p> <pre><code>JOBID PARTITION                      NAME     USER ST       TIME  NODES NODELIST(REASON)\n10191 sambanova            Gpt1.5B_run.sh  vsastry  R      23:18      2 sn30-r1-h1,sn30-r2-h2\n</code></pre> <p><code>sntilestat</code> can also be used to check the total numbers of tiles used for the runs.</p> <pre><code>TILE                 %idle %exec %pload %aload %chkpt %quiesce    PID     USER COMMAND\n/XRDU_0/RDU_0/TILE_0   8.0  91.6    0.3    0.1    0.0      0.0 2750333  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_1   8.0  91.6    0.3    0.1    0.0      0.0 2750333  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_2   7.9  91.6    0.3    0.3    0.0      0.0 2750333  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_3   7.7  91.8    0.3    0.3    0.0      0.0 2750333  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_4   7.6  91.9    0.4    0.1    0.0      0.0 2750339  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_5   7.5  91.9    0.5    0.1    0.0      0.0 2750339  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_6   7.5  91.8    0.5    0.3    0.0      0.0 2750339  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_0/TILE_7   7.3  92.0    0.6    0.0    0.0      0.0 2750339  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_0   8.9  89.9    1.0    0.1    0.0      0.0 2750338  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_1   9.0  89.9    0.9    0.1    0.0      0.0 2750338  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_2   8.6  89.8    1.4    0.1    0.0      0.0 2750338  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_3   8.5  89.9    1.4    0.1    0.0      0.0 2750338  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_4   7.9  90.9    0.9    0.4    0.0      0.0 2750343  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_5   7.7  90.9    0.9    0.5    0.0      0.0 2750343  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_6   7.7  91.0    0.9    0.4    0.0      0.0 2750343  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_0/RDU_1/TILE_7   8.0  91.0    0.6    0.4    0.0      0.0 2750343  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_0   7.6  92.0    0.3    0.1    0.0      0.0 2750345  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_1   7.6  92.0    0.3    0.1    0.0      0.0 2750345  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_2   7.5  92.1    0.3    0.1    0.0      0.0 2750345  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_3   7.5  92.1    0.3    0.1    0.0      0.0 2750345  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_4   7.5  92.1    0.3    0.1    0.0      0.0 2750335  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_5   7.5  92.1    0.3    0.1    0.0      0.0 2750335  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_6   7.5  92.1    0.3    0.1    0.0      0.0 2750335  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_0/TILE_7   7.5  92.1    0.3    0.1    0.0      0.0 2750335  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_0   7.7  91.5    0.4    0.4    0.0      0.0 2750330  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_1   7.9  91.5    0.3    0.4    0.0      0.0 2750330  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_2   7.9  91.5    0.3    0.4    0.0      0.0 2750330  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_3   7.6  91.8    0.4    0.3    0.0      0.0 2750330  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_4   7.7  91.9    0.4    0.0    0.0      0.0 2750334  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_5   7.7  91.9    0.4    0.0    0.0      0.0 2750334  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_6   7.9  91.9    0.3    0.0    0.0      0.0 2750334  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_1/RDU_1/TILE_7   7.9  91.9    0.3    0.0    0.0      0.0 2750334  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_0   8.0  91.8    0.1    0.1    0.0      0.0 2750346  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_1   8.0  91.8    0.1    0.1    0.0      0.0 2750346  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_2   8.0  91.8    0.1    0.1    0.0      0.0 2750346  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_3   7.7  91.9    0.1    0.3    0.0      0.0 2750346  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_4   7.5  92.0    0.5    0.0    0.0      0.0 2750336  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_5   7.6  91.9    0.5    0.0    0.0      0.0 2750336  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_6   7.6  91.9    0.4    0.1    0.0      0.0 2750336  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_0/TILE_7   7.5  91.9    0.4    0.3    0.0      0.0 2750336  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_0   7.5  91.8    0.6    0.1    0.0      0.0 2750331  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_1   7.5  91.8    0.6    0.1    0.0      0.0 2750331  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_2   7.7  91.6    0.5    0.1    0.0      0.0 2750331  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_3   7.7  91.6    0.5    0.1    0.0      0.0 2750331  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_4   7.9  91.4    0.8    0.0    0.0      0.0 2750329  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_5   7.9  91.4    0.8    0.0    0.0      0.0 2750329  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_6   8.1  91.4    0.5    0.0    0.0      0.0 2750329  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_2/RDU_1/TILE_7   8.2  91.4    0.4    0.0    0.0      0.0 2750329  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_0   7.5  91.8    0.4    0.4    0.0      0.0 2750344  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_1   7.5  91.8    0.4    0.4    0.0      0.0 2750344  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_2   7.5  91.8    0.4    0.4    0.0      0.0 2750344  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_3   7.5  91.8    0.4    0.4    0.0      0.0 2750344  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_4   7.6  91.8    0.3    0.4    0.0      0.0 2750337  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_5   7.7  91.8    0.1    0.4    0.0      0.0 2750337  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_6   7.7  91.8    0.3    0.3    0.0      0.0 2750337  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_0/TILE_7   7.7  91.9    0.3    0.1    0.0      0.0 2750337  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_0   7.7  92.0    0.1    0.1    0.0      0.0 2750347  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_1   7.7  92.0    0.1    0.1    0.0      0.0 2750347  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_2   7.7  92.1    0.1    0.0    0.0      0.0 2750347  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_3   7.7  92.1    0.1    0.0    0.0      0.0 2750347  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_4   7.3  91.9    0.5    0.3    0.0      0.0 2750332  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_5   7.3  91.9    0.5    0.3    0.0      0.0 2750332  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_6   7.3  91.9    0.5    0.3    0.0      0.0 2750332  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n/XRDU_3/RDU_1/TILE_7   7.3  92.0    0.5    0.1    0.0      0.0 2750332  vsastry /opt/sambaflow/apps/nlp/transformers_on_rdu/venv/b\n</code></pre> <p>The Slurm log associated with the JOBID (10191 in the above example) is located in the home directory. You can use the <code>tail</code> command to check the progress of the training.</p> <pre><code>vsastry@sn30-r1-h1:~$ tail -f ~/slurm-10191.out\nUsing /data/ANL/results/sn30-r1-h1/vsastry/041823.03/Gpt1.5B.out for output\n</code></pre> <pre><code>vsastry@sn30-r1-h1:~$ tail -f /data/ANL/results/sn30-r1-h1/vsastry/041823.03/Gpt1.5B.out\n</code></pre> <p>Once the run is completed, check the log file for the performance results.</p> <pre><code>{'e2e_train_time': 2179.2292835712433, 'training_sequences_per_second': 192467.31088004305, 'final_loss': 4.781678199768066}\n247/3247 [01:03&lt;00:00, 50.76it/s]\n</code></pre>"},{"location":"ai-testbed/sambanova/example-programs/","title":"Example Programs","text":"<p>You can use the link to the tutorials on the SambaNova GitHub site or the examples on the compute node (as explained below).</p> <ul> <li>Find the tutorials on the SambaNova GitHub site. If you use those instructions, ensure that you still use the steps for accessing the SN compute node, setting the required environment and compiling and running the applications as described in this documentation. </li> <li>Use the examples of well-known simple AI applications under the path: <code>/opt/sambaflow/apps/starters</code>, on all SambaNova compute nodes, as discussed on this page.  </li> </ul> <p>Make a copy of this to your home directory:</p> <pre><code>cd ~/\nmkdir apps\ncp -r /opt/sambaflow/apps/starters apps/starters\n</code></pre> <p>Deactivate any active conda environment. If you have conda installed and a conda environment is active, you will see something like <code>(base)</code> at the beginning of the command prompt. If so, you will need to deactivate it with <code>conda deactivate</code>. Conda is not used on the SambaNova SN30 cluster. </p>"},{"location":"ai-testbed/sambanova/example-programs/#lenet","title":"LeNet","text":"<p>Change directory</p> <pre><code>cd ~/apps/starters/lenet\n</code></pre>"},{"location":"ai-testbed/sambanova/example-programs/#common-arguments","title":"Common Arguments","text":"<p>Below are some of the common arguments used across most of the models in the example code.</p> Argument Default Help -b 1 Batch size for training -n, 100 Number of iterations to run --num-iterations the pef for -e, 1 Number epochs for training --num-epochs --log-path 'check Log path points' --num-workers 0 Number of workers --measure-train- None Measure training performance performance"},{"location":"ai-testbed/sambanova/example-programs/#lenet-arguments","title":"LeNet Arguments","text":"Argument Default Help --lr 0.01 Learning rate for training --momentum 0.0 Momentum value for training --weight-decay 0.01 Weight decay for training --data-path './data' Data path --data-folder 'mnist_ Folder containing mnist data data' <p>Note:  If you receive an \\\"HTTP error\\\" message on any of the following commands, run the command again. Such errors (e.g 503) are commonly an intermittent failure to download a dataset.</p> <p>Run these commands to compile and train the LeNet model:</p> <pre><code>srun python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\nsrun python lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>Alternatively to use Slurm sbatch, create submit-lenet-job.sh with the following contents:</p> <pre><code>#!/bin/sh\n\npython lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\npython lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>Then</p> <pre><code>mkdir -p pef/lenet\nsbatch --output=pef/lenet/output.log submit-lenet-job.sh\n</code></pre> <p>Squeue will give you the queue status.</p> <pre><code>squeue\n# One may also...\nwatch squeue\n</code></pre> <p>One may see the run log using:</p> <pre><code>cat pef/lenet/output.log\n</code></pre>"},{"location":"ai-testbed/sambanova/example-programs/#mnist-feed-forward-network","title":"MNIST - Feed Forward Network","text":"<p>Change directory</p> <pre><code>cd ~/apps/starters/ffn_mnist/\n</code></pre> <p>Commands to run MNIST example:</p> <pre><code>srun python ffn_mnist.py  compile -b 1 --pef-name=\"ffn_mnist\" --mac-v2\nsrun python ffn_mnist.py  run -b 1 -p out/ffn_mnist/ffn_mnist.pef\n</code></pre> <p>To run the same using Slurm sbatch, create and run the submit-ffn_mnist-job.sh with the following contents.</p> <pre><code>#!/bin/sh\npython ffn_mnist.py  compile -b 1 --pef-name=\"ffn_mnist\" --mac-v2\npython ffn_mnist.py  run -b 1 -p out/ffn_mnist/ffn_mnist.pef\n</code></pre> <pre><code>mkdir -p pef/ffn_mnist\nsbatch --output=pef/ffn_mnist/output.log submit-ffn_mnist-job.sh\n</code></pre>"},{"location":"ai-testbed/sambanova/example-programs/#logistic-regression","title":"Logistic Regression","text":"<p>Change directory</p> <pre><code>cd ~/apps/starters/logreg\n</code></pre>"},{"location":"ai-testbed/sambanova/example-programs/#logistic-regression-arguments","title":"Logistic Regression Arguments","text":"<p>This is not an exhaustive list of arguments.</p> <p>Arguments</p> Argument Default Help Step --lr 0.001 Learning rate for training Compile --momentum 0.0 Momentum value for training Compile --weight-decay 1e-4 Weight decay for training Compile --num-features 784 Number features for training Compile --num-classes 10 Number classes for training Compile --weight-norm na Enable weight normalization Compile <p>Run these commands:</p> <pre><code>srun python logreg.py compile --pef-name=\"logreg\" --output-folder=\"pef\"\nsrun python logreg.py run --pef=\"pef/logreg/logreg.pef\"\n</code></pre> <p>To use Slurm, create submit-logreg-job.sh with the following contents:</p> <pre><code>#!/bin/sh\npython logreg.py compile --pef-name=\"logreg\" --output-folder=\"pef\"\npython logreg.py run --pef=\"pef/logreg/logreg.pef\"\n</code></pre> <p>Then</p> <pre><code>mkdir -p pef/logreg\nsbatch --output=pef/logreg/output.log submit-logreg-job.sh\n</code></pre> <p>The output, pef/logreg/output.log, will look something like this:</p> <pre><code>2023-03-08 21:18:25.168190: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-03-08 21:18:25.334389: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-03-08 21:18:25.334430: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-03-08 21:18:26.422458: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-03-08 21:18:26.422701: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-03-08 21:18:26.422709: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n[Info][SAMBA]# Placing log files in /home/wilsonb/apps/starters/logreg/pef/logreg/logreg.samba.log\n[Info][MAC]# Placing log files in /home/wilsonb/apps/starters/logreg/pef/logreg/logreg.mac.log\n...\n\nEpoch [1/1], Step [10000/60000], Loss: 0.4642\nEpoch [1/1], Step [20000/60000], Loss: 0.4090\nEpoch [1/1], Step [30000/60000], Loss: 0.3863\nEpoch [1/1], Step [40000/60000], Loss: 0.3703\nEpoch [1/1], Step [50000/60000], Loss: 0.3633\nEpoch [1/1], Step [60000/60000], Loss: 0.3553\nTest Accuracy: 91.40  Loss: 0.3014\n2023-03-08T21:19:08 : [INFO][LIB][2688517]: sn_create_session: PEF File: pef/logreg/logreg.pef\n</code></pre>"},{"location":"ai-testbed/sambanova/example-programs/#unet2d","title":"UNet2D","text":"<p>The UNet application example is provided in the the path : <code>/opt/sambaflow/apps/image/segmentation/</code>. As any other application, we first compile and then train the model using compile and run arguments respectively. The scripts containing the compile and run commands for UNet2D model can be accessed at Unet2d.sh or at <code>/data/ANL/scripts/Unet2d.sh</code> on any SN30 compute node.</p> <p>Change directory and copy files.</p> <pre><code>mkdir -p ~/apps/image/unet\ncd ~/apps/image/unet\n</code></pre> <p>Copy and paste the contents of Unet2d.sh to a file with the same name into the current directory using your favorite editor.</p> <pre><code>chmod +x Unet2d.sh\n</code></pre> <p>Run these commands for training (compile + train):</p> <pre><code>./Unet2d.sh compile &lt;image size&gt; &lt;batch_size&gt; &lt;num of instances&gt; &lt;RunID&gt;\n./Unet2d.sh run &lt;image size&gt; &lt;batch_size&gt; &lt;num of instances&gt; &lt;RunID&gt;\n</code></pre> <p>The <code>compile</code> and <code>run</code> arguments of the script can only be run with number of instances equal to 1, indicating that this is a simple 4 tile run without data parallel framework. For a image size of 256x256 and batch size 256 when running just 1 instance, the commands are provided as follows.</p> <pre><code>./Unet2d.sh compile 256 256 1 unet2d_single_compile\n./Unet2d.sh run 256 256 1 unet2d_single_run\n</code></pre> <p>The above commands displays the file that contains the output for the execution of the above scripts, usually <code>/data/ANL/results/&lt;hostname&gt;/&lt;userid&gt;/&lt;RunID&gt;/Unet2d.out</code></p> <p>If we inspect the compile and run commands for the UNet application provided in the script, we see that the application is compiled with <code>--num-tiles 4</code>, which means that the entire application fits on 4 tiles or half of a RDU. The pef generated from the compilation process of the above command is placed under <code>out/Unet2d/unet_train_256_256_single_4</code> inside the current working directory.</p> <pre><code>python ${UNET}/compile.py compile --mac-v2 --in-channels=3 --in-width=${2} --in-height=${2} --batch-size=${BS} --enable-conv-tiling --num-tiles=4 --pef-name=unet_train_${BS}_${2}_single_${NUM_TILES} --output-folder=${OUTDIR}\n</code></pre> <pre><code>srun --nodelist $(hostname) python /opt/sambaflow/apps/image/segmentation//hook.py run --data-cache=${CACHE_DIR}  --data-in-memory --num-workers=${NUM_WORKERS} --enable-tiling  --min-throughput 395 --in-channels=3 --in-width=${2} --in-height=${2} --init-features 32 --batch-size=${BS} --epochs 10 --data-dir ${DS} --log-dir log_dir_unet_${2}_${BS}_single_${NUM_TILES} --pef=${OUTDIR}/unet_train_${BS}_${2}_single_${NUM_TILES}/unet_train_${BS}_${2}_single_${NUM_TILES}.pef\n</code></pre> <p>The performance data is located at the bottom of log file.</p> <pre><code>inner train loop time : 374.6789753437042 for 10 epochs, number of global steps: 130, e2e samples_per_sec: 88.82270474202953\n</code></pre>"},{"location":"ai-testbed/sambanova/example-programs/#gpt-15b","title":"Gpt 1.5B","text":"<p>The Gpt 1.5B application example is provided in the the path : <code>/opt/sambaflow/apps/nlp/transformers_on_rdu/</code>. The scripts containing the <code>compile</code> and <code>run</code> commands for Gpt1.5B model can be accessed at the path <code>/data/ANL/scripts/Gpt1.5B_base_single_compile.sh</code> and <code>/data/ANL/scripts/Gpt1.5B_base_single_run.sh</code> on any SN30 compute node. This script is compiled and run for only 1 instance and the model fits on 4 tiles or half of a RDU. The scripts are provided for reference. </p> <p>Change directory and copy files.</p> <pre><code>mkdir -p ~/apps/nlp/Gpt1.5B_single\ncd ~/apps/nlp/Gpt1.5B_single\n</code></pre> <p>Copy and paste the contents of Gpt1.5B_base_single_compile.sh and Gpt1.5B_base_single_run.sh  to a file with the same names into the current directory using your favorite editor.</p> <p>or copy the contents from <code>/data/ANL/scripts/Gpt1.5B_base_single_compile.sh</code> and <code>/data/ANL/scripts/Gpt1.5B_base_single_run.sh</code>.</p> <pre><code>cp /data/ANL/scripts/Gpt1.5B_base_single_compile.sh ~/apps/nlp/Gpt1.5B_single/\ncp /data/ANL/scripts/Gpt1.5B_base_single_run.sh ~/apps/nlp/Gpt1.5B_single/\n</code></pre> <p>Run the script with batch size as an argument(shown below with an example of 32).</p> <pre><code>chmod +x Gpt1.5B_base_single_compile.sh \n./Gpt1.5B_base_single_compile.sh 32\n</code></pre> <p>The Gpt1.5B_base_single_compile.sh  script will internally call the Gpt1.5B_base_single_run.sh to perform the training. You can inspect the <code>compile</code> and <code>run</code> commands in the scripts to learn that this model trains with a batch size of 32 for 1 instance over 4 tiles. The human decision file and the compiler config file helps to optimize the compute and memory resources specific to this Gpt 1.5B model run.</p> <pre><code>python /opt/sambaflow/apps/nlp/transformers_on_rdu/transformers_hook.py compile --pef-name=GPT1.5B_base_single_32 --output-folder=/data/scratch/user/GPT1.5B_base_single_32 --module_name gpt2_pretrain --task_name clm --max_seq_length 1024 -b 32  --output_dir=/data/scratch/user/GPT1.5B_base_single_32/hf_gpt1dot5b_ss1k_gas_1_bs32  --overwrite_output_dir --do_train  --per_device_train_batch_size 32   --tokenizer_name gpt2 --model_name gpt2 --mac-v2 --non_split_head --mac-human-decision /opt/sambaflow/apps/nlp/transformers_on_rdu/human_decisions_gm/mac_v2_overrides/gpt2_48_enc_full_recompute_training_spatialmapping_tiling16_clmerge_gm_pardp2_lnsd.json --compiler-configs-file /opt/sambaflow/apps/nlp/transformers_on_rdu/human_decisions_gm/compiler_configs/compiler_configs_gpt1dot5b_perf.json --skip_broadcast_patch --config_name /opt/sambaflow/apps/nlp/transformers_on_rdu/customer_specific/mv/configs/gpt2_config_xl_50260.json --no_index_select_patch --weight_decay 0.1  --max_grad_norm_clip 1.0 --num-tiles 4 --enable-stochastic-rounding\n</code></pre> <pre><code>COMMAND= /usr/local/bin/srun --mpi=pmi2 python /opt/sambaflow/apps/nlp/transformers_on_rdu/transformers_hook.py run  -b 32  --data_dir /data/ANL/ss1024 --pef=/data/scratch/user/GPT1.5B_base_single_32/GPT1.5B_base_single_32/GPT1.5B_base_single_32.pef --output_dir=/data/scratch/user/GPT1.5B_base_single_32/hf_gpt1dot5b_ss1k_gas_1_bs16 --module_name gpt2_pretrain --task_name clm --max_seq_length 1024  --overwrite_output_dir --do_train  --per_device_train_batch_size 32 --tokenizer_name gpt2 --model_name gpt2 --non_split_head --skip_broadcast_patch --no_index_select_patch --config_name /opt/sambaflow/apps/nlp/transformers_on_rdu/customer_specific/mv/configs/gpt2_config_xl_50260.json --max_grad_norm_clip 1.0 --skip_checkpoint --logging_steps 1 --max_steps 75000 --learning_rate 0.00025 --steps_this_run 100\n</code></pre> <p>The <code>sntilestat</code> command shows that the application runs on 4 tiles as shown below.</p> <pre><code>/XRDU_0/RDU_0/TILE_0   2.1  96.9    0.8    0.1    0.0      0.0 796481  user python /opt/sambaflow/apps/nlp/transformers_on_rdu/\n/XRDU_0/RDU_0/TILE_1   2.1  96.9    0.8    0.1    0.0      0.0 796481  user python /opt/sambaflow/apps/nlp/transformers_on_rdu/\n/XRDU_0/RDU_0/TILE_2   2.5  96.9    0.4    0.1    0.0      0.0 796481  user python /opt/sambaflow/apps/nlp/transformers_on_rdu/\n/XRDU_0/RDU_0/TILE_3   2.5  96.9    0.4    0.1    0.0      0.0 796481  user python /opt/sambaflow/apps/nlp/transformers_on_rdu/\n/XRDU_0/RDU_0/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n...\n</code></pre>"},{"location":"ai-testbed/sambanova/getting-started/","title":"Getting Started","text":""},{"location":"ai-testbed/sambanova/getting-started/#on-boarding","title":"On-Boarding","text":"<p>SambaNova SN30 can be accessed using your ALCF account. See Get Started to request an account and for additional information.</p>"},{"location":"ai-testbed/sambanova/getting-started/#setup","title":"Setup","text":""},{"location":"ai-testbed/sambanova/getting-started/#system-view","title":"System View","text":"<p>Connection to a SambaNova node is a two-step process. The first step is to ssh to the login node. This step requires an MFA passcode for authentication - an eight-digit passcode generated by an app on your mobile device, e.g., MobilePASS+. The second step is to log in to a SambaNova node from the login node.</p> <p></p>"},{"location":"ai-testbed/sambanova/getting-started/#log-in-to-login-node","title":"Log in to Login Node","text":"<p>Log in to the SambaNova login node from your local machine using the below command. This uses the MobilePASS+ token generated every time you log in to the system. This is the same passcode used to authenticate into other ALCF systems, such as Polaris.</p> <p>In the examples below, replace ALCFUserID with your ALCF user id.</p> <pre><code>ssh ALCFUserID@sambanova.alcf.anl.gov\nPassword: &lt; MobilePASS+ code &gt;\n</code></pre> <p>Note: Use the ssh \"-v\" option in order to debug any ssh problems.</p>"},{"location":"ai-testbed/sambanova/getting-started/#log-in-to-a-sambanova-node","title":"Log in to a SambaNova Node","text":"<p>Once you are on the login node, a SambaNova node can be accessed using an alias, sn30-r[1-4]-h[1-2] where 'r' stands for the rack number, and 'h' stands for host. sn30-r1-h1 is the first host of the first rack.</p> <p>The 8 nodes are aliased as : sn30-r1-h1 , sn30-r1-h2, sn30-r2-h1, sn30-r2-h2, sn30-r3-h1, sn30-r3-h2, sn30-r4-h1, sn30-r4-h2.</p> <p>sn30-r1-h1 can be accessed as below.</p> <pre><code>ssh sn30-r1-h1\n</code></pre>"},{"location":"ai-testbed/sambanova/getting-started/#sdk-setup","title":"SDK setup","text":"<p>The required software environment (SambaFlow software stack and the associated environmental variables) for a SN30 node is set up automatically at login. This is unlike the SN10 where the environment had to be set up by each user.</p>"},{"location":"ai-testbed/sambanova/job-queuing-and-submission/","title":"Job Queueing and Submission","text":""},{"location":"ai-testbed/sambanova/job-queuing-and-submission/#introduction","title":"Introduction","text":"<p>SambaNova uses Slurm for job submission and queueing. Below are some of the important commands for using Slurm. For more information refer to Slurm Documentation.</p> <p>Note: Run the Python scripts using 'srun' or 'sbatch', to ensure that concurrent jobs do not interfere with each other.</p> <p>Note: There is just one scheduler for all of the SambaNova nodes.</p>"},{"location":"ai-testbed/sambanova/job-queuing-and-submission/#srun","title":"SRun","text":"<p>The Slurm command <code>srun</code> can be used to run individual Python scripts in parallel with other scripts on a cluster managed by Slurm. Examples of <code>srun</code> usage are shown below.</p> <p>Slurm will assign a nodelist/host to run a job if a host is not specified.</p> <p>Example:</p> <pre><code>srun python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\nsrun python lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>You may specify which node/host on which to run a job.</p> <p>Reasons to specify a node list:</p> <ul> <li>One wants to test a specific node to verify the function of the HW and SW  (daily smoke tests do this)</li> <li>The nodes are at different software levels and one wants to use a node that has the needed software level for one's application.</li> </ul> <p>Example:</p> <pre><code>srun --nodelist=sn30-r1-h1 python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\n</code></pre>"},{"location":"ai-testbed/sambanova/job-queuing-and-submission/#sbatch","title":"SBatch","text":"<p>Alternatively, these jobs can be submitted to the Slurm workload manager through a batch script by using the <code>sbatch</code> command. To do this, create a bash script (submit-lenet-job.sh here as an example) with the commands that you want to execute.</p> <pre><code>#!/bin/sh\n\npython lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\npython lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>Then pass the bash script as an input to the <code>sbatch</code> command as shown below.</p> <pre><code>sbatch --output=pef/lenet/output.log submit-lenet-job.sh\n</code></pre> <p>In case of the need to use multiple RDUs (2 in the example shown below), the <code>sbatch</code> command would be altered as:</p> <pre><code>sbatch --gres=rdu:2 &lt;your_script.sh&gt;\n</code></pre>"},{"location":"ai-testbed/sambanova/job-queuing-and-submission/#squeue","title":"SQueue","text":"<p>The <code>squeue</code> command provides information about jobs located in the Slurm scheduling queue.</p> <pre><code>squeue\n</code></pre>"},{"location":"ai-testbed/sambanova/job-queuing-and-submission/#sinfo","title":"SInfo","text":"<p>SInfo is used to view partition and node information for a system running Slurm.</p> <p>Here is a suggested command:</p> <pre><code>sinfo -O AllocNodes, GresUsed, Gres, NodeList\n</code></pre> <p>For more information, see SInfo.</p>"},{"location":"ai-testbed/sambanova/job-queuing-and-submission/#scancel","title":"SCancel","text":"<p>SCancel is used to signal or cancel jobs, job arrays, or job steps.</p> <pre><code>scancel job_id\n</code></pre>"},{"location":"ai-testbed/sambanova/miscellaneous/","title":"Miscellaneous","text":""},{"location":"ai-testbed/sambanova/miscellaneous/#sdk-version","title":"SDK Version","text":"<p>To find the SDK version, run the following commands</p> <pre><code># TODO\n(venv) ALCFUserID@sn30-r1-h1:~$ python\nPython 3.7.6 (default, Feb 18 2020, 21:28:31)\n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import sambaflow\n&gt;&gt;&gt; sambaflow.__version__\n'1.11.5'\n&gt;&gt;&gt;\n</code></pre>"},{"location":"ai-testbed/sambanova/miscellaneous/#omp_num_threads","title":"OMP_NUM_THREADS","text":"<p>The OMP_NUM_THREADS environment variable sets the number of threads to use for parallel regions.</p> <p>The value of this environment variable must be a list of positive integer values. The values of the list set the number of threads to use for parallel regions at the corresponding nested levels.</p> <p>For the SambaNova system it, is usually set to one.</p> <pre><code>export OMP_NUM_THREADS=16\n</code></pre>"},{"location":"ai-testbed/sambanova/miscellaneous/#where-is-the-model","title":"Where is the Model?","text":"<p>Two copies of the model are maintained.  One in host CPU memory and one in RDU memory. They do not interfere with each other unless you explicitly sync the model/parameter in between using:</p> <pre><code>SambaTensor.rdu() # Moves the CPU model to the RDU\nSambaTensor.cpu() # Moves the RDU model to the CPU\n</code></pre> <p>In order to run the model on the CPU, you can simply use the PyTorch model as if there is no RDU. In order to run the model on RDU, you would need to use session.run().</p>"},{"location":"ai-testbed/sambanova/miscellaneous/#useful-commands","title":"Useful Commands","text":""},{"location":"ai-testbed/sambanova/miscellaneous/#sn-configuration","title":"SN Configuration","text":"<pre><code>snconfig show Node static\n</code></pre> <p>The snconfig utility shows the static configuration of the system. The configuration for the first node is as follows:</p> <pre><code>======================================================\n=======                NODE Info               =======\n======================================================\n=======                Static Info             =======\nTimestamp: 2023-03-16 17:00:04\nPlatform Name: DataScale SN30-8\nNode Name: NODE\n    Number of XRDUS: 4\n    XRDU Name: XRDU_0\n        Number of RDUS: 2\n        RDU name: RDU_0\n            Serial Number     : 205057B469B35895\n            Number of TILES: 8\n            TILE Name: TILE_0\n                Serial Number     : N/A\n            TILE Name: TILE_1\n                Serial Number     : N/A\n\n\n...\n\n\n                    Size              : 128.0 GB\n                    Serial Number     : 1F5BC22\n            DDR CH Name: DDRCH_6\n                Number of DIMMS: 1\n                DIMM Name: DIMM_L0\n                    Size              : 128.0 GB\n                    Serial Number     : 1F5BC99\n            DDR CH Name: DDRCH_7\n                Number of DIMMS: 1\n                DIMM Name: DIMM_M0\n                    Size              : 128.0 GB\n                    Serial Number     : 1F5BB68\n        Total XRDU_3 memory size (GB): 2048.0\n</code></pre>"},{"location":"ai-testbed/sambanova/miscellaneous/#sambanova-daemon-service","title":"SambaNova Daemon Service","text":"<p>The following command checks if the SambaNova daemon service is running.</p> <pre><code>systemctl status snd\n</code></pre> <p>The output should look something like this:</p> <pre><code>\u25cf snd.service - SN Devices Service\n     Loaded: loaded (/lib/systemd/system/snd.service; enabled; vendor preset: enabled)\n    Drop-In: /etc/systemd/system/snd.service.d\n             \u2514\u2500override.conf\n     Active: active (running) since Fri 2023-01-27 04:03:14 UTC; 1 months 18 days ago\n   Main PID: 5635 (snd)\n      Tasks: 9 (limit: 629145)\n     Memory: 156.8M\n     CGroup: /system.slice/snd.service\n             \u2514\u25005635 /opt/sambaflow/bin/snd\n\nWarning: some journal files were not opened due to insufficient permissions.\n</code></pre>"},{"location":"ai-testbed/sambanova/miscellaneous/#tile-status","title":"Tile status","text":"<pre><code>sntilestat\nwatch sntilestat\n</code></pre> <p>The output shown below is when the system is completely idle.</p> <pre><code>TILE                 %idle %exec %pload %aload %chkpt %quiesce    PID     USER COMMAND\n/XRDU_0/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_0/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_0/RDU_1/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_0/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_1/RDU_1/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_0/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_2/RDU_1/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_0/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_0 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_1 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_2 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_3 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_4 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_5 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_6 100.0   0.0    0.0    0.0    0.0      0.0\n/XRDU_3/RDU_1/TILE_7 100.0   0.0    0.0    0.0    0.0      0.0\n</code></pre>"},{"location":"ai-testbed/sambanova/miscellaneous/#finding-hung-tiles","title":"Finding Hung Tiles","text":"<pre><code>snconfig show Node dynamic | grep perfect\n</code></pre>"},{"location":"ai-testbed/sambanova/miscellaneous/#how-busy-is-the-system","title":"How busy is the system?","text":"<p>Use one of</p> <pre><code>top\nhtop\n</code></pre>"},{"location":"ai-testbed/sambanova/running-a-model-or-program/","title":"Running a Model/Program","text":"<p>Note:  Please be mindful of how you are using the system. For example, consider running larger jobs in the evening or on weekends</p> <p>Note: Please use only Slurm commands, i.e., srun and sbatch, to run your code. If you run your code directly using the 'python' command, it may cause conflicts on the system.</p> <p>Note: If you have conda installed and a conda environment is active, you will see something like <code>(base)</code> at the beginning of the command prompt. If so, you will need to deactivate it with <code>conda deactivate</code>. Conda is not used on the SambaNova SN30 cluster.</p>"},{"location":"ai-testbed/sambanova/running-a-model-or-program/#introduction","title":"Introduction","text":"<p>The SambaNova workflow includes the following main steps to run a model.</p> <ol> <li>Compile</li> <li>Run</li> <li>Test (optional)</li> </ol> <p>The system uses the Slurm job scheduler to schedule the jobs and manage the workload on the system. For more information on Slurm, see Job Queueing and Submission.</p> <p>Example Programs lists the different example applications with corresponding commands for each of the above steps.</p>"},{"location":"ai-testbed/sambanova/running-a-model-or-program/#compile","title":"Compile","text":"<p>Compiles the model and generates a .pef file. This file contains information on how to reconfigure the hardware, and map the compute and memory resources required to run an application on RDUs. The pef files are by default saved in the 'out' directory; the SambaNova documentation advises saving pef files in separate directories with the '--output-folder' option.</p> <p>It is necessary to re-compile only when the model changes, or parameters specific to the model graph change, including the batch size.</p> <p>Compile times can be significant. Compiling the UNet sample, for example, when using images of size 32x32 pixels, takes 358(s), and 1844(s) for images of size 256x256.</p> <p>The entire compile process is executed on the host and no RDUs are involved in the compile step.</p> <p>Example of compiling the LeNet application:</p> <pre><code>srun python lenet.py compile -b=1 --pef-name=\"lenet\" --output-folder=\"pef\"\n</code></pre> <p>where</p> Argument Default Help -b 1 Batch size for training"},{"location":"ai-testbed/sambanova/running-a-model-or-program/#run","title":"Run","text":"<p>As part of this step, the model is trained on the RDUs by passing in the PEF file and the training dataset. The location of the pef file generated in the compile step is passed as an argument to the run command. Below is the example of the <code>run</code> command that trains a LeNet model.</p> <pre><code>srun python lenet.py run --pef=\"pef/lenet/lenet.pef\"\n</code></pre> <p>The location of the pef file generated in the compile step is passed as an argument to the run command.</p>"},{"location":"ai-testbed/sambanova/running-a-model-or-program/#test-optional","title":"Test (Optional)","text":"<p>This command is used to run the model on both the host CPU and a SambaNova RDU.  It compares the results from the CPU and RDU and will report if any discrepancies are found. Pass the pef file generated as part of the compile step as the input to this command.</p> <pre><code>srun python lenet.py test --pef=\"pef/lenet/lenet.pef\"\n</code></pre>"},{"location":"ai-testbed/sambanova/sambatune/","title":"Profiling and performance tuning with SambaTune","text":"<p>This section covers how to use the SambaTune profiling performance tuning tool, and the SambaTune UI for viewing the results.</p>"},{"location":"ai-testbed/sambanova/sambatune/#_1","title":"SambaTune for profiling and performance tuning","text":"<p>SambaTune uses a yaml file that describes how to profile an application. There are samples in <code>/opt/sambaflow/sambatune/configs</code>.  This section shows how to run the simplest sample, a linear net.</p> <p>First, ssh into one of the nodes in the SN30 cluster.  Next, start a slurm interative job reserving a full node (8 RDUs), for 8 hours (480 minutes): <pre><code>$ /usr/local/bin/srun --time=480 --gres=rdu:8 --pty bash\n</code></pre> Record the hostname: <pre><code>$ hostname\nsn30-r1-h1\n</code></pre></p> <p>Next, set an environment variable indicating where the profiling information should be stored: <pre><code>export DUMP_ROOT=~/Sambatune\n</code></pre></p> <p>If running a large model, the profiling information can be hundreds of gigabytes or more, and the DUMP_ROOT should be set to some location with more storage than your home directory (which has a quota). E.g. somewhere that you have write access to in <code>/projects</code></p> <p>Optionally, examine the sample yaml file. You will see that it has 5 top-level sections: <code>app:</code>, <code>model-args:</code>, <code>compile-args:</code>, <code>run-args:</code>, <code>env:</code></p> <p>Next, run sambatune using a sample sambatune yaml configuration file. This sample command line requests profiling with the <code>benchmark</code>, <code>instrument</code>, and <code>run</code> modes. <pre><code>$ sambatune --modes benchmark instrument run -- /opt/sambaflow/sambatune/configs/linear_net.yaml\n</code></pre></p> <p>This will take a while to run, particularly if the yaml for a larger model is used.</p> <p>Then, run <code>sambatune_ui</code>: <pre><code>$ export ST_PORT=8576\n$ sambatune_ui --directory $DUMP_ROOT/artifact_root/sambatune_gen --port $ST_PORT\n</code></pre></p> <p>Copy the password shown (e.g. to your clipboard). The userid is always admin. The password is different for every sambatune_ui run. </p> <p>In a fresh console on your working machine where you will run the browser, set up a two-hop ssh tunnel to the target node. Replace the <code>ALCFUserID</code> in the ssh command line with your ALCF userid. <pre><code>$ export ST_PORT=8576\n$ ssh -L $ST_PORT:localhost:$ST_PORT ALCFUserID@sambanova.alcf.anl.gov  -t ssh -L $ST_PORT:localhost:$ST_PORT -N sn30-r1-h1\n</code></pre></p> <p>Put localhost:8576 in the url bar of a Chrome-family browser. (Chrome, Brave, Vivaldi, Opera tested.) A login prompt for the sambatune ui should show. Enter admin and the password copied previously. You should now see the SambaTune UI. </p> <p>If the browser does not show a login prompt, or if any previous step complains about a port conflict, try another value for ST_PORT on both the target node and for the ssh tunnel command, e.g. 8577.</p> <p>See SambaNova's SambaTune documentation for more information about using SambaTune and the SambaTune UI. This section is a good starting point: Workflow overview</p> <p>When finished: - Break the ssh tunnel with ctrl-c or equivalent. - Stop the sambatune_ui server on the target node with ctrl-c or equivalent. - Exit the interactive slurm job to release the reserved resources.</p> <p>A disconnected job can be canceled by determining its job id with <code>squeue -a</code> and canceling the job with <code>scancel &lt;jobid&gt;</code></p>"},{"location":"ai-testbed/sambanova/system-overview/","title":"System Overview","text":""},{"location":"ai-testbed/sambanova/system-overview/#introduction","title":"Introduction","text":"<p>The SambaNova DataScale SN30 system is architected around the next-generation Reconfigurable Dataflow Unit (RDU) processor for optimal dataflow processing and acceleration. The AI Testbed's SambaNova SN30 system consists of eight nodes in 4 full racks, each node featuring eight RDUs interconnected to enable model and data parallelism. SambaFlow, Sambanova's software stack, extracts, optimizes, and maps the dataflow graphs to the RDUs from standard machine learning frameworks like PyTorch.</p> <p>Below are some of the links to SambaNova documentation.</p> <p>SambaNova white paper: Accelerated Computing with a Reconfigurable Dataflow Architecture</p> <p>SN30 documentation: SambaNova Documentation</p>"},{"location":"ai-testbed/sambanova/tunneling-and-forwarding-ports/","title":"Tunneling and Forwarding Ports","text":"<p>Port forwarding is covered here.  This is specifically for TensorBoard.</p>"},{"location":"ai-testbed/sambanova/tunneling-and-forwarding-ports/#tensorboard-port-forwarding","title":"TensorBoard Port Forwarding","text":"<p>This section describes the steps to be followed to set up port forwarding for applications, like TensorBoard, which runs on the SambaNova system and binds to one or more ports. This example uses 6006 and 16006 as port numbers. Using port numbers other than these may avoid collisions with other users.</p>"},{"location":"ai-testbed/sambanova/tunneling-and-forwarding-ports/#from-your-local-machine","title":"From Your Local Machine","text":"<p>Replace ALCFUserID with your ALCF User ID.</p> <p>Run</p> <pre><code># Forward a port number from sambanova.alcf.anl.gov to your local machine.\nssh -v -N -f -L localhost:16006:localhost:16006 ALCFUserID@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilePass+ code &gt;\n\n# Connect to sambanova.alcf.anl.gov\nssh ALCFUserID@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilePass+ code &gt;\n</code></pre>"},{"location":"ai-testbed/sambanova/tunneling-and-forwarding-ports/#from-sambanovaalcfanlgov","title":"From sambanova.alcf.anl.gov","text":"<p>Below are the commands specific to sn30-r1-h1. You may replace sn30-r1-h1 with any other node when using the appropriate system.</p> <p>Run</p> <p>Note:  The full name is sn30-r1-h1.ai.alcf.anl.gov and it may also be used.</p> <pre><code># Forward the port.\nssh -N -f -L localhost:16006:localhost:6006 ALCFUserID@sn30-r1-h1\n# Connect to the system.\nssh ALCFUserID@sn30-r1-h1\n</code></pre>"},{"location":"ai-testbed/sambanova/tunneling-and-forwarding-ports/#on-sn30-r1-h1","title":"On sn30-r1-h1","text":"<p>Activate the venv appropriate to your project.</p> <p>Navigate to the appropriate directory for your model. Launch your model using srun or sbatch.</p> <pre><code>cd /path/to/your/project\nsbatch --output=pef/my_model/output.log submit-my_model-job.sh\n</code></pre>"},{"location":"ai-testbed/sambanova/tunneling-and-forwarding-ports/#on-another-sn30-r1-h1-terminal-window","title":"On Another sn30-r1-h1 Terminal Window","text":"<p>The SambaNova system has a bash shell script to setup the required software environment. This sets up the SambaFlow software stack, the associated environmental variables and activates a pre-configured virtual environment.</p> <p>Use the command appropriate for your environment.</p> <p>For example, if you are using LogReg:</p> <pre><code>ALCFUserID@sn30-r1-h1:~$ source /opt/sambaflow/apps/starters/logreg/venv/bin/activate\n(venv) ALCFUserID@sn30-r1-h1:~$\n</code></pre> <p>Navigate to the appropriate directory for your model.</p> <pre><code>cd /path/to/your/project\ntensorboard --logdir /logs --port 6006\n</code></pre>"},{"location":"ai-testbed/sambanova/tunneling-and-forwarding-ports/#browser-on-local-machine","title":"Browser on Local Machine","text":"<p>Then, navigate in your browser to, in this example, http://localhost:16006 on your local machine.</p>"},{"location":"ai-testbed/sambanova/tunneling-and-forwarding-ports/#notes","title":"Notes","text":"<p>Explanation of ssh command:</p> <pre><code>-N : no remote commands\n\n-f : put ssh in the background\n\n-L &lt;machine1&gt;:&lt;portA&gt;:&lt;machine2&gt;:&lt;portB&gt; :\n\nThe full command line will forward &lt;machine2&gt;:&lt;portB&gt; (remote scope) to &lt;machine1&gt;:&lt;portA&gt; (local scope)\n</code></pre> <p>Adapted from:  How can I run Tensorboard on a remote server?</p>"},{"location":"ai-testbed/sambanova/virtual-environment/","title":"Virtual Environments","text":""},{"location":"ai-testbed/sambanova/virtual-environment/#using-a-venv","title":"Using a Venv","text":"<p>To create a virtual environment, one can use the --system-site-packages flag:</p> <pre><code>python -m venv --system-site-packages my_env\nsource my_env/bin/activate\n</code></pre>"},{"location":"ai-testbed/sambanova/virtual-environment/#installing-packages","title":"Installing Packages","text":"<p>Install packages in the normal manner such as:</p> <pre><code>python3 -m pip install &lt;package&gt;\n</code></pre> <p>For more details see Use pip for installing.</p> <p>To install a different version of a package that is already installed in one's environment, one can use:</p> <pre><code>pip install --ignore-installed  ... # or -I\n</code></pre>"},{"location":"ai-testbed/sambanova/virtual-environment/#pre-built-sample-venv","title":"Pre-Built Sample Venv","text":"<p>Each of the samples or application examples provided by SambaNova has its own pre-built virtual environment which can be readily used. They are present in the <code>/opt/sambaflow/apps/</code> directory tree within each of the applications.</p> <p>Note: Conda is not supported on the SambaNova system.</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/","title":"CosmicTagger Conversion","text":"<p>The intent of this page is to show conceptually how to convert a model to run on the SambaNova system. It is not necessary to convert CosmicTagger because it has already been converted and is located at CosmicTagger on the SambaNova branch. The original is located at CosmicTagger.</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#run-model-on-cpu","title":"Run Model on CPU","text":"<p>The first step to converting a model is to verify that it runs on the CPU.  This step has been verified for CosmicTagger.</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#configpy","title":"Config.py","text":"<p>CosmicTagger can run on multiple machines.  As such, it is necessary to specify the architecture that one is using.  For example, CPU or GPU.  The architecture is stored in the ComputeMode class.</p> <p>Edit src/config/config.py.  Add RDU to the ComputeMode class.</p> <pre><code>class ComputeMode(Enum):\n    CPU   = 0\n    #...\n    RDU   = 6\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#trainerpy","title":"Trainer.py","text":"<p>Edit src/utils/torch/trainer.py.</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#import-sambanova-packages","title":"Import SambaNova Packages","text":"<p>Insert the imports at the top of the file.</p> <p>SambaFlow is a complete software stack designed to take input from standard machine learning frameworks such as PyTorch and TensorFlow. SambaFlow automatically extracts, optimizes, and maps dataflow graphs onto RDUs.</p> <pre><code>try:\n    from sambaflow import samba\n\n    import sambaflow.samba.utils as utils\n    from sambaflow.samba.utils.argparser import parse_app_args\n    from sambaflow.samba.utils.common import common_app_driver\nexcept:\n    pass\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#wrap-model","title":"Wrap Model","text":"<p>Wrap the model using poptorch.trainingModel() so that it may be ran on IPUs for training.</p> <p>Wrap the model using poptorch.inferenceModel() when not training.</p> <p>Find the following code around line 90 in the init_network method.</p> <pre><code>        # Foregoing any fusions as to not disturb the existing ingestion pipeline\n        if self.is_training() and self.args.mode.quantization_aware:\n            self._raw_net.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n            self._net = torch.quantization.prepare_qat(self._raw_net)\n        else:\n            self._net = self._raw_net\n</code></pre> <p>After the above code, add:</p> <pre><code>        if self.args.run.compute_mode == ComputeMode.IPU:\n            if self.is_training():\n                opts = poptorch.Options()\n                self._net = poptorch.trainingModel(self._net, opts, optimizer=torch.optim.SGD(self._net.parameters(), lr=1e-3))\n            else:\n                self._net = poptorch.inferenceModel(self._net)\n</code></pre> <p>See poptorch.trainingModel() and poptorch.inferenceModel() for more information.</p> <p>There is also a Build the Model tutorial.</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#update-optimizer","title":"Update Optimizer","text":"<p>Update init_optimizer() to use the poptorch class instead of the torch class as needed.</p> <p>Change:</p> <pre><code>        if self.args.mode.optimizer.name == OptimizerKind.rmsprop:\n            self._opt = torch.optim.RMSprop(self._net.parameters(), 1.0, eps=1e-4)\n        else:\n            self._opt = torch.optim.Adam(self._net.parameters(), 1.0)\n</code></pre> <p>to:</p> <pre><code>        if self.args.mode.optimizer.name == OptimizerKind.rmsprop:\n            if self.args.run.compute_mode == ComputeMode.IPU:\n                self._opt = poptorch.optim.RMSprop(self._net.parameters(), 1.0, eps=1e-4)\n            else:\n                self._opt = torch.optim.RMSprop(self._net.parameters(), 1.0, eps=1e-4)\n        else:\n            if self.args.run.compute_mode == ComputeMode.IPU:\n                self._opt = poptorch.optim.Adam(self._net.parameters(), 1.0)\n            else:\n                self._opt = torch.optim.Adam(self._net.parameters(), 1.0)\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#update-the-forward-pass","title":"Update the Forward Pass","text":"<p>Putting the loss calculation in forward_pass() allows the loss computation to be performed on the IPUs. This will be faster because the data will not need to be transfered round-trip to the CPU.</p> <p>Change forward_pass():</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#original","title":"Original","text":"<pre><code>            if net is None:\n                logits_image = self._net(minibatch_data['image'])\n            else:\n                logits_image = net(minibatch_data['image'])\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#updated","title":"Updated","text":"<p>The following code changes are to account for the loss function, i.e., self.loss_calculator, and the image labels, i.e., labels_image, to be passed to the model's forward_pass method.  Additionally, the calculated loss is returned from the forward_pass method.</p> <pre><code>            if net is None:\n                if self.args.run.compute_mode == ComputeMode.IPU:\n                    logits_image, labels_image, loss = self._net(minibatch_data['image'], self.loss_calculator, labels_image)\n                    return logits_image, labels_image, loss\n                else:\n                    logits_image = self._net(minibatch_data['image'])\n            else:\n                if self.args.run.compute_mode == ComputeMode.IPU and self.args.mode.name != ModeKind.inference:\n                    logits_image, labels_image, loss = net(minibatch_data['image'], self.loss_calculator, labels_image)\n                    return logits_image, labels_image, loss\n                else:\n                    logits_image = net(minibatch_data['image'])\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#update-the-training-step","title":"Update the Training Step","text":"<p>Receive the extra loss variable from the forward_pass method.</p> <p>Update the train_step method.</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#original-training-step","title":"Original Training Step","text":"<pre><code>                    with self.timing_context(\"forward\"):\n                        if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\n                            with torch.cuda.amp.autocast():\n                                logits_image, labels_image = self.forward_pass(minibatch_data)\n                        else:\n                            logits_image, labels_image = self.forward_pass(minibatch_data)\n\n                    verbose = False\n\n                    # Compute the loss based on the logits\n                    with self.timing_context(\"loss\"):\n                        loss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#updated-training-step","title":"Updated Training Step","text":"<p>The forward_pass() method was changed to return the extra variable loss in the previous section.  It is now received conditionally when using an IPU(s).</p> <p>In the with self.timing_context(\"loss\"): section, only calculate loss if not using an IPU(s).</p> <pre><code>                    with self.timing_context(\"forward\"):\n                        if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\n                            with torch.cuda.amp.autocast():\n                                logits_image, labels_image = self.forward_pass(minibatch_data)\n                        else:\n                            if self.args.run.compute_mode == ComputeMode.IPU:\n                                logits_image, labels_image, loss = self.forward_pass(minibatch_data)\n                            else:\n                                logits_image, labels_image = self.forward_pass(minibatch_data)\n\n                    verbose = False\n\n\n                    # Compute the loss based on the logits\n                    with self.timing_context(\"loss\"):\n                        if self.args.run.compute_mode == ComputeMode.IPU:\n                            loss = loss\n                        else:\n                            loss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#update-validation-step","title":"Update Validation Step","text":"<p>Update the val_step method.</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#original-validation-step-code","title":"Original Validation Step Code","text":"<p>Find this code.</p> <pre><code>            if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\n                with torch.cuda.amp.autocast():\n                    logits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n            else:\n                logits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n\n            # Compute the loss based on the logits\n            loss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#updated-validation-step-code","title":"Updated Validation Step Code","text":"<p>Change the code to the following.</p> <pre><code>            if self.args.run.precision == Precision.mixed and self.args.run.compute_mode == ComputeMode.GPU:\n                with torch.cuda.amp.autocast():\n                    logits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n\n                    # Compute the loss based on the logits\n                    loss = self.loss_calculator(labels_image, logits_image)\n            else:\n                if self.args.run.compute_mode == ComputeMode.IPU:\n                    logits_image, labels_image, loss = self.forward_pass(minibatch_data, net=val_net)\n                else:\n                    logits_image, labels_image = self.forward_pass(minibatch_data, net=val_net)\n\n                    # Compute the loss based on the logits\n                    loss = self.loss_calculator(labels_image, logits_image)\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#uresnet2d-model","title":"UResNet2D Model","text":""},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#update-model","title":"Update Model","text":"<p>The Graphcore system is more computationally efficient if the loss function is on the IPU.  This is accomplished by using the loss function within the model's forward method.</p> <p>Edit src/networks/torch/uresnet2D.py.</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#update-the-forward-declaration","title":"Update the Forward Declaration","text":"<p>Find the forward method.</p> <pre><code>def forward(self, input_tensor):\n</code></pre> <p>Update the argument list to include the loss function, i.e., loss_calculator and the image labels, i.e., labels_image.</p> <pre><code>def forward(self, input_tensor, loss_calculator=None, labels_image=None):\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#add-loss-calculation","title":"Add Loss Calculation","text":"<p>Add the loss calculation just before the forward method returns.</p> <pre><code>        if loss_calculator is not None:\n\n            labels_image = labels_image.long()\n            labels_image = torch.chunk(labels_image, chunks=3, dim=1)\n            shape =  labels_image[0].shape\n            labels_image = [ _label.view([shape[0], shape[-2], shape[-1]]) for _label in labels_image ]\n\n            loss = loss_calculator(labels_image, x)\n            import poptorch\n            loss = poptorch.identity_loss(loss , reduction=\"mean\")\n            return x, labels_image, loss\n\n        # This return already exists.\n        return x\n</code></pre> <p>The poptorch.identity_loss method takes a single PyTorch tensor and will backpropagate a gradient of ones through it.  You may find an example at here</p>"},{"location":"ai-testbed/sambanova/unused/cosmictagger-conversion/#binexecpy","title":"bin/exec.py","text":"<p>The following is included for completeness.  One will not likely find this in other code.</p> <p>Open bin/exec.py in your favorite editor.  Change:</p> <pre><code>@hydra.main(version_base=None, config_path=\"../src/config\", config_name=\"config\")\n</code></pre> <p>to</p> <pre><code>@hydra.main(config_path=\"../src/config\", config_name=\"config\")\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/performance-tools/","title":"Performance Tools","text":""},{"location":"ai-testbed/sambanova/unused/performance-tools/#tile-status","title":"Tile Status","text":"<pre><code>sntilestat\nwatch sntilestat\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/performance-tools/#measure-tflops","title":"Measure TFLOPs","text":"<p>This is an example for measuring TFLOPs for Conv2D forward pass.</p> <pre><code>elif args.command == 'run':\n    samba.session.run(inputs, section_types=['fwd'])\n    #samba.session.run(inputs, section_types=['bckwd'])\n    n_iters = 100\n    forward_pass_time = []\n    print(\"run starts\")\n    start_time_forward = time.time()\n    for loop in range(n_iters):\n        samba.session.run(inputs, section_types=['fwd'])\n        #samba.session.run(inputs, section_types=['bckwd'])\n        #samba.session.run(inputs, section_types=['fwd', 'bckwd'])\n    end_time_forward = time.time()\n    forward_pass_time.append(end_time_forward - start_time_forward)\n    print(\"run ends\")\n\n    w_0 = (args.w + 2*args.pad_w - args.s)/args.wstride + 1\n    h_0 = (args.h + 2*args.pad_h - args.r)/args.hstride + 1\n    tflops = 2 * (w_0*h_0) * args.s * args.r * args.c * args.k * args.n\n    tflops_forw = tflops/(sum(forward_pass_time)/n_iters/5)/(10**12) #tflops\n    print(tflops)\n    print(sum(forward_pass_time))\n    print(\"tflops: %f\"%tflops_forw)\n    print(\"SN,Training,%s,Conv2d_fwd,%d,100,1,%d,%d,%d,%d,%d,%d,%d,0.0,%f,None,%f,%f,%f\" % (\"dtype\", args.n, args.w, args.h, args.c, args.k, args.s, args.pad_w, args.wstride, (sum(forward_pass_time)/n_iters)/args.n, args.n/(sum(forward_pass_time)/n_iters), tflops_forw, (sum(forward_pass_time)/n_iters)/args.n))\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/running-GPT2-multi-node/","title":"Running GPT-2 on Multiple Nodes","text":"<p>This GPT-2 example is for 1.5B parameters on two (2) nodes. Each node has eight (8) RDUs for a total of sixteen (16) RDUs.</p>"},{"location":"ai-testbed/sambanova/unused/running-GPT2-multi-node/#create-a-directory","title":"Create a Directory","text":"<pre><code>cd &lt;path to desired directory&gt;\nmkdir GPT1.5B\ncd GPT1.5B\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/running-GPT2-multi-node/#establish-script","title":"Establish Script","text":"<p>Using your favorite editor, create the file 'Gpt1.5B.sh'.</p> <p>Copy the contents of Gpt1.5B.sh.</p> <p>Make the script executable:</p> <pre><code>chmod +x Gpt1.5B.sh\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/running-GPT2-multi-node/#multiple-nodes","title":"Multiple Nodes","text":"<p>Gpt1.5B.sh contains the sbatch command:</p> <pre><code>/usr/local/bin/sbatch --output=${HOME}/slurm-%A.out --ntasks 32 --gres=rdu:1 --ntasks-per-node 16  --nodes 2 --cpus-per-task=8  /data/ANL/scripts/Gpt1.5B_run.sh ${1} &gt;&gt; ${OUTPUT_PATH} 2&gt;&amp;1\n</code></pre> <p>The sbatch nodes argument specifies the number of nodes to use.</p> <p>nodes 2 Nodes to use.</p> <p>Additionally, here are the other sbatch arguments.</p> <p>--ntasks 32: This option specifies the number of tasks to be used in the job.</p> <p>ntasks-per-node 16: This option specifies the number of tasks per node.</p> <p>gres=rdu:1 Indicates the model fits on a single RDU.</p> <p>cpus-per-task=8 CPUs per task.</p>"},{"location":"ai-testbed/sambanova/unused/running-GPT2-multi-node/#run","title":"Run","text":"<p>The script accepts an optional first parameter to specify the log directory.</p> <p>Run the script:</p> <pre><code>./Gpt1.5B.sh &lt;optional log directory&gt;\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/running-GPT2-multi-node/#output","title":"Output","text":"<p>The output can be found at /data/ANL/results/$(hostname)/${USER}/${LOGDIR}/${MODEL_NAME}.out. The actual path will be displayed on the screen.</p>"},{"location":"ai-testbed/sambanova/unused/running-GPT2/","title":"Running GPT2","text":"<p>The Pile and OWT data are located in:</p> <pre><code>/data/ANL/pile\n/data/ANL/openwebtext_ss2048\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/running-bert-large-on-sn30/","title":"Running BERT-Large on SambaNova DataScale SN30-8","text":""},{"location":"ai-testbed/sambanova/unused/running-bert-large-on-sn30/#set-up","title":"Set Up","text":"<p>Establish a test directory from which to work.</p> <pre><code>mkdir $HOME/app-test\ncd $HOME/app-test\n</code></pre> <p>Copy BertLarge.sh into your current directory.</p> <pre><code>cp /data/ANL/scripts/BertLarge.sh .\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/running-bert-large-on-sn30/#running-bert-large-options","title":"Running Bert Large Options","text":"<p>Let's cover several options for executing the script.</p> <ol> <li>Basic</li> </ol> <pre><code>sbatch --output=${HOME}/app-test/slurm-%A.out --cpus-per-task=128 --gres=rdu:16 BertLarge.sh\n</code></pre> <ol> <li>Specify a Log File</li> </ol> <p>This is helpful if doing multiple runs and one wishes to specify a run ID.    This bash script argument is optional.  Place it at the very end of the command.</p> <p>Example:</p> <pre><code>sbatch --output=${HOME}/app-test/slurm-%A.out --cpus-per-task=128 --gres=rdu:16 BertLarge.sh my_runID\n</code></pre> <ol> <li>Specify Nodelist</li> </ol> <p>One may optionally specify a nodelist for sbatch. An example is to use hostname.</p> <pre><code>sbatch --nodelist $(hostname) --output=${HOME}/app-test/slurm-%A.out --cpus-per-task=128 --gres=rdu:16 BertLarge.sh\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/running-bert-large-on-sn30/#running-bert-large","title":"Running Bert Large","text":"<p>Let's specify the log file and the nodelist.</p> <p>Run</p> <pre><code>sbatch --nodelist $(hostname) --output=${HOME}/app-test/slurm-%A.out --cpus-per-task=128 --gres=rdu:16 BertLarge.sh\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/running-bert-large-on-sn30/#output","title":"Output","text":"<p>Display the slurm output.  For example:</p> <pre><code>cat slurm-9637.out\n</code></pre> <p>The output will look something like:</p> <pre><code>Using /data/ANL/results/sn30-r3-h1/userid/040423.19/BertLarge.out for output\n</code></pre> <p>You may display that file.  You may want to use less to do so because it is quite long.</p> <pre><code>less /data/ANL/results/sn30-r3-h1/userid/040423.19/BertLarge.out\n</code></pre> <p>The organization of the file is:</p> <ol> <li>System Status</li> <li>Compile (very long)</li> <li>Run</li> <li>System Status</li> <li>Run Duration</li> </ol>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/","title":"SambaTune","text":""},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#notes","title":"Notes","text":"<p>Rick  4/16/2023 [10:16 AM] /home/rweisner/sambatune_ui_dir contains  the 1.15.3 version which is the latest released version. It should work on your experimental. You will need browser access to wherever you install it.</p> <pre><code>cd /home/rweisner/tmp/uno_test\n</code></pre> <pre><code>#TODOBRW\nssh wilsonb@homes.cels.anl.gov\nssh sm-02\nMobilePass+ password\nOn sm-02\nsource /opt/sambaflow/venv/bin/activate\nexport PATH=/opt/sambaflow/bin:$PATH\nsambatune linear_net.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\nsambatune_ui --directory /home/wilsonb/tmp/sambatune_gen --port 8580\n#There will be a username and password displayed that you will use in your browser on your laptop.\nCommand used on laptop for port forward\nssh -XL 8580:127.0.0.1:8580 wilsonb@sm-02.cels.anl.gov\nMobilePass+ password\n# You will be logged into sm-02 but, you do not need to do anything.\naddress used in browser on laptop localhost:8580\n#Use username and password from sambatune_ui.\nUsername\nPassword\n\n#TODOBRW\n/home/wilsonb/DL/Sambanova/apps_1.12/private/anl/2022-09-21T19-21-05.html\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#about-sambatune","title":"About SambaTune","text":"<p>SambaTune is a tool for profiling, debugging, and tuning the performance of applications running on SN hardware.</p> <p>The tool automates the collection of hardware performance counters, metrics aggregation, report generation, and visualization. It also automates benchmarking of the application to compute average throughput over a sufficient number of runs. The tool is designed to aid the user with performance bottleneck analysis and tuning.</p> <p>SambaTune is currently used by SN engineers involved in performance tuning efforts. SambaTune is also planned for release to external customers to aid with performance bottleneck analysis and resolution.</p>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#run-sambatune","title":"Run SambaTune","text":"<pre><code>ssh ALCFUserID@sambanova.alcf.anl.gov\n# Enter MobilePass+ pass code\nssh sm-01\n</code></pre> <pre><code>#TODOBRW\nssh wilsonb@sambanova.alcf.anl.gov\n# Enter MobilePass+ pass code\nssh sm-01\n</code></pre> <p>First, enter the virtual environment on sm-01 or sm-02:</p> <pre><code>source /opt/sambaflow/venv/bin/activate\n</code></pre> <p>Update path:</p> <pre><code>export PATH=/opt/sambaflow/bin:$PATH\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#usage","title":"Usage","text":"<pre><code>usage: sambatune [-h] [--artifact-root ARTIFACT_ROOT] [--disable-override]\n                 [--compile-only | -m MODES [MODES ...]] [--version]\n                 config\n\npositional arguments:\n  config                YAML file with model, compile, run configuration.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --artifact-root ARTIFACT_ROOT\n                        Custom location to save compile/run artifacts;\n                        defaults to '$DUMP_ROOT/artifact_root' (default: None)\n  --disable-override    Reuse the placement from the baseline compilation\n                        (default: False)\n  --compile-only        Run compilation of PEFs for selected modes only\n                        (default: False)\n  -m MODES [MODES ...], --modes MODES [MODES ...]\n                        Select modes to execute from ['benchmark',\n                        'instrument', 'run'] (default: ['benchmark'])\n  --version             version of sambatune and sambaflow.\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#command-overview","title":"Command Overview","text":"<p>By default, it will run with the benchmarking mode enabled. Use the --modes flag to run modes individually or in any combination. Benchmark-Only:</p> <pre><code>sambatune example_net.yaml --artifact-root $(pwd)/artifact_root --modes benchmark\n</code></pre> <p>Instrument-Only:</p> <pre><code>sambatune example_net.yaml --artifact-root $(pwd)/artifact_root --modes instrument\n</code></pre> <p>All modes:</p> <pre><code>sambatune example_net.yaml --artifact-root $(pwd)/artifact_root --modes instrument\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#command-example","title":"Command Example","text":"<pre><code># From Bill\npython /opt/sambaflow/apps/private/anl/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --pef-name=uno_16_4_500_ws --output-folder=/home/arnoldw//models_dir/1520847 --mac-v1\n\npython /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --pef=/home/arnoldw//models_dir/1520847/uno_16_4_500_ws/uno_16_4_500_ws.pef --in_dir /var/tmp/raw/ --mac-v1\n</code></pre> <pre><code># From Bill --&gt; Bruce\npython /opt/sambaflow/apps/private/anl/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --pef-name=uno_16_4_500_ws --output-folder='.' --mac-v1\n\nexport OMP_NUM_THREADS=1\npython /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial --pef=./uno_16_4_500_ws/uno_16_4_500_ws.pef --in_dir /var/tmp/raw/ --mac-v1\n</code></pre> <pre><code>#TODOBRW  This works.  9/19/22\nsm-01/home/wilsonb/tmp/uno_test/uno_ccle.yaml\napp: /opt/sambaflow/apps/private/anl/uno_full.py\n\nmodel-args: --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial\n\ncompile-args: compile --plot --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nrun-args: --multiprocess-pickle --use-pickle-train  --measure-spatial --train-samba-spatial --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_500 --converted-pickle\n\nenv:\n     OMP_NUM_THREADS: 16,\n     SF_RNT_NUMA_BIND: 2\n</code></pre> <p>Run the following example:</p> <pre><code>sambatune uno_ccle.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <pre><code>#TODOBRW\n# Stand-alone\nexport UNO=.\nexport NS=500\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --pef-name=uno_16_4_${NS}_ws --output-folder='.' --mac-v1\n\nexport OMP_NUM_THREADS=1\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS}\n\nexport UNO=.\nexport NS=500\nexport OMP_NUM_THREADS=1\nsrun pyinstrument /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS} &gt; pyinstrument_1.13.log 2&gt;&amp;1\n\n\n\nRicks run python ${UNO}/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\u201cout/uno_16_4_${NS}/uno_16_4_${NS}.pef\u201d --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\n</code></pre> <pre><code>#TODOBRW\nsm-01/home/wilsonb/DL/Sambanova/apps_1.12/private/anl/uno_brw_CCLE_1_12.yaml\nexport OMP_NUM_THREADS=16\napp: /home/wilsonb/DL/Sambanova/apps_1.12/private/anl/uno_full.py\n\nmodel-args: --weight-sharing -b 16 -mb 4 --num-spatial-batches 500 --mapping spatial\n\ncompile-args: compile --plot --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nrun-args: --measure-spatial --train-samba-spatial --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_500\n\nenv:\n     OMP_NUM_THREADS: 16,\n     SF_RNT_NUMA_BIND: 2\n</code></pre> <p>Run the following example:</p> <pre><code>sambatune uno_brw_CCLE_1_12.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n\nexport UNO=.\nexport NS=50\nexport OMP_NUM_THREADS=1\n\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py compile --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nxsrun pyinstrument /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS} --epochs 1 &gt; my.log 2&gt;&amp;1\n\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py run --multiprocess-pickle  --measure-spatial --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./out/uno_full_16_47_${NS}/uno_full_16_47_${NS}.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_${NS} &gt; pyinstrument_1.13.log 2&gt;&amp;1\n\ncat my.log # Has pyinstrument run name.\npyinstrument --load-prev 2022-09-21T19-21-05 -r html\n\n\n1.13\n\nsource /opt/sambaflow/venv/bin/activate\ncd ~/tmp/uno_test/\nexport UNO=.\nexport NS=500\nexport OMP_NUM_THREADS=1\nexport PATH=/opt/sambaflow/bin:$PATH\nsntilestat\n\n\n\n./uno_pickl.sh compile 500\n./uno_pickl.sh run 500\n</code></pre> <pre><code>sambatune uno_brw_CCLE_1_12.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n\nexport UNO=.\nexport NS=50\nexport OMP_NUM_THREADS=1\n\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py compile --mac-human-decision /opt/sambaflow/apps/private/anl/samba_uno/human_decisions_spatial.json --mac-v1\n\nxsrun pyinstrument /opt/sambaflow/apps/private/anl/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./uno_16_4_${NS}_ws/uno_16_4_${NS}_ws.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --data-dir /software/sambanova/dataset/CCLE_16_${NS} --epochs 1 &gt; my.log 2&gt;&amp;1\n\nsrun python /opt/sambaflow/apps/private/anl/uno_full.py run --multiprocess-pickle  --measure-spatial --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=./out/uno_full_16_47_${NS}/uno_full_16_47_${NS}.pef --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE --lr 0.001 --data-dir /software/sambanova/dataset/CCLE_16_${NS} &gt; pyinstrument_1.13.log 2&gt;&amp;1\n\ncat my.log # Has pyinstrument run name.\npyinstrument --load-prev 2022-09-21T19-21-05 -r html\n\n\n1.13\n\nsource /opt/sambaflow/venv/bin/activate\ncd ~/tmp/uno_test/\nexport UNO=.\nexport NS=500\nexport OMP_NUM_THREADS=1\nexport PATH=/opt/sambaflow/bin:$PATH\nsntilestat\n</code></pre> <p>uno_pickl.sh</p> <pre><code>#! /bin/bash -x\n#set -e\nsource /opt/sambaflow/venv/bin/activate\nSECONDS=0\nNS=${2}\nUNO=/opt/sambaflow/apps/private/anl/\nDS=\"ALL\"\nDS=\"CCLE\"\n\nBS=$((NS*16))\nexport OMP_NUM_THREADS=16\n\necho \"Model: UNO_SPA_TRN\"\necho \"Date: \" $(date +%m/%d/%y)\necho \"Time: \" $(date +%H:%M)\nif [ \"${1}\" == \"convert\" ] ; then\npython3 ${UNO}/uno/uno_data_loaders_converted.py   --in_dir /var/tmp/raw/ --out_dir /software/sambanova/dataset/${DS}_16_${NS}  --batch-size ${BS} --train_sources ${DS} --file-write-frequency 10\n\n\nelif [ \"${1}\" == \"compile\" ] ; then\n  echo \"COMPILE\"\n  python ${UNO}/uno_full.py compile --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --mac-human-decision ${UNO}/samba_uno/human_decisions_spatial.json --pef-name=\"uno_16_4_${NS}\" --mac-v1\n\n\nelif [ \"${1}\" == \"run\" ] ; then\n  echo \"RUN ${DS}\"\n  SF_RNT_NUMA_BIND=2\n  #python ${UNO}/uno_full.py run --acc-test --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\n  python ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --epochs 1\n  #python ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\"\n\nelif [ \"${1}\" == \"pyinstrument\" ] ; then\n  echo \"RUN ${DS}\"\n  SF_RNT_NUMA_BIND=2\n  #python ${UNO}/uno_full.py run --acc-test --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\n  pyinstrument ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --epochs 1\n  #python ${UNO}/uno_full.py run --mac-v1 --multiprocess-pickle --use-pickle-train --train-samba-spatial -b 16 -mb 4 --num-spatial-batches ${NS} --lr 0.001 --mapping spatial --data-dir /software/sambanova/dataset/${DS}_16_${NS} --converted-pickle --train_sources ${DS} --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\"\n\nelif [ \"${1}\" == \"no_pickle\" ] ; then\n  echo \"no_pickle ${DS}\"\n  SF_RNT_NUMA_BIND=2\n  python ${UNO}/uno_full.py run --train-samba-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --in_dir /var/tmp/raw/ --mac-v1 --train_source CCLE\n\nelif [ \"${1}\" == \"mp\" ] ; then\necho \"Duration: \" $SECONDS\n\nelif [ \"${1}\" == \"mp\" ] ; then\necho \"Duration: \" $SECONDS\necho \"PERF\"\npython uno_full.py measure-performance --measure-spatial --weight-sharing -b 16 -mb 4 --num-spatial-batches ${NS} --mapping spatial --pef=\"out/uno_16_4_${NS}/uno_16_4_${NS}.pef\" --num-iterations 20 --mac-v1\nfi\n\necho \"Duration: \" $SECONDS\n</code></pre> <pre><code>./uno_pickl.sh compile 500\n./uno_pickl.sh run 500\n./uno_pickl.sh pyinstrument 500\npyinstrument --load-prev 2022-09-22T18-31-24 -r html\nstdout is a terminal, so saved profile output to /tmp/tmpeo5ehksn.html\ncp /tmp/tmpeo5ehksn.html .\n</code></pre> <p>On dev terminal</p> <pre><code>scp wilsonb@sambanova.alcf.anl.gov:tmp/uno_test/tmpeo5ehksn.html .\n</code></pre> <p>View in local browser.</p>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#running","title":"Running","text":"<p>Create a directory for your work.</p> <pre><code>mkdir ~/sambatune\ncd ~/sambatune\n</code></pre> <p>Create small_vae.yaml with the following content using your favorite editor.</p> <pre><code>app: /opt/sambaflow/apps/private/anl/moleculevae.py\n\nmodel-args: -b 128 --in-width 512 --in-height 512\n\ncompile-args: compile --plot --enable-conv-tiling --compiler-configs-file /opt/sambaflow/apps/private/anl/moleculevae/compiler_configs_conv.json --mac-v2 --mac-human-decision /opt/sambaflow/apps/private/anl/moleculevae/symmetric_human_decisions_tiled_v2.json\n\nrun-args: --input-path /var/tmp/dataset/moleculevae/ras1_prot-pops.h5 --out-path ${HOME}/moleculevae_out --model-id 0 --epochs 10\n\nenv:\n     OMP_NUM_THREADS: 16\n     SF_RNT_FSM_POLL_BUSY_WAIT: 1\n     SF_RNT_DMA_POLL_BUSY_WAIT: 1\n     CONVFUNC_DEBUG_RUN: 0\n</code></pre> <p>Run the following example:</p> <pre><code>sambatune small_vae.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <p>Create linear_net.yaml with the following content using your favorite editor.</p> <pre><code>app: /opt/sambaflow/apps/micros/linear_net.py\n\nmodel-args: &gt;\n  -b 1024\n  -mb 64\n  --in-features 8192\n  --out-features 4096\n  --repeat 128\n  --inference\n\ncompile-args: &gt;\n  --n-chips 2\n  --plot\n\nenv:\n  SF_RNT_FSM_POLL_BUSY_WAIT: 1\n  SF_RNT_DMA_POLL_BUSY_WAIT: 1\n  CONVFUNC_DEBUG_RUN\": 0\n</code></pre> <p>NOTE: The following takes 45 minutes to run.</p> <p>Run the following example:</p> <pre><code>sambatune linear_net.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <pre><code>#TODOBRW\ncd ~/tmp/uno_test\nscreen\nsambatune uno.yaml --artifact-root $(pwd)/artifact_root --modes benchmark instrument run\n</code></pre> <p>where linear_net.yaml is a user-specified configuration file you created above.</p>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#sambatune-ui","title":"SambaTune UI","text":""},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#port-availability","title":"Port Availability","text":"<p>It is recommended that you check if the port you want to use is available. You may check by:</p> <pre><code>ps -elf | grep desired_port\n</code></pre> <p>Example:</p> <pre><code>ps -elf | grep 8576\n</code></pre> <p>Alternatively, you may check for all ports in use by sambatune_ui:</p> <pre><code>ps -elf | grep sambatune_ui\n</code></pre> <p>If you need to free a port that you are finished with, you may use the kill command.</p>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#start-sambatune-ui","title":"Start SambaTune UI","text":"<p>If you followed the above directions, your artifact_root will be at ~/sambatune/artifact_root.</p> <p>Start the UI:</p> <p>It will tell you the username and password.</p> <p>NOTE: It is recommended to use a port other than 8576 in case someone else is using it.  Select another port close to 8576.</p> <p>Next</p> <pre><code>sambatune_ui --directory ~/sambatune/artifact_root/sambatune_gen/ --port 8576\n</code></pre> <pre><code>#TODOBRW\nsambatune_ui --directory ~/sambatune/artifact_root/sambatune_gen/ --port 8580\nsambatune_ui --directory /home/wilsonb/tmp/uno_test/artifact_root/sambatune_gen --port 8580\nusername: \"admin\", password: \"4f7cac2c-351e-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"aaf1fc88-35c8-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"bf64e4f8-3831-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"8feca89e-384c-11ed-93a3-f7ef9c6e5d46\"\nusername: \"admin\", password: \"355222d6-3a88-11ed-93a3-f7ef9c6e5d46\"\n</code></pre> <p>You will see something like:</p> <pre><code>with the,\n    username: \"admin\", password: \"05c63938-2941-11ed-93a3-f7ef9c6e5d46\"\n[2022-08-31 15:24:36 +0000] [1344959] [Info] Starting gunicorn 20.1.0\n[2022-08-31 15:24:36 +0000] [1344959] [Info] Listening at: http://0.0.0.0:8576 (1344959)\n[2022-08-31 15:24:36 +0000] [1344959] [Info] Using worker: sync\n[2022-08-31 15:24:36 +0000] [1345092] [Info] Booting worker with pid: 1345092\n[2022-08-31 15:24:36 +0000] [1345093] [Info] Booting worker with pid: 1345093\n</code></pre> <p>NOTE: Write down the username and password.</p> <p>NOTE: The password only works with this one instance of sambatune_ui.  If you stop this instance of sambatune_ui and start another instance, it will have a new password.</p> <p>NOTE: You will need to &gt; or use the kill command to stop sambatune_ui when you have finished. Not doing so will tie up the port. You can ps -elf | grep the_port_you_used to find the running processes. If you are not comfortable doing this, please ask for help."},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#use-port-forwarding","title":"Use Port-Forwarding","text":"<p>This describes the steps to set up port-forwarding for applications, like SambaTune UI, which runs on the SambaNova system and binds to one or more ports. This example uses 8576 and 18576 as port numbers. Using port numbers other than these may avoid collisions with other users.</p>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#from-your-local-machine","title":"From your local machine","text":"<p>This command sets up a port forward SambaNova login node to your local machine.</p> <p>Run</p> <pre><code>ssh -N -f -L localhost:18576:localhost:18576 ALCFUserID@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilePass+ code &gt;\n\nssh ALCFUserID@sambanova.alcf.anl.gov\n</code></pre> <pre><code>#TODOBRW\nssh -v -N -f -L localhost:8580:localhost:8580 wilsonb@sambanova.alcf.anl.gov\nssh -N -f -L localhost:8580:localhost:8580 wilsonb@sambanova.alcf.anl.gov\n...\nPassword: &lt; MobilePass+ code &gt;\n\nssh wilsonb@sambanova.alcf.anl.gov\n</code></pre> <p>replacing ALCFUserID with your ALCF User ID.</p>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#from-sambanovaalcfanlgov","title":"From sambanova.alcf.anl.gov","text":"<p>This command sets up a port forward from a SambaNova node to the sambanova login machine.</p> <p>Below are the commands specific to sm-01. You may replace sm-01 with sm-02 when using that system.</p> <p>Run</p> <p>NOTE:  The full name is sm-01.ai.alcf.anl.gov and it may also be used.</p> <pre><code>ssh -N -f -L localhost:18576:localhost:8576 ALCFUserID@sm-01\n</code></pre> <pre><code>#TODOBRW\nssh -N -f -L localhost:8580:localhost:8580 wilsonb@sm-01\n</code></pre>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#browser-on-local-machine","title":"Browser on Local Machine","text":"<p>Then, navigate in your browser to, in this example, http://localhost:18576 on your local machine.</p> <p>Use the username and password from sm-01 to log in.</p>"},{"location":"ai-testbed/sambanova/unused/sambatune-user-guide/#ssh-notes","title":"SSH Notes","text":"<p>Explanation of ssh command:</p> <pre><code>-N : no remote commands\n\n-f : put ssh in the background\n\n-L &lt;machine1&gt;:&lt;portA&gt;:&lt;machine2&gt;:&lt;portB&gt; :\n\nThe full command line will forward &lt;machine1&gt;:&lt;portA&gt; (local scope) to &lt;machine2&gt;:&lt;portB&gt; (remote scope)\n</code></pre> <p>Adapted from:  How can I run Tensorboard on a remote server?</p>"},{"location":"aurora/aurora-pe/","title":"Aurora Programming Environment","text":""},{"location":"aurora/aurora-pe/#overview","title":"Overview","text":"<p>The Aurora Programming Environment (Aurora PE) is Aurora's default software environment and consists of the OneAPI SDK, MPICH, and the Spack PE. The Aurora PE is loaded in the user environment through a default module set. Alternative versions of the Aurora PE, as well as Aurora PE components not loaded by default, are available through the module interface.</p> <p>As of May 2024, the default modules are <pre><code>  1) spack-pe-gcc/0.6.1-23.275.2   7) mpich-config/collective-tuning/1024\n  2) gmp/6.2.1-pcxzkau             8) intel_compute_runtime/release/agama-devel-736.25\n  3) mpfr/4.2.0-w7v7yjv            9) oneapi/eng-compiler/2023.12.15.002\n  4) mpc/1.3.1-dfagrna            10) libfabric/1.15.2.0\n  5) gcc/12.2.0                   11) cray-pals/1.3.3\n  6) mpich/icc-all-pmix-gpu/52.2  12) cray-libpals/1.3.3\n</code></pre> Besides the latter three modules, this set of modules is loaded from the Aurora PE. The <code>oneapi</code>, <code>intel_compute_runtime</code>, and <code>mpich</code> modules are part of version <code>23.275.2</code> of the Aurora PE. Each version of the PE may have multiple compilers, runtimes, and/or MPICH installations; these are interchangeable within a particular PE version. In addition to the OneAPI and MPICH installations, the Aurora PE contains the Spack PE, which contains a myriad of general and scientific computing software. See the Spack PE page for more details.</p>"},{"location":"aurora/aurora-pe/#switching-pe-or-sdk-versions","title":"Switching PE or SDK versions","text":"<p>Aurora PE and OneAPI SDK versions can be viewed with <code>module avail</code> and switched with <code>module load</code> commands. The Aurora PE modules utilize Lmod's hierarchical module system to allow seamless switching between versions of the Aurora PE and its components. For example, if the user loads a different version of the <code>oneapi</code> SDK, the other modules in the Aurora PE, such as <code>intel_compute_runtime</code>, <code>mpich</code>, and <code>spack-pe-gcc</code>, will be reloaded to guarantee compatibility. In short, the hierarchical modulefile system ensures that the module environment is self-consistent with minimal user input.</p>"},{"location":"aurora/aurora-pe/#aurora-pe-and-soft","title":"Aurora PE and /soft","text":"<p>The Aurora PE is installed in <code>/opt/aurora</code> and is mounted as a read-only squashfs. <code>/soft</code> is also available to provide software not present in the Aurora PE. Modules in <code>/soft/modulefiles</code> are not part of the default environment in order to avoid filesystem metadata overhead, which can have significant performance impacts. Users wishing to use software in <code>/soft</code> will need to first run <code>module use /soft/modulefiles</code> to access the modules.</p> <p>The Aurora PE has a longer-term upgrade cadence (on the order of months), so ad-hoc software requests will be fulfilled through software installations in <code>/soft</code>. Pre-release previews of Aurora PE components may also be made available in <code>/soft</code> for testing. <code>/soft</code> installations will be considered for incorporation into the Aurora PE during upgrade cycles.</p> <p>Modules in <code>/soft</code> may conflict with modules in the Aurora PE. Lmod has some limitations in automatically handling module conflicts, so the user may need to manually resolve conflicts arising from modules outside of the Aurora PE. Users are also advised to sanitize module paths that they add from other non-standard locations, such as <code>/home</code>, to avoid conflicts. Additionally, packages targeted for future Aurora PE updates are staged for testing under a separate module path  <code>/soft/preview-modules/&lt;PE_VERSION&gt;</code> to prevent accidental module clashes.</p>"},{"location":"aurora/getting-started-on-aurora/","title":"Getting Started on Aurora","text":""},{"location":"aurora/getting-started-on-aurora/#overview","title":"Overview","text":"<p>*** ACCESS IS CURRENTLY ENABLED FOR ESP and ECP TEAMS ONLY ***</p>"},{"location":"aurora/getting-started-on-aurora/#how-to-get-access-to-aurora-for-new-users","title":"How to Get Access to Aurora (for New Users)","text":""},{"location":"aurora/getting-started-on-aurora/#if-you-already-have-access-to-sunspot","title":"If You Already Have Access to Sunspot","text":"<p>If you already have access to Sunspot, all you need to do to gain access to Aurora is send an email to support@alcf.anl.gov requesting access to Aurora. In your email, include</p> <ul> <li>Your ALCF username</li> <li>Your institutional email address</li> <li>The ESP or ECP project in which you are a member</li> </ul>"},{"location":"aurora/getting-started-on-aurora/#for-aurora-early-science-program-esp-team-members","title":"For Aurora Early Science Program (ESP) Team Members","text":"<p>If you have never had access to Sunspot, here are the steps to gain access to Aurora:</p> <ol> <li>Verify that your institution has signed a CNDA with Intel that covers you.</li> <li>If you do not have an active ALCF account, request one using the ALCF Account request webpage. When you come to the part about joining a project, request the <code>ProjectName_aesp_CNDA</code> project.</li> <li>Acknowledge the Intel Terms of Use agreement (TOU) for the Aurora Software Development Kit (SDK) by submitting this form.</li> </ol> <p>Getting a new ALCF account typically takes anywhere from a few days to a few weeks (processing new access for foreign nationals is what can take weeks). After you acknowledge the TOU, there is a manual step that typically takes a few days. You will receive an email notifying you when Aurora access is granted, including some getting started instructions.</p>"},{"location":"aurora/getting-started-on-aurora/#for-aurora-exascale-computing-project-ecp-team-members","title":"For Aurora Exascale Computing Project (ECP) Team Members","text":"<p>See this page for instructions.</p>"},{"location":"aurora/getting-started-on-aurora/#caveats-about-using-aurora-and-reporting-findings","title":"Caveats About Using Aurora and Reporting Findings","text":"<p>NOTE: Sharing of any results from Aurora publicly no longer requires a review or approval from Intel. However, anyone publishing these results should include the following in their materials: </p> <p>\"This work was done on a pre-production supercomputer with early versions of the Aurora software development kit.\"</p> <p>In addition, users should acknowledge the ALCF. Refer to the acknowledgement policy page for details. Please note that certain information on Aurora hardware and software is considered NDA and cannot be shared publicly.</p> <p>Aurora is in the very early stages of the system deployment - do not expect a production environment!</p> <p>Expect to experience:</p> <ul> <li>Hardware instabilities - possible frequent downtime</li> <li>Software instabilities - non-optimized compilers, libraries and tools; frequent software updates</li> <li>Non-final configurations (storage, OS versions, etc...)</li> <li>Short notice for downtimes (scheduled downtimes will be with 4 hr notice, but sometimes downtimes may occur with just an email notice). Notices go to the aurora-notify@alcf.anl.gov email list. All users with access are added to the list initially.</li> </ul>"},{"location":"aurora/getting-started-on-aurora/#getting-help","title":"Getting Help","text":"<p>Email ALCF support at support@alcf.anl.gov for bugs, technical questions, software requests, reservations, priority boosts, etc...</p> <ul> <li>ALCF's user support team will triage and forward the tickets to the appropriate technical SME as needed.</li> <li>Expect turnaround times to be slower than on a production system as the technical team will be focused on stabilizing and debugging the system.</li> </ul> <p>For faster assistance, consider contacting your project's POC at ALCF (project catalyst or liaison)</p> <ul> <li>They are an excellent source of assistance during this early period and will be aware of common bugs and known issues.</li> </ul> <p>ECP and ESP users will be added to a CNDA Slack workspace, where CNDA discussions may occur. An invite to the Slack workspace will be sent when a user is added to the Aurora resource.</p>"},{"location":"aurora/getting-started-on-aurora/#known-issues","title":"Known Issues","text":"<p>See this page for known issues.</p> <p>A known issues page can be found in the JLSE Wiki space used for NDA content. Note that this page requires a JLSE Aurora early hw/sw resource account for access. See page for other known issues.</p>"},{"location":"aurora/getting-started-on-aurora/#allocation-usage","title":"Allocation usage","text":"<p>The allocation accounting system sbank is not yet installed on Aurora.</p> <p>To obtain the usage information for all your projects on Aurora, issue the sbank command on another ALCF resource where <code>sbank</code> is installed, such as Polaris.</p> <pre><code>$ sbank-list-allocations -r aurora\n</code></pre> <p>For more information, see this page.</p>"},{"location":"aurora/getting-started-on-aurora/#transition-to-aurora-from-sunspot","title":"Transition to Aurora from Sunspot","text":"<p>Some guidance is provided here to aid users in the process of moving their work from the Sunspot Test &amp; Development System.</p>"},{"location":"aurora/getting-started-on-aurora/#logging-into-aurora","title":"Logging Into Aurora","text":"<p>Logging into Aurora is a two-stage process. You must first login through the bastion node via: <pre><code>ssh &lt;username&gt;@bastion.alcf.anl.gov\n</code></pre> Then, type in the one-timepassword from your CRYPTOCard/MobilePASS+ token.</p> <p>This bastion node is a pass-through erected for security purposes, and is not meant to host files. Once on the bastion, SSH to <code>login.aurora.alcf.anl.gov</code>. It is round robin to the aurora login nodes.</p> <pre><code>ssh &lt;username&gt;@login.aurora.alcf.anl.gov\n</code></pre>"},{"location":"aurora/getting-started-on-aurora/#proxies-for-outbound-connections-git-ssh-etc","title":"Proxies for outbound connections: Git, ssh, etc...","text":"<p>The Aurora login nodes don't currently have outbound network connectivity enabled by default. Setting the following environment variables will provide access to the proxy host. This is necessary, for example, to clone remote git repos.</p> <pre><code># proxy settings\nexport HTTP_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport HTTPS_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\n</code></pre>"},{"location":"aurora/getting-started-on-aurora/#ssh-to-other-machines","title":"SSH to other machines","text":"<p>To ssh to another machine from an Aurora login node, it can be helpful to add a proxyjump through Bastion in your <code>.ssh/config</code> file. The first password prompt would be for bastion, followed by a prompt for the remote machine.</p> <pre><code>$ cat .ssh/config\nHost my.awesome.machine.edu\n    ProxyJump bastion.alcf.anl.gov\n\n$ ssh me@my.awesome.machine.edu\n</code></pre> <p>Additional guidance on scp and transfering files to Aurora is available and here.</p>"},{"location":"aurora/getting-started-on-aurora/#working-with-git-repos","title":"Working with Git repos","text":"<p>The default SSH port is currently blocked on Aurora; by default, this prevents communicate with Git remotes that are SSH URLs such as the following.</p> <p><pre><code>git clone [user@]server:project.git\n</code></pre> For a workaround for GitHub, GitLab, and Bitbucket, the following can be added to your <code>~.ssh/config</code> file. This requires updating your environment with the above proxy settings.</p> <pre><code>Host github.com\n    User git\n    hostname ssh.github.com\n\nHost gitlab.com\n    User git\n    hostname altssh.gitlab.com\n\nHost bitbucket.org\n    User git\n    hostname altssh.bitbucket.org\n\nHost github.com gitlab.com bitbucket.org\n    Port 443\n    ProxyCommand /usr/bin/socat - PROXY:proxy.alcf.anl.gov:%h:%p,proxyport=3128\n</code></pre> <p>If you need to use something besides your default SSH key on Aurora for authentication to GitHub in conjunction with the above SSH workaround, you may set</p> <pre><code>export GIT_SSH_COMMAND=\"ssh -i ~/.ssh/specialGitKey\"\n</code></pre> <p>where specialGitKey is the name of the private key in your <code>.ssh</code> directory, for which you have uploaded the public key to GitHub. The <code>-F</code> option can be used to specify a different SSH config file if needed; for example, <code>-F none</code> will completely ignore your config file, including the above workaround. </p>"},{"location":"aurora/getting-started-on-aurora/#hardware-overview","title":"Hardware Overview","text":"<p>An overview of the Aurora system including details on the compute node architecture is available on the Machine Overview page.</p>"},{"location":"aurora/getting-started-on-aurora/#file-systems-and-daos","title":"File Systems and DAOS","text":""},{"location":"aurora/getting-started-on-aurora/#home-and-project-directories","title":"Home and Project Directories","text":"<p>Home directories on Aurora are <code>/home/username</code>, available on login and compute nodes. This is provided from <code>/lus/gecko/home</code>. The default quota is 50 GB. Note that bastions have a different <code>/home</code> and the default quota is 500 MB.</p> <p>Lustre project directories are under <code>/lus/gecko/projects</code>. ALCF staff should use /lus/gecko/projects/Aurora_deployment project directory. ESP and ECP project members should use their corresponding project directories. The project name is similar to the name on Polaris with an _CNDA suffix (e.g.: projectA_aesp_CNDA, CSC250ADABC_CNDA). Default quota is 1 TB. The project PI should email support@alcf.anl.gov if their project requires additional storage.</p> <p>Gecko is a small Lustre system for early Aurora use on login and compute nodes. Eventually, production file systems Eagle and Grand will be mounted on Aurora login nodes.</p>"},{"location":"aurora/getting-started-on-aurora/#daos","title":"DAOS","text":"<p>The primary storage system on Aurora is not a file system, but rather an object store called the Distributed Asynchronous Object Store. This is a key-array based system embedded directly in the Slingshot fabric, which provides much faster I/O than conventional block-based parallel file systems such as Lustre (even those using non-spinning disk and/or burst buffers). Project PIs will have requested a storage pool on DAOS via INCITE/ALCC/DD allocation proposals.</p> <p>Preproduction ESP and ECP Aurora project PIs should email support@alcf.anl.gov to request DAOS storage with the following information</p> <ul> <li>Project name (e.g. FOO_aesp_CNDA)</li> <li>Storage capacity (For ESP projects, if this is different than in the ESP   proposal, please give brief justification)</li> </ul> <p>See DAOS Overview for more on using DAOS for I/O.</p>"},{"location":"aurora/getting-started-on-aurora/#software-environment","title":"Software Environment","text":"<p>The Aurora Programming Environment (Aurora PE) provides the OneAPI SDK, MPICH, runtime libraries, and a suite of additional tools and libraries. The Aurora PE is available in the default environment and is accessible through modules. For example, tools and libraries like <code>cmake</code>, <code>boost</code>, and <code>hdf5</code> are available in the default environment. <pre><code>module load cmake\n</code></pre> More details are on the Aurora PE page.</p> <p>Additional software is installed in <code>/soft</code> and can be accessed by adding <code>/soft/modulefiles</code> to the module search path. <pre><code>module use /soft/modulefiles\n</code></pre> This will make available a handful of additional software modules, such as <code>kokkos</code>.</p>"},{"location":"aurora/getting-started-on-aurora/#compiling-applications","title":"Compiling Applications","text":"<p>Users are encouraged to read through the Compiling and Linking Overview page and corresponding pages depending on the target compiler and programming model.</p> <p>Autotools and cmake are available in the default Aurora PE environment and can be loaded via modules.</p> <pre><code>$ module load autoconf cmake\n</code></pre>"},{"location":"aurora/getting-started-on-aurora/#python-on-aurora","title":"Python on Aurora","text":"<p>Frameworks on Aurora can be loaded into a users environment by loading the <code>frameworks</code> module as follows. The conda environment loaded with this module makes available TensorFlow, Horovod, and Pytorch with Intel extensions and optimizations.</p> <pre><code>module load frameworks\n</code></pre> <p>Note that there is a separate Python installation in <code>spack-pe-gcc</code> which is used as a dependency of a number of Spack PE packages. Users will need to exercise caution when loading both <code>frameworks</code> and <code>python</code> from the Spack PE.</p>"},{"location":"aurora/getting-started-on-aurora/#submitting-and-running-jobs","title":"Submitting and Running Jobs","text":"<p>Aurora uses the PBSPro job scheduler system. For Aurora-specific job documentation, refer to Running Jobs on Aurora</p>"},{"location":"aurora/getting-started-on-aurora/#getting-assistance","title":"Getting Assistance","text":"<p>Please direct all questions, requests, and feedback to support@alcf.anl.gov.</p>"},{"location":"aurora/known-issues/","title":"Known Issues","text":"<p>This is a collection of known issues that have been encountered during Aurora's early user phase. Documentation will be updated as issues are resolved. Users are encouraged to email support@alcf.anl.gov to report issues.</p> <p>A known issues page can be found in the CELS Wiki space used for NDA content. Note that this page requires a JLSE Aurora early hw/sw resource account for access.</p>"},{"location":"aurora/known-issues/#running-applications","title":"Running Applications","text":"<ol> <li><code>Cassini Event Queue overflow detected.</code> errors may occur for certain MPI communications and may happen for a variety of reasons - software and hardware, job placement, job routing, and the sate of the machine. Simply speaking, it means one of the network interfaces is getting messages too fast and cannot keep up to process them</li> </ol> <pre><code>libfabric:16642:1701636928::cxi:core:cxip_cq_eq_progress():531&lt;warn&gt; x4204c1s3b0n0: Cassini Event Queue overflow detected.\n</code></pre> <p>As a workaround, the following environment variables can be set to try alleviating the problem.</p> <pre><code>export FI_CXI_DEFAULT_CQ_SIZE=131072\nexport FI_CXI_OFLOW_BUF_SIZE=8388608\nexport FI_CXI_CQ_FILL_PERCENT=20\n</code></pre> <p>The value of <code>FI_CXI_DEFAULT_CQ_SIZE</code> can be set to something larger if issues persist. This is directly impacted by the number of unexpected messages sent and so may need to be increased as the scale of the job increases. </p> <ol> <li><code>double free detected</code> output while running with the mpich/52.2/* modules</li> </ol> <p>A core dump might indicate communicator cleanup e.g. after calling MPI_Comm_split_type. A workaround is to unset a few config-file related variables:  <pre><code>unset MPIR_CVAR_CH4_COLL_SELECTION_TUNING_JSON_FILE\nunset MPIR_CVAR_COLL_SELECTION_TUNING_JSON_FILE\nunset MPIR_CVAR_CH4_POSIX_COLL_SELECTION_TUNING_JSON_FILE\n</code></pre> Additional information is here: https://github.com/pmodels/mpich/pull/6730 </p> <ol> <li>Slower-than expected GPU-Aware MPI: You can try one of those 2 set of env:</li> <li> <p>RDMA <pre><code>            export MPIR_CVAR_CH4_OFI_ENABLE_HMEM=1\n            export MPIR_CVAR_CH4_OFI_ENABLE_MR_HMEM=0\n            export MPIR_CVAR_CH4_OFI_ENABLE_MULTI_NIC_STRIPING=0\n            export MPIR_CVAR_CH4_OFI_MAX_NICS=8\n            export MPIR_CVAR_CH4_OFI_GPU_RDMA_THRESHOLD=0\n</code></pre></p> </li> <li> <p>Pipelining <pre><code>            export MPIR_CVAR_CH4_OFI_ENABLE_GPU_PIPELINE=1\n            export MPIR_CVAR_CH4_OFI_GPU_PIPELINE_THRESHOLD=0\n            export MPIR_CVAR_CH4_OFI_GPU_PIPELINE_BUFFER_SZ=4194304\n            export MPIR_CVAR_CH4_OFI_GPU_PIPELINE_NUM_BUFFERS_PER_CHUNK=256\n            export MPIR_CVAR_CH4_OFI_GPU_PIPELINE_MAX_NUM_BUFFERS=256\n            export MPIR_CVAR_CH4_OFI_GPU_PIPELINE_D2H_ENGINE_TYPE=0\n</code></pre></p> </li> <li> <p>Compiler error like <pre><code>_libm_template.c:(.text+0x7): failed to convert GOTPCREL relocation against '__libm_acos_chosen_core_func_x'; relink with --no-relax\n</code></pre> in SYCL</p> </li> <li> <p>Please try linking with <code>-flink-huge-device-code</code></p> </li> <li> <p>General MPI Error</p> </li> </ol> <p>Similar to Issue #1, it maybe be useful to use other libfabric environment settings. In particular, the setting below may be useful to try. These are what what Cray MPI sets by default Cray MPI libfabric Settings. <pre><code>export FI_CXI_RDZV_THRESHOLD=16384\nexport FI_CXI_RDZV_EAGER_SIZE=2048\nexport FI_CXI_DEFAULT_CQ_SIZE=131072\nexport FI_CXI_DEFAULT_TX_SIZE=1024\nexport FI_CXI_OFLOW_BUF_SIZE=12582912\nexport FI_CXI_OFLOW_BUF_COUNT=3\nexport FI_CXI_RX_MATCH_MODE=hardware\nexport FI_CXI_REQ_BUF_MIN_POSTED=6\nexport FI_CXI_REQ_BUF_SIZE=12582912\nexport FI_MR_CACHE_MAX_SIZE=-1\nexport FI_MR_CACHE_MAX_COUNT=524288\nexport FI_CXI_REQ_BUF_MAX_CACHED=0\nexport FI_CXI_REQ_BUF_MIN_POSTED=6\n</code></pre></p> <ol> <li>SYCL Device Free Memory Query Error</li> </ol> <p>Note that if you are querying the free memory on a device with the Intel SYCL extension \"get_info();\", you will need to set <code>export ZES_ENABLE_SYSMAN=1</code>. Otherwise you may see an error like: <pre><code>x1921c1s4b0n0.hostmgmt2000.cm.americas.sgi.com 0: The device does not have the ext_intel_free_memory aspect -33 (PI_ERROR_INVALID_DEVICE)\nx1921c1s4b0n0.hostmgmt2000.cm.americas.sgi.com 0: terminate called after throwing an instance of 'sycl::_V1::invalid_object_error'\n  what():  The device does not have the ext_intel_free_memory aspect -33 (PI_ERROR_INVALID_DEVICE)\n</code></pre>"},{"location":"aurora/known-issues/#submitting-jobs","title":"Submitting Jobs","text":"<p>Jobs may fail to successfully start at times (particularly at higher node counts). If no error message is apparent, then one thing to check is the <code>comment</code> field in the full job information for the job using the command <code>qstat -xfw [JOBID] | grep comment</code>. Some example comments follow.</p> <p><pre><code>comment = Job held by [USER] on Tue Feb 6 05:20:00 2024 and terminated\n</code></pre> The user has placed the job on hold; user can <code>qrls</code> the job when ready for it to be queued again.</p> <pre><code>comment = Not Running: Queue not started. and terminated\n</code></pre> <p>User has submitted to a queue that is not currently running; user should <code>qmove</code> the job to an appropriate queue.</p> <pre><code>comment = job held, too many failed attempts to run\n</code></pre> <p>The job tried and failed to start. In this scenario, the user should find that their job was placed on hold. This does not indicate a problem the users' job script, but indicates PBS made several attempts to find a set of nodes to run the job and was not able too. Users can <code>qdel</code> the job and resubmit or <code>qrls</code> the job to try running it again.</p> <pre><code>comment = Not Running: Node is in an ineligible state: down and terminated\n</code></pre> <p>There are an insufficient number of nodes are online and free for the job to start</p> <p>In the event of a node going down during a job, users may encounter messages such as <code>ping failed on x4616c0s4b0n0: Application 047a3c9f-fb41-4595-a2ad-4a4d0ec1b6c1 not found</code>. The node will likely have started a reboot and won't be included in jobs again until checks pass.</p> <p>To increase the chances that a large job does not terminate due to a node failure, you may choose to interactively route your MPI job around nodes that fail during your run. See this page on Working Around Node Failures for more information.</p>"},{"location":"aurora/known-issues/#other-issues","title":"Other issues","text":"<ul> <li>Interim Filesystem: The early access filesystem is not highly performant. Intermittent hangs or pauses should be expected - waiting for IO to complete is recommended and IO completions should pass without failure. Jobs requiring significant filesystem performance must be avoided at this time.</li> <li>Large number of Machine Check Events from the PVC, that causes nodes to panic and reboot.</li> <li>HBM mode is not automatically validated. Jobs requiring flat memory mode should test by looking  at <code>numactl -H</code> for 4 NUMA memory nodes instead of 16 on the nodes.</li> <li>Application failures at large node-count are being tracked in the CNDA Slack workspace. See this canvas table for more information and to document your case. ESP and ECP project members with access to Aurora should have access to the CNDA slack workspace. Contact support@alcf.anl.gov if you have have access to Aurora and belong to an ESP or ECP project,  but are not in the CNDA Slack workspace.</li> <li>Application failures at single-node are tracked in the JLSE wiki/confluence page</li> </ul>"},{"location":"aurora/running-jobs-aurora/","title":"Running Jobs on Aurora","text":""},{"location":"aurora/running-jobs-aurora/#queues","title":"Queues","text":"<p>There is a single routing queue in place called <code>EarlyAppAccess</code> which submits to the <code>LustreApps</code> queue. The total number of nodes available on this queue is changing often.</p> <p>Queue Policy: 1 RUNNING job per user.</p> <p>For example, a one-node interactive job can be requested for 30 minutes with the following command, where <code>[your_ProjectName]</code> is replaced with an appropriate project name.</p> <pre><code>qsub -l select=1 -l walltime=30:00 -A [your_ProjectName] -q EarlyAppAccess -I\n</code></pre> <p>Recommended PBSPro options follow.</p> <pre><code>#!/bin/sh\n#PBS -A [your_ProjectName]\n#PBS -N\n#PBS -l walltime=[requested_walltime_value]\n#PBS -k doe\n#PBS -l place=scatter\n#PBS -q EarlyAppAccess\n</code></pre>"},{"location":"aurora/running-jobs-aurora/#working-around-node-failures","title":"Working Around Node Failures","text":"<p>As Aurora is still a pre-production supercomputer, node failures are a fact of life. If you would like to increase the chances that a large job does not terminate due to a node failure, you may choose to interactively route your MPI job around nodes that fail during your run. To do this, you must run interactively and use must manually adjust your run on the fly to remove nodes that have been marked as failed.</p> <p>We recommend against useing <code>-W tolerate_node_failures=all</code> in your qsub command, but we acknowledge its use can be helpful. However, you MUST MANUALLY VERIFY your job and remove faulted nodes from your mpiexec command YOURSELF!</p> <ol> <li>Start your interactive job</li> <li>When the job transitions to Running state, run <code>pbsnodes -l | grep &lt;jobid&gt;</code></li> <li> <p>Manually REMOVE all nodes identified in that output from inclusion in your mpiexec</p> <pre><code>$ cat $PBS_NODEFILE &gt; local.hostfile\n# edit local.hostfile to remove problem nodes\n$ mpiexec --hostfile local.hostfile [other mpiexec arguments]\n</code></pre> </li> <li> <p>Continue to execute</p> </li> <li>If other nodes go down during your job, it will not be killed, and you can further exclude those nodes from your mpiexec as needed</li> </ol> <p>It is important to note that all nodes marked as faulty by PBS will not be used in subsequent jobs. This mechanism only provides you with a means to execute additional mpiexec commands under the same interactive job after manually removing nodes identified as faulty. Once your PBS job has exited, those faulty nodes will remain offline until further intervention by Aurora staff.</p>"},{"location":"aurora/running-jobs-aurora/#aurora-mpich","title":"Aurora MPICH","text":"<p>The standard version of the MPI (Message Passing Interface) library on Aurora is Aurora MPICH. This resulted from a collaboration between Intel and the Argonne MPICH developer team. The <code>mpiexec</code> and <code>mpirun</code> commands used to launch multi-rank jobs come from the Cray PALS (Parallel Application Launch Service) system.</p> <p>There are many, many configuration and tuning parameters for Aurora MPICH. Simple ASCII text documentation of the environment variables usable to control behavior is in</p> <pre><code>$MPI_ROOT/share/doc/mpich/README.envvar\n</code></pre> <p>This includes, for example, settings to select different optional sub-algorithms used in MPI collective operations.</p>"},{"location":"aurora/running-jobs-aurora/#running-mpiopenmp-applications","title":"Running MPI+OpenMP Applications","text":"<p>Once a submitted job is running calculations can be launched on the compute nodes using <code>mpiexec</code> to start an MPI application. Documentation is accessible via <code>man mpiexec</code> and some helpful options follow.</p> <ul> <li><code>-n</code> total number of MPI ranks</li> <li><code>-ppn</code> number of MPI ranks per node</li> <li><code>--cpu-bind</code> CPU binding for application</li> <li><code>--depth</code> number of cpus per rank (useful with <code>--cpu-bind</code>)</li> <li><code>--env</code> set environment variables (<code>--env OMP_NUM_THREADS=2</code>)</li> <li><code>--hostfile</code> indicate file with hostnames (the default is <code>--hostfile $PBS_NODEFILE</code>)</li> </ul> <p>A sample submission script with directives is below for a 4-node job with 28 MPI ranks on each node and 4 OpenMP threads per rank (1 per CPU core).</p> <pre><code>#!/bin/bash -l\n#PBS -N AFFINITY\n#PBS -l select=4\n#PBS -l place=scatter\n#PBS -l walltime=0:10:00\n#PBS -q workq\n#PBS -A MYPROJECT\n\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS=28 # Number of MPI ranks to spawn per node\nNDEPTH=4 # Number of hardware threads per rank (i.e. spacing between MPI ranks)\nNTHREADS=4 # Number of software threads per rank to launch (i.e. OMP_NUM_THREADS)\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS} THREADS_PER_RANK= ${NTHREADS}\"\n\ncd /home/knight/affinity\nmpiexec -n ${NTOTRANKS} -ppn ${NRANKS} --depth=${NDEPTH} --cpu-bind depth -env OMP_NUM_THREADS=${NTHREADS} --env OMP_PLACES=cores ./hello_affinity\n</code></pre>"},{"location":"aurora/running-jobs-aurora/#running-gpu-enabled-applications","title":"Running GPU-enabled Applications","text":"<p>GPU-enabled applications will similarly run on the compute nodes using the above example script. - The environment variable <code>MPICH_GPU_SUPPORT_ENABLED=1</code> needs to be set if your application requires MPI-GPU support whereby the MPI library sends and receives data directly from GPU buffers. - If running on a specific GPU or subset of GPUs and/or tiles is desired, then the <code>ZE_AFFINITY_MASK</code> environment variable can be used. For example, if one only wanted an application to access the first two GPUs on a node, then setting <code>ZE_AFFINITY_MASK=0,1</code> could be used.</p>"},{"location":"aurora/running-jobs-aurora/#mpi-rank-and-thread-binding-to-cores-and-gpus","title":"MPI rank and thread binding to cores and GPUs","text":"<p>Each node on Aurora has 2 sockets, each with 2 CPUs and 3 PVC GPUs. Each CPU has 52 physical cores, with 2 logical processors (provided by Intel hyper threading) per physical core, for a total of 104 physical cores and 208 logical processors on the CPUs per Aurora node. Each GPU has two tiles on it, for a total of 6 GPUs and 12 GPU tiles on the GPUs per Aurora node. When a parallel job is run, the job must have some way of mapping MPI ranks or threads to each of the 208 logical processors and 6 GPUs or 12 GPU tiles. Mapping is typically done by an affinity mask, which assigns hardware resources to each MPI rank or thread to use.</p> <p>A visual representation of node in Aurora is shown below. Each socket is represented by a large blue bubble. Inside, each CPU is represented by a red bubble. Inside of CPU, the white boxes represent the physical cores, and the two grey squares in each tile represent the two logical processors. Each GPU is represented by a large white box, with two grey boxes inside to represent the two tiles.</p> <p> </p> Simplified representation of Aurora node  <p>For the two CPUs, the numbers inside the boxes identify the specific logical processors in the core. That is, logical processor 0 and 104 are the 2 logical processors on the first physical core. Logical processors 1 and 105 are the 2 logical processors that share the second physical core. Since there are 208 logical processors, the numbers run from 0 to 207. For i from 0 to 51, logical processors i and i+104 share a physical core. </p> <p>For the six GPUs, the GPU number identifies the GPU, and the tile numbers identify the tile in the GPU, with tiles from 0 to 5 with each GPU have two tiles each $gpu.0 and $gpu.1.</p>"},{"location":"aurora/running-jobs-aurora/#binding-mpi-ranks-and-threads-to-cores","title":"Binding MPI ranks and threads to cores","text":"<p>Using the \u2013cpu-bind argument to mpiexec, MPI ranks and threads can be assigned to run on specific logical processors on the CPUs. For more information about the flags to mpiexec, see Running MPI+OpenMP-Applications. Four examples of using mpiexec are given below to show how the cpu-bind=depth, cpu-bind=list, --depth arguments affect where MPI ranks and OpenMP threads are mapped.</p>"},{"location":"aurora/running-jobs-aurora/#example-1-2-nodes-4-ranksnode-1-threadrank","title":"Example 1: 2 nodes, 4 ranks/node, 1 thread/rank","text":"<pre><code>mpiexec -n 8 -ppn 4 --depth 1 --cpu-bind=depth &lt;app&gt; &lt;app_args&gt;\n</code></pre> <ul> <li>The \"-n 8\" argument says to use 8 MPI ranks in total and \"-ppn 4\" places 4 ranks per node.</li> <li>The \"--depth 1\" argument says to use 1 logical processor for each MPI rank.</li> <li>The \"--cpu-bind depth\" argument says to spread out the ranks in a round robin manner across the logical processors, first putting one rank on the first logical processor of one physical core, and then looping back to put a second one on the second logical processor. This is done such that there's N logical processors for each MPI rank, where N is the value from the --depth argument (so it's 1 in this case).</li> </ul> <p>This is the same as</p> <pre><code>mpiexec -n 8 -ppn 4 --cpu-bind=list:0:1:2:3 &lt;app&gt; &lt;app_args&gt;\n</code></pre> <ul> <li>The \"--cpu-bind list\" argument explicitly lists which logical processor to bind to per node. Each MPI rank is bound to the logical processors that are listed between \":\". So here, rank 0 to logical processor 0, rank 1 to logical processor 1, etc.</li> </ul>"},{"location":"aurora/running-jobs-aurora/#resulting-mapping","title":"Resulting mapping","text":"<p>MPI ranks 0,1,2,3,4,5,6,7 map to logical processors 0,1,2,3 on each of the two nodes. Assuming the job was allocated on node 0 and node 1:</p> <ul> <li> <p>MPI rank 0 \u2192 node 0, logical processor 0</p> </li> <li> <p>MPI rank 1 \u2192 node 0, logical processor 1</p> </li> <li> <p>MPI rank 2 \u2192 node 0, logical processor 2</p> </li> <li> <p>MPI rank 3 \u2192 node 0, logical processor 3</p> </li> <li> <p>MPI rank 4 \u2192 node 1, logical processor 0</p> </li> <li> <p>MPI rank 5 \u2192 node 1, logical processor 1</p> </li> <li> <p>MPI rank 6 \u2192 node 1, logical processor 2</p> </li> <li> <p>MPI rank 7 \u2192 node 1, logical processor 3</p> </li> </ul> <p>The figure below shows the mapping, where the different colors are different MPI ranks.</p> <p> </p> Example 1 Mapping"},{"location":"aurora/running-jobs-aurora/#example-2-2-nodes-2-ranksnode-2-threadrank","title":"Example 2: 2 nodes, 2 ranks/node, 2 thread/rank","text":"<pre><code>OMP_PLACES=threads OMP_NUM_THREADS=2 mpiexec -n 4 -ppn 2 --depth 2 --cpu-bind=depth &lt;app&gt; &lt;app_args&gt;\n</code></pre> <ul> <li>The \"-n 4\" argument says to use 4 MPI ranks in total and \"-ppn 2\" places 2 ranks per node.</li> <li>The \"--depth 2\" argument says to use 2 logical processor for each MPI rank.</li> <li>The \"--cpu-bind depth\" argument says to spread out the ranks in a round robin manner across the logical processors, first putting one rank on the first logical processor of one physical core, and then looping back to put a second one on the second logical processor. This is done such that there's N logical processors for each MPI rank, where N is the value from the --depth argument (so it's 2 in this case).</li> <li>OMP_NUM_THREADS=2 launches two threads per MPI rank</li> <li>OMP_PLACES=threads says to bind the OpenMP threads to logical processors</li> </ul> <p>This is the same as</p> <pre><code>OMP_PLACES=threads OMP_NUM_THREADS=2 mpiexec -n 4 -ppn 2 --cpu-bind=list:0,1:2,3 &lt;app&gt; &lt;app_args&gt;\n</code></pre> <ul> <li>The \"--cpu-bind list\" argument explicitly lists which logical processor to bind to. Each MPI rank is bound to the logical processors that are listed between \":\". Between \":\", the logical processors to bind to are listed in a comma-separated manner. So here, rank 0 is bound to logical processors 0 and 1, rank 2 to logical processors 2 and 3. OMP_PLACES=threads then binds the specific threads to the logical processors in the list.</li> </ul>"},{"location":"aurora/running-jobs-aurora/#resulting-mapping_1","title":"Resulting mapping","text":"<p>Assuming the job was allocated on node 0 and node 1:</p> <ul> <li> <p>MPI rank 0, OpenMP thread 0 \u2192 node 0, logical processor 0</p> </li> <li> <p>MPI rank 0, OpenMP thread 1 \u2192 node 0, logical processor 1</p> </li> <li> <p>MPI rank 1, OpenMP thread 0 \u2192 node 0, logical processor 2</p> </li> <li> <p>MPI rank 1, OpenMP thread 1 \u2192 node 0, logical processor 3</p> </li> <li> <p>MPI rank 2, OpenMP thread 0 \u2192 node 1, logical processor 0</p> </li> <li> <p>MPI rank 2, OpenMP thread 1 \u2192 node 1, logical processor 1</p> </li> <li> <p>MPI rank 3, OpenMP thread 0 \u2192 node 1, logical processor 2</p> </li> <li> <p>MPI rank 3, OpenMP thread 1 \u2192 node 1, logical processor 3</p> </li> </ul> <p>The figure below shows the mapping, where the different colors are different MPI ranks.</p> <p> </p> Example 2 Mapping"},{"location":"aurora/running-jobs-aurora/#example-3-2-nodes-2-ranksnode-1-threadrank-compact-fashion","title":"Example 3: 2 nodes, 2 ranks/node, 1 thread/rank, compact fashion","text":"<pre><code>mpiexec -n 4 -ppn 2 --cpu-bind=list:0:104 &lt;app&gt; &lt;app_args&gt;\n</code></pre> <ul> <li>The \"--cpu-bind list\" argument explicitly lists which logical processor to bind to per node. Each MPI rank is bound to the logical processors that are listed between \":\". So here, rank 0 to logical processor 0, rank 1 to logical processor 104, which share the same physical core.</li> </ul>"},{"location":"aurora/running-jobs-aurora/#resulting-mapping_2","title":"Resulting mapping","text":"<p>Assuming the job was allocated on node 0 and node 1:</p> <ul> <li> <p>MPI rank 0 \u2192 node 0, logical processor 0</p> </li> <li> <p>MPI rank 1 \u2192 node 0, logical processor 104</p> </li> <li> <p>MPI rank 2 \u2192 node 1, logical processor 0</p> </li> <li> <p>MPI rank 3 \u2192 node 1, logical processor 104</p> </li> </ul> <p>The figure below shows the mapping, where the different colors are different MPI ranks.</p> <p> </p> Example 3 Mapping"},{"location":"aurora/running-jobs-aurora/#example-4-1-node-12-ranksnode","title":"Example 4: 1 node, 12 ranks/node","text":"<p>This setup is a common case for applications: 12 ranks/node, where each rank will offload to one of the 12 GPU tiles. Note that explicit list binding is needed here to avoid binding a MPI rank to a logical processor on different socket than the GPU it might be targetting (as would happen if cpu_bind=depth was used). </p> <pre><code>mpiexec -n 12 -ppn 12 --cpu-bind=list:0-7:8-15:16-23:24-31:32-39:40-47:52-59:60-67:68-75:76-83:84-91:92-99 &lt;app&gt; &lt;app_args&gt;\n</code></pre> <ul> <li>The \"--cpu-bind list\" argument explicitly lists which logical processor to bind to per node. Each MPI rank is bound to the logical processors that are listed between \":\". So here, rank 0 to logical processors 0-7, rank 1 to logical processors 8-15, etc.</li> </ul>"},{"location":"aurora/running-jobs-aurora/#resulting-mapping_3","title":"Resulting mapping","text":"<p>Assuming the job was allocated on node 0 and node 1, the mapping looks like:</p> <ul> <li> <p>MPI rank 0 \u2192 node 0, socket 0, logical processors 0-7</p> </li> <li> <p>MPI rank 1 \u2192 node 0, socket 0, logical processor 8-15</p> </li> <li> <p>MPI rank 2 \u2192 node 0, socket 0, logical processor 16-23</p> </li> <li> <p>MPI rank 3 \u2192 node 0, socket 0, logical processor 24-31</p> </li> <li> <p>MPI rank 4 \u2192 node 0, socket 0, logical processor 32-39</p> </li> <li> <p>MPI rank 5 \u2192 node 0, socket 0, logical processor 40-47</p> </li> <li> <p>MPI rank 6 \u2192 node 0, socket 1, logical processor 52-59</p> </li> <li> <p>MPI rank 7 \u2192 node 0, socket 1, logical processor 60-67</p> </li> <li> <p>MPI rank 8 \u2192 node 0, socket 1, logical processor 68-75</p> </li> <li> <p>MPI rank 9 \u2192 node 0, socket 1, logical processor 76-83</p> </li> <li> <p>MPI rank 10 \u2192 node 0, socket 1, logical processor 84-91</p> </li> <li> <p>MPI rank 11 \u2192 node 0, socket 1, logical processor 92-99</p> </li> </ul> <p>The important point here is that with explicit binding, we were able to ensure socket 0 had 6 ranks and socket 1 has 6 ranks. Note how MPI rank 5 ends at logical processor 47, but MPI rank 6 begins with logical processor 52, so this involves leaving several cores empty. However, it allows the cores to be spread evenly across the two sockets.   </p> <p>The figure below shows the mapping, where the different colors are different MPI ranks.</p> <p> </p> Example 4 Mapping  <p>If instead we used \"--depth\" as so: <pre><code>mpiexec -n 12 -ppn 12 --depth 8 --cpu-bind=depth &lt;app&gt; &lt;app_args&gt;\n</code></pre> then the mapping is:</p> <ul> <li> <p>MPI rank 0 \u2192 node 0, socket 0, logical processors 0-7</p> </li> <li> <p>MPI rank 1 \u2192 node 0, socket 0, logical processor 8-15</p> </li> <li> <p>MPI rank 2 \u2192 node 0, socket 0, logical processor 16-23</p> </li> <li> <p>MPI rank 3 \u2192 node 0, socket 0, logical processor 24-31</p> </li> <li> <p>MPI rank 4 \u2192 node 0, socket 0, logical processor 32-39</p> </li> <li> <p>MPI rank 5 \u2192 node 0, socket 0, logical processor 40-47</p> </li> <li> <p>MPI rank 6 \u2192 node 0, socket 0 and socket 1, logical processor 48-55</p> </li> <li> <p>MPI rank 7 \u2192 node 0, socket 1, logical processor 56-63</p> </li> <li> <p>MPI rank 8 \u2192 node 0, socket 1, logical processor 64-71</p> </li> <li> <p>MPI rank 9 \u2192 node 0, socket 1, logical processor 72-79</p> </li> <li> <p>MPI rank 10 \u2192 node 0, socket 1, logical processor 80-87</p> </li> <li> <p>MPI rank 11 \u2192 node 0, socket 1, logical processor 88-95</p> </li> </ul> <p>Note that the threads MPI rank 6 are bound to cross both socket 0 and socket 1, which potentially will lead to worse performance than using cpu-bind=list to explicitly spread out the ranks and avoid splitting one over two sockets. This is shown in the image below. Note that the pink MPI rank (rank 6) is split between socket 0 and socket 1.</p> <p> </p> Example 4 Mapping Which Splits a MPI Rank Across Sockets  <p>NOTE: For a script to help provide cpu-bindings, you can use get_cpu_bind_aurora. Please see User Guide for Aurora CPU Binding Script for documentation. </p>"},{"location":"aurora/running-jobs-aurora/#binding-mpi-ranks-to-gpus","title":"Binding MPI ranks to GPUs","text":"<p>Support in MPICH on Aurora to bind MPI ranks to GPUs is currently work-in-progress. For applications that need this support, this instead can be handled by use of a small helper script that will appropriately set <code>ZE_AFFINITY_MASK</code> for each MPI rank. Users are encouraged to use the <code>/soft/tools/mpi_wrapper_utils/gpu_tile_compact.sh</code> script for instances where each MPI rank is to be bound to a single GPU tile with a round-robin assignment.</p> <p>This script can be placed just before the executable in an <code>mpiexec</code> command like so.</p> <pre><code>mpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth /soft/tools/mpi_wrapper_utils/gpu_tile_compact.sh ./hello_affinity\n</code></pre> <p>A simple version of this script is below to illustrate how <code>ZE_AFFINITY_MASK</code> is uniquely set for each MPI rank.</p> <pre><code>#!/bin/bash -l\nnum_gpu=6\nnum_tile=2\ngpu_id=$(( (PALS_LOCAL_RANKID / num_tile ) % num_gpu ))\ntile_id=$((PALS_LOCAL_RANKID % num_tile))\nunset EnableWalkerPartition\nexport ZE_ENABLE_PCI_ID_DEVICE_ORDER=1\nexport ZE_AFFINITY_MASK=$gpu_id.$tile_id\n#echo \u201cRANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}\u201d\nexec \"$@\"\n</code></pre> <p>Users with different MPI-GPU affinity needs, such as assigning multiple GPUs/tiles per MPI rank, are encouraged to modify a local copy of <code>/soft/tools/mpi_wrapper_utils/gpu_tile_compact.sh</code> to suit their needs.</p> <p>One example below shows a common mapping of MPI ranks to cores and GPUs.</p>"},{"location":"aurora/running-jobs-aurora/#example-1-1-node-12-ranksnode-1-threadrank-1-rankgpu","title":"Example 1: 1 node, 12 ranks/node, 1 thread/rank, 1 rank/GPU","text":"<pre><code>mpiexec -n 12 -ppn 12 --cpu-bind=list:0-7:8-15:16-23:24-31:32-39:40-47:52-59:60-67:68-75:76-83:84-91:92-99 /soft/tools/mpi_wrapper_utils/gpu_tile_compact.sh &lt;app&gt; &lt;app_args&gt;\n</code></pre> <ul> <li>The \"-n 12\" argument says to use 12 MPI ranks in total and \"-ppn 12\" places 12 ranks per node.</li> <li>The \"--cpu-bind list\" argument gives the mapping of MPI ranks to cores, as described in Binding MPI ranks and threads to cores.</li> <li>The /soft/tools/mpi_wrapper_utils/gpu_tile_compact.sh wrapper sets ZE_AFFINITY_MASK for each of the 12 ranks such that rank 0 maps to GPU 0, Tile 0, rank 1 maps to GPU 0, Tile 1, rank 2 naps to GPU 1, Tile 0 etc. in a round-robin compact fashion.  </li> </ul>"},{"location":"aurora/running-jobs-aurora/#resulting-mapping_4","title":"Resulting mapping","text":"<p>This is one of the most common cases, with 1 MPI rank targeting each GPU tile. A figure representing this is below. The different MPI ranks are represented by different colors. Assuming the job was allocated on node 0 and node 1, the mapping looks like:</p> <ul> <li> <p>MPI rank 0 \u2192 node 0, socket 0, logical processors 0-7, GPU 0, Tile 0</p> </li> <li> <p>MPI rank 1 \u2192 node 0, socket 0, logical processor 8-15, GPU 0, Tile 1</p> </li> <li> <p>MPI rank 2 \u2192 node 0, socket 0, logical processor 16-23, GPU 1, Tile 0</p> </li> <li> <p>MPI rank 3 \u2192 node 0, socket 0, logical processor 24-31, GPU 1, Tile 1</p> </li> <li> <p>MPI rank 4 \u2192 node 0, socket 0, logical processor 32-39, GPU 2, Tile 0</p> </li> <li> <p>MPI rank 5 \u2192 node 0, socket 0, logical processor 40-47, GPU 2, Tile 1</p> </li> <li> <p>MPI rank 6 \u2192 node 0, socket 1, logical processor 52-59, GPU 3, Tile 0</p> </li> <li> <p>MPI rank 7 \u2192 node 0, socket 1, logical processor 60-67, GPU 3, Tile 1</p> </li> <li> <p>MPI rank 8 \u2192 node 0, socket 1, logical processor 68-75, GPU 4, Tile 0</p> </li> <li> <p>MPI rank 9 \u2192 node 0, socket 1, logical processor 76-83, GPU 4, Tile 1</p> </li> <li> <p>MPI rank 10 \u2192 node 0, socket 1, logical processor 84-91, GPU 5, Tile 0</p> </li> <li> <p>MPI rank 11 \u2192 node 0, socket 1, logical processor 92-99, GPU 5, Tile 1</p> </li> </ul> <p> </p> Example 1 GPU Tile Mapping"},{"location":"aurora/running-jobs-aurora/#interactive-jobs-on-compute-nodes","title":"Interactive Jobs on Compute Nodes","text":"<p>Here is how to submit an interactive job to, for example, edit/build/test an application on Aurora compute nodes:</p> <pre><code>qsub -I -l select=1,walltime=1:00:00,place=scatter -A MYPROJECT -q workq\n</code></pre> <p>This command requests 1 node for a period of 1 hour in the <code>workq</code> queue. After waiting in the queue for a node to become available, a shell prompt on a compute node will appear. You may then start building applications and testing gpu affinity scripts on the compute node.</p> <p>NOTE: If you want to <code>ssh</code> or <code>scp</code> to one of your assigned compute nodes you will need to make sure your <code>$HOME</code> directory and your <code>$HOME/.ssh</code> directory permissions are both set to <code>700</code>.</p>"},{"location":"aurora/running-jobs-aurora/#running-multiple-mpi-applications-on-a-node","title":"Running Multiple MPI Applications on a node","text":"<p>Multiple applications can be run simultaneously on a node by launching several <code>mpiexec</code> commands and backgrounding them. For performance, it will likely be necessary to ensure that each application runs on a distinct set of CPU resources and/or targets specific GPUs and tiles. One can provide a list of CPUs using the <code>--cpu-bind</code> option, which when combined with <code>ZE_AFFINITY_MASK</code> provides a user with specifying exactly which CPU and GPU resources to run each application on. In the simple example below, twelve instances of the application are simultaneously running on a single node. In the first instance, the application is spawning MPI ranks 0-3 on CPU cores 0-3 and using GPU 0 tile 0.</p> <pre><code>export OMP_NUM_THREADS=1\nexport ZE_ENABLE_PCI_ID_DEVICE_ORDER=1\n\nexport ZE_AFFINITY_MASK=0.0\nmpiexec --np 4 --ppn 4 --cpu-bind list:0:1:2:3 ./hello_affinity &amp;\n\nexport ZE_AFFINITY_MASK=0.1\nmpiexec -n 4 --ppn 4 --cpu-bind list:4:5:6:7 ./hello_affinity &amp;\n\nexport ZE_AFFINITY_MASK=1.0\nmpiexec -n 4 --ppn 4 --cpu-bind list:8:9:10:11 ./hello_affinity &amp;\n\n\nexport ZE_AFFINITY_MASK=5.1\nmpiexec -n 4 --ppn 4 --cpu-bind list:40:41:42:43 ./hello_affinity &amp;    \n\nwait\n</code></pre> <p>Users will likely find it beneficial to launch processes across CPU cores in both sockets of a node.</p>"},{"location":"aurora/running-jobs-aurora/#compute-node-access-to-the-internet","title":"Compute Node Access to the Internet","text":"<p>Currently, the only access to the internet is via a proxy.  Here are the proxy environment variables for Aurora:</p> <pre><code>export http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\n</code></pre> <p>In the future, though we don't have a timeline on this because it depends on future features in slingshot and internal software development, we intend to have public IP addresses be a schedulable resource.  For instance, if only your head node needed public access your select statement might looks something like: <code>-l select=1:pubnet=True+63</code>.</p>"},{"location":"aurora/running-jobs-aurora/#controlling-where-your-job-runs","title":"Controlling Where Your Job Runs","text":"<p>If you wish to have your job run on specific nodes form your select like this: <code>-l select=1:vnode=&lt;node name1&gt;+1:vnode=&lt;node name2&gt;...</code> . Obviously, that gets tedious for large jobs.</p> <p>If you want to control the location of a few nodes, for example 2 out of 64, but the rest don't matter, you can do something like this: <code>-l select=1:vnode=&lt;node name1&gt;+1:vnode=&lt;node name2&gt;+62:system=foo</code>.</p>"},{"location":"aurora/running-jobs-aurora/#network-rack-and-dragonfly-group-mappings","title":"Network: Rack and Dragonfly Group Mappings","text":"<p>Content coming soon.</p>"},{"location":"aurora/sunspot-to-aurora/","title":"Transitioning from Sunspot to Aurora","text":"<p>Sunspot is two racks (128 nodes) of the same hardware as Aurora that teams were initially given access to for testing. Virtually all hardware and software aspects of Sunspot are identical to Aurora, so the transition to Aurora should largely be staightforward in terms of compiling applications and submitting batch jobs. </p> <p>The primary documentation source for Sunspot for pre-production users (ECP and ESP) is Getting Started on Sunspot.</p>"},{"location":"aurora/sunspot-to-aurora/#transferring-files-to-aurora","title":"Transferring files to Aurora","text":"<p>One key immediate difference that users will need to account for is that Sunspot (gila) and Aurora (gecko) mount their own independent filesystems and DAOS systems. This means that Sunspot users will need to manually transfer files and/or DAOS containers to Aurora's filesystem if they desire to retain anything they've created on Sunspot.</p> <p>With the bastion pass-through nodes currently used to access both Sunspot and Aurora, users will find it helpful to modify their <code>.ssh/config</code> files appropriately to facilitate transfers to Aurora. These changes are similar to what Sunspot users may have already implemented and a simple example follows.</p> <pre><code>$ cat .ssh/config\nknight@aurora-uan-0009:~&gt; cat .ssh/config \n\nHost bastion.alcf.anl.gov\n     User knight\n\nHost *.sunspot.alcf.anl.gov sunspot.alcf.anl.gov\n     ProxyJump bastion.alcf.anl.gov\n     DynamicForward 3142\n     user knight\n\nHost polaris.alcf.anl.gov\n     ProxyJump bastion.alcf.anl.gov\n     DynamicForward 3142\n     user knight\n</code></pre> <p>From an Aurora login-node, this readily enables one to transfer files from Sunspot's gila filesystem or one of the production filesystems at ALCF (home, grand, and eagle). With the use of ProxyJump here, entering the MobilePass+ or Cryptocard passcode twice will be needed (once for bastion and once for the other resource).</p> <p>This simple example transfers a file from Sunspot.</p> <pre><code>knight@aurora-uan-0009:~&gt; scp knight@sunspot.alcf.anl.gov:/lus/gila/projects/Aurora_deployment/knight/test.txt ./\n---------------------------------------------------------------------------\n                            Notice to Users\n...\n[Password:\n---------------------------------------------------------------------------\n                            Notice to Users\n... \n[Password:\nknight@aurora-uan-0009:~&gt; cat test.txt \nfrom_sunspot gila\n</code></pre> <p>This simple example transfers a file from the grand filesystem via Polaris.</p> <pre><code>knight@aurora-uan-0009:~&gt; scp knight@polaris.alcf.anl.gov:/grand/catalyst/proj-shared/knight/test.txt ./\n---------------------------------------------------------------------------\n                            Notice to Users\n...\n[Password:\n---------------------------------------------------------------------------\n                            Notice to Users\n... \n[Password:\nknight@aurora-uan-0009:~&gt; cat test.txt \nfrom_polaris grand\n</code></pre>"},{"location":"aurora/sunspot-to-aurora/#default-software-environment","title":"Default software environment","text":"<p>Users should be mindful of differences in the default enviroment on Aurora compared to Sunspot. This is especially important if applications require specific versions of software. As an example, the default oneapi modules on Sunspot is <code>oneapi/eng-compiler/2023.05.15.006</code> compared to <code>oneapi/eng-compiler/2022.12.30.003</code> on Aurora.</p> <pre><code>knight@aurora-uan-0009:~&gt; module list\n\nCurrently Loaded Modules:\n  1) gcc/11.2.0                    3) intel_compute_runtime/release/agama-devel-551   5) libfabric/1.15.2.0   7) cray-libpals/1.2.12\n  2) mpich/51.2/icc-all-pmix-gpu   4) oneapi/eng-compiler/2022.12.30.003              6) cray-pals/1.2.12\n</code></pre> <p>Users on Aurora can access additional versions of software on the login node and in job scripts by appending <code>/soft/modulefiles</code> to their MODULEPATH as in the following example.</p> <pre><code>knight@aurora-uan-0009:~&gt; module use /soft/modulefiles\nknight@aurora-uan-0009:~&gt; module avail oneapi\n\n------------------------------------------------------------------------- /soft/modulefiles -------------------------------------------------------------------------\n   oneapi/eng-compiler/2023.05.15.003    oneapi/eng-compiler/2023.05.15.007        oneapi/release/2023.05.15.001        spack-pe-oneapi/0.4-rc1 (D)\n   oneapi/eng-compiler/2023.05.15.006    oneapi/eng-compiler/2023.10.15.002 (D)    oneapi/release/2023.10.15.001 (D)    spack-pe-oneapi/0.5-rc1\n\n---------------------------------------------------------- /opt/aurora/23.073.0/oneapi/release/modulefiles ----------------------------------------------------------\n   oneapi/release/2022.12.30.001\n\n----------------------------------------------------- /opt/aurora/23.073.0/CNDA/oneapi/eng-compiler/modulefiles -----------------------------------------------------\n   oneapi/eng-compiler/2022.12.30.003 (L)\n</code></pre> <p>Users can then load the desired modules to best match the environment used in work on Sunspot. </p> <pre><code>knight@aurora-uan-0009:~&gt; module swap oneapi/eng-compiler/2022.12.30.003 oneapi/eng-compiler/2023.05.15.006\n\nThe following have been reloaded with a version change:\n  1) intel_compute_runtime/release/agama-devel-551 =&gt; intel_compute_runtime/release/agama-devel-647\n  2) oneapi/eng-compiler/2022.12.30.003 =&gt; oneapi/eng-compiler/2023.05.15.006\n</code></pre>"},{"location":"aurora/applications-and-libraries/libraries/cabana-aurora/","title":"Cabana","text":""},{"location":"aurora/applications-and-libraries/libraries/cabana-aurora/#cabana_1","title":"Cabana","text":"<p>Cabana is built atop Kokkos. It provides class templates useful for implementing particle codes</p>"},{"location":"aurora/applications-and-libraries/libraries/cabana-aurora/#cabana-documentation","title":"Cabana Documentation","text":"<ul> <li>Cabana Wiki</li> <li>Cabana github</li> </ul>"},{"location":"aurora/applications-and-libraries/libraries/cabana-aurora/#cabana-on-aurora","title":"Cabana on Aurora","text":"<p>Built against the prebuilt Kokkos on Aurora, the prebuilt Cabana includes 3 backends: Serial and OpenMP for CPU execution and SYCL for GPU execution. To use it, run</p> <pre><code>module use /soft/modulefiles\nmodule load cabana\n</code></pre> <p>Currently, Cabana is a headers-only installation; there are no libraries per se.</p>"},{"location":"aurora/applications-and-libraries/libraries/math-libraries/","title":"Math Libraries","text":""},{"location":"aurora/applications-and-libraries/libraries/mkl/","title":"MKL","text":""},{"location":"aurora/applications-and-libraries/libraries/mpi/","title":"Aurora MPICH","text":"<p>Placeholder</p>"},{"location":"aurora/applications-and-libraries/libraries/onedal/","title":"oneDAL","text":""},{"location":"aurora/applications-and-libraries/libraries/spack-pe/","title":"Spack PE","text":"<p>The Spack PE is a software stack which provides various build tools, utilities, and libraries. The Spack PE consists of two parts: <code>spack-pe-gcc</code> and <code>spack-pe-oneapi</code>. <code>spack-pe-gcc</code> contains commonly used software packages compiled for CPU. <code>spack-pe-oneapi</code> is based on the E4S Project and provides performant HPC libraries built with the OneAPI SDK. <code>spack-pe-oneapi</code> is dependent on both <code>spack-pe-gcc</code> and the OneAPI SDK; in combination, the Spack PE with the OneAPI SDK and MPICH constitute the Aurora PE. </p>"},{"location":"aurora/applications-and-libraries/libraries/spack-pe/#using-software-from-the-spack-pe","title":"Using software from the Spack PE","text":"<p>The Spack PE is loaded into the environment by default as part of the Aurora PE. To view the available modules, run <code>module avail</code>. A full listing of software including hidden dependencies can be viewed with <code>module --show-hidden avail</code>. The Spack PE modules will be in paths under <code>/opt/aurora/&lt;AURORA_PE_VERSION&gt;/spack</code>. These can be loaded like any other module, for example with <code>module load cmake</code>.</p>"},{"location":"aurora/applications-and-libraries/libraries/spack-pe/#inspecting-packages","title":"Inspecting packages","text":"<p>When a module within the Spack PE is loaded, several environment variables are updated to integrate the package into the user's environment. Additionally, the <code>PACKAGE_ROOT</code> variable is set to contain the path to the installation prefix of the package. For example, after loading <code>cmake</code>:</p> <pre><code>$ echo $CMAKE_ROOT\n/opt/aurora/23.275.2/spack/gcc/0.6.1/install/linux-sles15-x86_64/gcc-12.2.0/cmake-3.27.7-mbl7dvgbiblpavhu53h5cheyrmpaikdz\n$ ls -a $CMAKE_ROOT\n.  ..  bin  doc  share  .spack\n</code></pre> <p>This variable can be used to inspect software installations. Additionally, Spack packages have a <code>.spack</code> directory in the installation prefix which contains build logs and information on configure and build options.</p>"},{"location":"aurora/applications-and-libraries/libraries/spack-pe/#rpath-linking","title":"RPATH linking","text":"<p>Spack PE packages are built with <code>RPATH</code> linking. <code>RPATH</code> hardcodes a default search path for dynamic runtime linking of binaries. By setting <code>RPATH</code>, the loader only needs to search a single path for each library, reducing the number of filesystem calls performed when loading libraries. However, this means <code>LD_LIBRARY_PATH</code> will be ignored when loading binaries installed in the Spack PE. This has been set both to provide a performance benefit and to guarantee proper compatibility of linked libraries. </p> <p>Software installed outside of the Spack PE tree, such as in <code>/soft</code>, will typically be installed with <code>RUNPATH</code> linking or with no runtime search path, both of which respect <code>LD_LIBRARY_PATH</code>. <code>RUNPATH</code> linking, like <code>RPATH</code>, hardcodes a default path, but it does not have precedence over <code>LD_LIBRARY_PATH</code>. Spack PE preview deployments in <code>/soft</code> are installed with <code>RUNPATH</code> linking.</p>"},{"location":"aurora/applications-and-libraries/libraries/spack-pe/#building-software-with-spack","title":"Building software with Spack","text":"<p>Spack is a powerful package manager designed for HPC. The Spack PE is installed and managed with Spack; users can also install Spack in their own home or project directory to manage their software builds. Spack has a steep learning curve, but it may benefit workflows involving frequent builds with complex dependencies.</p> <p>For users who wish to use Spack to install their own software, we provide configuration files corresponding to the Spack PE deployments. These configuration files can be found in <code>/opt/aurora/&lt;AURORA_PE_VERSION&gt;/spack</code> in <code>config</code> directories organized by Spack PE version. Not all of these settings will be useful for all builds and it is not recommended to adopt these wholesale as global settings. The recommended method is to include these settings ad hoc in a spack environment to control what information spack uses for its builds. However, we do recommend using the provided configurations for the compilers, OneAPI SDK components, and MPICH, as these can be difficult to configure properly.</p> <p>Support requests and feedback for ALCF-specific issues should be directed to support@alcf.anl.gov. For general spack questions, users are encouraged to consult the following resources:</p> <ul> <li>Spack development website</li> <li>Spack documentation</li> <li>Spack tutorial</li> <li>Spack Slack channel</li> </ul>"},{"location":"aurora/build-tools/cmake-aurora/","title":"CMake","text":""},{"location":"aurora/build-tools/cmake-aurora/#cmake_1","title":"CMake","text":"<p>CMake is a build configuration system that uses higher-level description files to automatically generate Makefiles.</p>"},{"location":"aurora/build-tools/cmake-aurora/#cmake-documentation","title":"CMake Documentation","text":"<ul> <li>CMake website</li> </ul>"},{"location":"aurora/build-tools/cmake-aurora/#cmake-on-aurora","title":"CMake on Aurora","text":"<p>To use CMake on Aurora, run</p> <pre><code>module load cmake\n</code></pre>"},{"location":"aurora/compiling-and-linking/aurora-example-program-makefile/","title":"Aurora Example Program Makefile","text":"<p>Several simple examples of building CPU and GPU-enabled codes on Aurora are available in the ALCF GettingStarted repo for supported programming models. If building your application on the login node is problematic for some reason (e.g. absense of a GPU), then users are encouraged to build and test applications directly on one of the Aurora compute nodes via an interactive job. The discussion below makes use of the <code>oneAPI</code> compilers in the default environment as illustrative examples.</p>"},{"location":"aurora/compiling-and-linking/aurora-example-program-makefile/#cpu-mpiopenmp-example","title":"CPU MPI+OpenMP Example","text":"<p>One of the first useful tasks with any new machine, schedule, and job launcher is to ensure one is binding MPI ranks and OpenMP threads to the host cpu as intended. A simple HelloWorld MPI+OpenMP example is available here to get started with.</p> <p>The Aurora compute nodes are dual-socket with 52 physical cores in each socket for a total of 104 cores. As hyperthreading is enabled, each core will show up as two CPUs for a total of 208. In many of the examples below, only a single process is spawned on each physical core.</p> <p>The application can be straigthforwardly compiled using the MPICH compiler wrappers in the default environment.</p> <pre><code>mpicxx -g -fopenmp -O3 main.cpp\n</code></pre> <p>The executable <code>hello_affinity</code> can then be launched in a job script (or directly in shell of an interactive job) using <code>mpiexec</code> as discussed here.</p> <pre><code>#!/bin/sh\n#PBS -l select=1\n#PBS -l place=scatter\n#PBS -l walltime=0:15:00\n#PBS -q workq\n#PBS -A Catalyst\n#PBS -l filesystems=home\n\n#cd ${PBS_O_WORKDIR}\n\n# MPI example w/ MPI ranks and OpenMP threads spread evenly across cores (one process per physical core)\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=26\nNDEPTH=4\nNTHREADS=4\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth --env OMP_NUM_THREADS=${NTHREADS} --env OMP_PLACES=cores ./hello_affinity\n</code></pre>"},{"location":"aurora/compiling-and-linking/aurora-example-program-makefile/#gpu-openmp-example","title":"GPU OpenMP Example","text":"<p>A simple OpenMP offload example is available here. Compilation proceeds similar to the above CPU-only example except for the use of compiler flags to enable GPU offload.</p> <pre><code>mpicxx -fiopenmp -fopenmp-targets=spir64 main.cpp\n</code></pre> <p>Running the example with 12 MPI ranks and no other settings will generate output like the following.</p> <pre><code>$ make\n\n$ mpiexec -n 12 --ppn 12 --depth=1 --cpu-bind depth ./hello_affinity\nNUM_OF_NODES= 1 TOTAL_NUM_RANKS= 12 RANKS_PER_NODE= 12 THREADS_PER_RANK= 1\n\n  Using OPENMP v5.0\n  num_devices=     6\n  Default device=  0\n  Host=            6\n  num_teams=       896\n  num_threads=     1\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 0  list_cores= (0)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 1  list_cores= (1)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 2  list_cores= (2)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 3  list_cores= (3)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 4  list_cores= (4)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 5  list_cores= (5)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 6  list_cores= (6)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 7  list_cores= (7)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 8  list_cores= (8)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 9  list_cores= (9)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 10  list_cores= (10)  num_devices= 6  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 11  list_cores= (11)  num_devices= 6  gpu_id= 0\n</code></pre> <p>This simple application does not handle binding of MPI ranks to GPUs, so each of the 12 MPI ranks detects all six GPUs on the node and by default all will select the first GPU listed. The binding of MPI ranks to GPUs can be handled <code>mpiexec</code> in the near future, but for the time being a simple helper script is available for those that need it. There is a centrally installed general <code>gpu_tile_compact.sh</code> script available for use, but the examples include the following example script for convenience in case one would like to explore different cpu-gpu bindings (e.g. bind first N MPI ranks to first GPU).</p> <p><pre><code>$ cat set_affinity_gpu_sunspot.sh \n#!/usr/bin/env bash\n\nnum_gpu=6\nnum_tile=2\n\ngpu_id=$(( (PALS_LOCAL_RANKID / num_tile ) % num_gpu ))\ntile_id=$((PALS_LOCAL_RANKID % num_tile))\n\nunset EnableWalkerPartition\nexport ZE_ENABLE_PCI_ID_DEVICE_ORDER=1\nexport ZE_AFFINITY_MASK=$gpu_id.$tile_id\n\necho \u201cRANK= ${PALS_RANKID} LOCAL_RANK= ${PALS_LOCAL_RANKID} gpu= ${gpu_id}  tile= ${tile_id}\u201d\n\n#https://stackoverflow.com/a/28099707/7674852\n\"$@\"\n</code></pre> The <code>ZE_AFFINITY_MASK</code> environment variable sets the devices that will be available to the CPU process and can be a comma-separated list of GPUs and/or GPU tiles. Each Aurora GPU consists of two tiles that can be separately bound to CPU processes. This simple script will set <code>ZE_AFFINITY_MASK</code> for each MPI rank such that GPU tiles on a node are round-robin assigned. </p> <pre><code>$ mpiexec -n 12 --ppn 12 --depth=1 --cpu-bind depth ./set_affinity_gpu_sunspot.sh ./hello_affinity\nNUM_OF_NODES= 1 TOTAL_NUM_RANKS= 12 RANKS_PER_NODE= 12 THREADS_PER_RANK= 1\n\n  Using OPENMP v5.0\n  num_devices=     1\n  Default device=  0\n  Host=            1\n  num_teams=       448\n  num_threads=     1\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 0  list_cores= (0)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 1  list_cores= (1)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 2  list_cores= (2)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 3  list_cores= (3)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 4  list_cores= (4)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 5  list_cores= (5)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 6  list_cores= (6)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 7  list_cores= (7)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 8  list_cores= (8)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 9  list_cores= (9)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 10  list_cores= (10)  num_devices= 1  gpu_id= 0\n\nTo affinity and beyond!! nname= x1922c1s1b0n0  rnk= 11  list_cores= (11)  num_devices= 1  gpu_id= 0\n</code></pre>"},{"location":"aurora/compiling-and-linking/aurora-example-program-makefile/#gpu-sycl-example","title":"GPU SYCL Example","text":"<p>A simple SYCL offload example is available here. Compilation proceeds similar to the above examples except for the compiler flags enabling GPU offload.</p> <pre><code>mpicxx -std=c++17 -fsycl -fsycl-targets=spir64 main.cpp\n</code></pre> <p>Note, this particular example makes use of the Level-Zero API and requires linking with <code>-lze_loader</code>, which is not something required of a typical SYCL application. Running the SYCL example using the affinity script binding MPI ranks to individual GPU tiles results in output like the following.</p> <pre><code>$ make\n\n$ mpiexec -n 12 --ppn 12 --depth=1 --cpu-bind depth ./set_affinity_gpu_sunspot.sh ./hello_affinity\n\nNUM_OF_NODES= 1 TOTAL_NUM_RANKS= 12 RANKS_PER_NODE= 12 THREADS_PER_RANK= 1\nCOMMAND= mpiexec -n 12 --ppn 12 --depth=1 --cpu-bind depth ./set_affinity_gpu_sunspot.sh ./hello_affinity\n\n\u201cRANK= 0 LOCAL_RANK= 0 gpu= 0 tile= 0\u201d\n\u201cRANK= 1 LOCAL_RANK= 1 gpu= 0 tile= 1\u201d\n\u201cRANK= 2 LOCAL_RANK= 2 gpu= 1 tile= 0\u201d\n\u201cRANK= 3 LOCAL_RANK= 3 gpu= 1 tile= 1\u201d\n\u201cRANK= 4 LOCAL_RANK= 4 gpu= 2 tile= 0\u201d\n\u201cRANK= 5 LOCAL_RANK= 5 gpu= 2 tile= 1\u201d\n\u201cRANK= 6 LOCAL_RANK= 6 gpu= 3 tile= 0\u201d\n\u201cRANK= 7 LOCAL_RANK= 7 gpu= 3 tile= 1\u201d\n\u201cRANK= 8 LOCAL_RANK= 8 gpu= 4 tile= 0\u201d\n\u201cRANK= 9 LOCAL_RANK= 9 gpu= 4 tile= 1\u201d\n\u201cRANK= 10 LOCAL_RANK= 10 gpu= 5 tile= 0\u201d\n\u201cRANK= 11 LOCAL_RANK= 11 gpu= 5 tile= 1\u201d\n\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 0  list_cores= (0)  num_devices= 1  gpu_uuid=  01000000-0000-0000-dbb1-2f985946b0dd\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 1  list_cores= (1)  num_devices= 1  gpu_uuid=  02000000-0000-0000-dbb1-2f985946b0dd\n\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 2  list_cores= (2)  num_devices= 1  gpu_uuid=  01000000-0000-0000-9d4c-a3a038130bd2\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 3  list_cores= (3)  num_devices= 1  gpu_uuid=  02000000-0000-0000-9d4c-a3a038130bd2\n\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 4  list_cores= (4)  num_devices= 1  gpu_uuid=  01000000-0000-0000-f684-455a4554b231\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 5  list_cores= (5)  num_devices= 1  gpu_uuid=  02000000-0000-0000-f684-455a4554b231\n\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 6  list_cores= (6)  num_devices= 1  gpu_uuid=  01000000-0000-0000-d04a-9a289a53274e\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 7  list_cores= (7)  num_devices= 1  gpu_uuid=  02000000-0000-0000-d04a-9a289a53274e\n\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 8  list_cores= (8)  num_devices= 1  gpu_uuid=  01000000-0000-0000-a178-e2f3a2a0df2b\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 9  list_cores= (9)  num_devices= 1  gpu_uuid=  02000000-0000-0000-a178-e2f3a2a0df2b\n\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 10  list_cores= (10)  num_devices= 1  gpu_uuid=  01000000-0000-0000-1b72-105049dfed26\nTo affinity and beyond!! nname= x1922c2s6b0n0  rnk= 11  list_cores= (11)  num_devices= 1  gpu_uuid=  02000000-0000-0000-1b72-105049dfed26\n</code></pre> <p>Upon carefully comparing the uuids from each rank, once can see the first field distinguishing the 1st or 2nd tile on a GPU and the last two fields distinguishing the 6 GPUs on a compute node. If the affinity script was not used for binding MPI ranks to GPUs, then each MPI rank would report uuids for all GPUs like in the following.</p> <pre><code>To affinity and beyond!! nname= x1922c2s6b0n0  rnk= 0  list_cores= (0)  num_devices= 6  gpu_uuid=  00000000-0000-0000-dbb1-2f985946b0dd 00000000-0000-0000-9d4c-a3a038130bd2 00000000-0000-0000-f684-455a4554b231 00000000-0000-0000-d04a-9a289a53274e 00000000-0000-0000-a178-e2f3a2a0df2b 00000000-0000-0000-1b72-105049dfed26\n</code></pre>"},{"location":"aurora/compiling-and-linking/aurora-example-program-makefile/#gpu-opencl-example","title":"GPU OpenCL Example","text":"<p>A simple OpenCL example is available here. The <code>include</code> and <code>lib</code> directories for the OpenCL headers and libraries are in the default environment. One simply needs to link the application against <code>-lOpenCL</code>.</p> <pre><code>mpicxx main.cpp -lOpenCL\n</code></pre> <p>This simple example can be run on a single tile of an Aurora GPU as follows.</p> <pre><code>$ export XE_AFFINITY_MASK=0.0\n$ ./vecadd\nRunning on GPU!\nUsing double-precision\n\n    CL_DEVICE_NAME: Intel(R) Data Center GPU Max 1550\n    CL_DEVICE_VERSION: OpenCL 3.0 NEO \n    CL_DEVICE_OPENCL_C_VERSION: OpenCL C 1.2 \n    CL_DEVICE_MAX_COMPUTE_UNITS: 896\n    CL_DEVICE_MAX_CLOCK_FREQUENCY: 1600\n    CL_DEVICE_MAX_WORK_GROUP_SIZE: 1024\n\nResult is CORRECT!! :)\n</code></pre>"},{"location":"aurora/compiling-and-linking/aurora-programming-models/","title":"Aurora Programming Models","text":"<p>The software environment on Aurora supports several parallel programming models targeting the CPUs and GPUs. </p>"},{"location":"aurora/compiling-and-linking/aurora-programming-models/#cpu-parallel-programming-models","title":"CPU Parallel Programming Models","text":"<p>The Aurora MPICH compiler wrappers <code>mpicc</code>, <code>mpicxx</code>, and <code>mpifort</code> are recommended for MPI applications to be built using the oneAPI compilers. A summary of available CPU parallel programming models and relevant compiler flags is show below. Users are encouraged to review the corresponding man pages and documentation.</p> Programming Model oneAPI OpenMP -fopenmp <p>Higher-level programming models such as Kokkos and Raja may also be used for CPU programming on Aurora.</p>"},{"location":"aurora/compiling-and-linking/aurora-programming-models/#gpu-programming-models","title":"GPU Programming Models","text":"<p>A summary of available GPU programming models and relevant compiler flags is shown below for compilers that generate offloadable code. Two modes of compilation are currently available with the oneAPI compilers: Just-in-Time (JIT) and Ahead-of-Time (AoT). With AOT compilation, flags for specifying the backend are only needed when linking the application. Users are encouraged to review the corresponding man pages and documentation.</p> Programming Model oneAPI (JIT) oneAPI (AoT) OpenCL -- N/A OpenMP -fiopenmp -fopenmp-targets=spir64 -fiopenmp -fopenmp-targets=spir64_gen -Xopenmp-target-backend=spir64_gen \"-device 12.60.7\" SYCL --intel -fsycl -fsycl-targets=spir64 --intel -fsycl -fsycl-targets=spir64_gen -Xsycl-target-backend \"-device 12.60.7\" <p>The oneAPI compiler flags For some build systems (e.g. <code>cmake</code>), it may be necessary to use the backslash character to escape the double quotes when specifying the device in AoT builds.</p> <pre><code>-Xopenmp-target-backend=spir64_gen \\\"-device 12.60.7\\\"\n</code></pre> <p>OpenCL is supported, but does not require specific compiler flags per-se as the offloaded kernels are JIT-compiled. One does need to link against the OpenCL library <code>-lOpenCL</code>. Abstraction programming models, such as Kokkos, can be built on top of these programming models.</p>"},{"location":"aurora/compiling-and-linking/cce-compilers-aurora/","title":"CCE Compilers on Polaris","text":""},{"location":"aurora/compiling-and-linking/compiling-and-linking-overview/","title":"Compiling and Linking Overview","text":""},{"location":"aurora/compiling-and-linking/compiling-and-linking-overview/#compiling-on-aurora-login-and-compute-nodes","title":"Compiling on Aurora Login and Compute Nodes","text":"<p>If your build system does not require GPUs for the build process, compilation of GPU-accelerated codes is generally expected to work well on the Aurora login nodes. If your build system does require GPUs, then currently that must be done on the compute nodes either via an interactive or batch job submission. Doing this interactively in a single-node job may be the preferred route as it also provides opportunity to quickly test the executable.</p>"},{"location":"aurora/compiling-and-linking/compiling-and-linking-overview/#filesystem","title":"Filesystem","text":"<p>It is helpful to realize that currently there is a single temporary filesystem <code>gecko</code> mounted on the Aurora login and compute nodes available to users, where both <code>home</code> and <code>project</code> spaces reside. It is important to realize that this filesystem is not backed up and users should take care to retain copies of important files (e.g. local resources or ALCF's <code>grand</code> and <code>eagle</code> filesystems).</p>"},{"location":"aurora/compiling-and-linking/compiling-and-linking-overview/#oneapi-programming-environment","title":"OneAPI Programming Environment","text":"<p>The oneAPI programming environment is currently the single environment for building and running software to maximally use the available hardware resources. The oneAPI environment is loaded by default for users and is principally defined by the following set of modules and related variants.</p> <ul> <li>oneapi</li> <li>intel_compute_runtime</li> <li>mpich</li> </ul> <p>Additional modules loading <code>GNU</code> CPU compilers and parallel application launch support (e.g. libfabric and cray-pals) are also provided in the default environment. The oneAPI environment provides C, C++, and Fortran compilers and associated MPICH MPI wrappers for building applications targeting CPUs and GPUs based on the OpenMP, SYCL, and OpenCL programming models. </p> <ul> <li><code>mpicc</code> - C Compiler</li> <li><code>mpicxx</code> - C++ Compiler (a.k.a <code>mpic++</code>)</li> <li><code>mpifort</code> - Fortran Compiler (a.k.a <code>mpif70</code> &amp; <code>mpif90</code>)</li> </ul>"},{"location":"aurora/compiling-and-linking/compiling-and-linking-overview/#compilers-provided-by-cray-programming-environments","title":"Compilers provided by Cray Programming Environments","text":"<p>The <code>PrgEnv-gnu</code> and <code>PrgEnv-cray</code> Cray programming environments are currently available as modules for users that neede them. The compilers provided by these environments do not currently support Intel GPUs and should only be used, if at all, for CPU-only code.</p>"},{"location":"aurora/compiling-and-linking/compiling-and-linking-overview/#mixed-cc-fortran-applications","title":"Mixed C/C++ &amp; Fortran Applications","text":"<p>For applications consisting of a mix of programming languages that use MPI, it is important to use the same Fortran compiler for building the application as was used to build MPI because of mpi.mod (and similar) incompatibilities. </p>"},{"location":"aurora/compiling-and-linking/compiling-and-linking-overview/#additional-software-and-build-tools","title":"Additional software and build tools","text":"<p>A suite of build tools and libraries are available in the default Aurora PE environment. Users can look at the list of available modules with <code>module avail</code> to find build tools such as <code>cmake</code>.</p> <pre><code>$ module load cmake\n$ cmake --version\ncmake version 3.27.7\n</code></pre>"},{"location":"aurora/compiling-and-linking/continuous-integration-aurora/","title":"Continuous Integration Aurora","text":""},{"location":"aurora/compiling-and-linking/gnu-compilers-aurora/","title":"GNU Compilers Polaris","text":""},{"location":"aurora/compiling-and-linking/llvm-compilers-aurora/","title":"LLVM Compilers","text":"<p>LLVM Compilers Aurora</p>"},{"location":"aurora/data-management/daos/daos-overview/","title":"DAOS Architecture","text":"<p>DAOS is a high performance storage system for storing checkpoints and analysis files. DAOS is fully integrated with the wider Aurora compute fabric as can be seen in the overall storage architecture below.</p> <p></p>"},{"location":"aurora/data-management/daos/daos-overview/#daos-overview","title":"DAOS Overview","text":"<p>Users should submit a request as noted below to have a DAOS pool created for their project. Once created, users may create and manage containers within the pool as they wish.</p>"},{"location":"aurora/data-management/daos/daos-overview/#note","title":"Note","text":"<p>This is an initial test DAOS configuration and as such, any data on the DAOS system will envtually be deleted when the configuration is changed into a larger system. Warning will be given before the system is wiped to allow time for users to move any important data off.</p>"},{"location":"aurora/data-management/daos/daos-overview/#pool-allocation","title":"Pool Allocation","text":"<p>Email support@alcf.anl.gov to request a pool and provide which primary project you're on. The pool will be set to allow anyone in the project unix group to access the pool. Please request the capacity of allocation you would like.</p>"},{"location":"aurora/data-management/daos/daos-overview/#modules","title":"Modules","text":"<p>Please load the <code>daos/base</code> module when using DAOS. This should be done when logging into the UAN or when using DAOS from a compute job script:</p> <pre><code>module load daos/base\n</code></pre>"},{"location":"aurora/data-management/daos/daos-overview/#pool","title":"Pool","text":"<p>Confirm you are able to query the pool via: <pre><code>daos pool query &lt;pool name&gt;\n</code></pre></p> <p>Example output: <pre><code>daos pool query software\nPool 050b20a3-3fcc-499b-a6cf-07d4b80b04fd, ntarget=640, disabled=0, leader=2, version=131\nPool space info:\n- Target(VOS) count:640\n- Storage tier 0 (SCM):\nTotal size: 6.0 TB\n  Free: 4.4 TB, min:6.5 GB, max:7.0 GB, mean:6.9 GB\n- Storage tier 1 (NVMe):\n  Total size: 200 TB\n  Free: 194 TB, min:244 GB, max:308 GB, mean:303 GB\nRebuild done, 4 objs, 0 recs\n</code></pre></p>"},{"location":"aurora/data-management/daos/daos-overview/#container","title":"Container","text":"<p>The container is the basic unit of storage. A POSIX container can contain hundreds of millions of files, you can use it to store all of your data. You only need a small set of containers; perhaps just one per major unit of project work is sufficient.</p>"},{"location":"aurora/data-management/daos/daos-overview/#create-a-container","title":"Create a container","text":"<p>ALCF has provided a script, <code>mkcont</code>, to help create a container with reasonable defaults. <pre><code>mkcont --type POSIX --pool &lt;pool name&gt; --user $USER --group &lt;group&gt; &lt;container name&gt;\n</code></pre></p> <p>Example output: <pre><code>&gt; mkcont --type=POSIX --pool iotest --user harms --group users random\n&gt;   Container UUID : 9a6989d3-3835-4521-b9c6-ba1b10f3ec9c\n&gt;   Container Label: random\n&gt;   Container Type : POSIX\n&gt;\n&gt; Successfully created container 9a6989d3-3835-4521-b9c6-ba1b10f3ec9c\n&gt; 0\n</code></pre> Alternatively, the <code>daos</code> command can be used to create a container directly.</p>"},{"location":"aurora/data-management/daos/daos-overview/#mount-a-container","title":"Mount a container","text":"<p>Currently, you must manually mount your container prior to use on any node you are working on. In the future, we hope to automate some of this via additional <code>qsub</code> options.</p>"},{"location":"aurora/data-management/daos/daos-overview/#uan","title":"UAN","text":"<p>Create a directory to mount the POSIX container on and then mount the container via <code>dfuse</code>. <pre><code>dfuse --pool=&lt;pool name&gt; --cont=&lt;cont name&gt; -m $HOME/daos/&lt;pool&gt;/&lt;cont&gt;\n</code></pre></p> <p>mkdir -p $HOME/daos/iotest/random dfuse --pool=iotest --cont=random -m $HOME/daos/iotest/random mount | grep iotest dfuse on /home/harms/daos/iotest/random type fuse.daos (rw,nosuid,nodev,noatime,user_id=4211,group_id=100,default_permissions)</p>"},{"location":"aurora/data-management/daos/daos-overview/#compute-node","title":"Compute Node","text":"<p>From a compute node, you need to mount the container on all compute nodes. We provide some scripts to help perform this from within your job script. More examples are available in <code>/soft/daos/examples</code>. The following example uses two support scripts, <code>launch-dfuse.sh</code> and <code>clean-dfuse.sh</code>, to startup dfuse on each compute node and then shut it down at job end, respectively.</p> <p>Job Script Example: <pre><code>#!/bin/bash\n#PBS -A &lt;project&gt;\n#PBS -lselect=1\n#PBS -lwalltime=30:00\n#PBS -k doe\n#\n# Test case for MPI-IO code example\n\n# ranks per node\nrpn=4\n\n# threads per rank\nthreads=1\n\n# nodes per job\nnnodes=$(cat $PBS_NODEFILE | wc -l)\n\n# Verify the pool and container are set\nif [ -z \"$DAOS_POOL\" ];\nthen\n    echo \"You must set DAOS_POOL\"\n    exit 1\nfi\n\nif [ -z \"$DAOS_CONT\" ];\nthen\n    echo \"You must set DAOS_CONT\"\n    exit 1\nfi\n\n# load daos/base module (if not loaded)\nmodule load daos/base\n\n# print your module list (useful for debugging)\nmodule list\n\n# print your environment (useful for debugging)\n#env\n\n# turn on output of what is executed\nset -x\n\n#\n# clean previous mounts (just in case)\n#\nclean-dfuse.sh ${DAOS_POOL}:${DAOS_CONT}\n\n# launch dfuse on all compute nodes\n# will be launched using pdsh\n# arguments:\n#   pool:container\n# may list multiple pool:container arguments\n# will be mounted at:\n#   /tmp/\\&lt;pool\\&gt;/\\&lt;container\\&gt;\nlaunch-dfuse.sh ${DAOS_POOL}:${DAOS_CONT}\n\n# change to submission directory\ncd $PBS_O_WORKDIR\n\n# run your job(s)\n# these test cases assume 'testfile' is in the CWD\ncd /tmp/${DAOS_POOL}/${DAOS_CONT}\n\necho \"write\"\n\nmpiexec -np $((rpn*nnodes)) \\\n-ppn $rpn \\\n-d $threads \\\n--cpu-bind numa \\\n--no-vni \\ # enables DAOS access\n-genvall \\\n/soft/daos/examples/src/posix-write\n\necho \"read\"\nmpiexec -np $((rpn*nnodes)) \\\n-ppn $rpn \\\n-d $threads \\\n--cpu-bind numa \\\n--no-vni \\ # enables DAOS access\n-genvall \\\n/soft/daos/examples/src/posix-read\n\n# cleanup dfuse mounts\nclean-dfuse.sh ${DAOS_POOL}:${DAOS_CONT}\n\nexit 0\n</code></pre></p>"},{"location":"aurora/data-management/daos/daos-overview/#job-submission","title":"Job Submission","text":"<p>The above job script expects two environment variables which you set to the relevant pool and container. The <code>-ldaos=default</code> switch will ensure that DAOS is available on the compute node.</p> <pre><code>qsub -v DAOS_POOL=&lt;name&gt;,DAOS_CONT=&lt;name&gt; -ldaos=default ./job-script.sh\n</code></pre>"},{"location":"aurora/data-management/daos/daos-overview/#onescratch","title":"oneScratch","text":"<p>The current DAOS system is configured with 20 server nodes. The remaining balance of server nodes is still reserved for internal testing.</p>"},{"location":"aurora/data-management/daos/daos-overview/#hardware","title":"Hardware","text":"<p>Each DAOS server nodes is based on the Intel Coyote Pass platform. * (2) Xeon 5320 CPU (Ice Lake) * (16) 32GB DDR4 DIMMs * (16) 512GB Intel Optane Persistent Memory 200 * (16) 15.3TB Samsung PM1733 NVMe * (2) HPE Slingshot NIC</p> <p></p>"},{"location":"aurora/data-management/daos/daos-overview/#performance","title":"Performance","text":"<p>The peak performance of the oneScratch storage is approximately 800 GB/s to 1000 GB/s. Obtaining this performance should be possible from a job running in the available user partition but there are many considerations to understand achieving good performance.</p>"},{"location":"aurora/data-management/daos/daos-overview/#single-node","title":"Single Node","text":"<p>First is to consider the throughput you can obtain from a single compute node in the test case. * dfuse is a single process and will therefore attach to a single NIC, limiting throughput to ~20 GB/s per compute node. * dfuse offers caching and can thus show performance greater than theoretical due to cache effects based on the workload running. * MPI-IO, Intercept Library, or other interfaces that use libdfs will bond to a NIC per-process.   * DAOS will bond to NICs in a round-robin fashion to NICs which are located on the same socket.   * For Aurora, DAOS processes running on socket 0 will only use 4 NICs assuming at least four processes are used but will not use more until the second socket is used.   * IF running with a lower process count such as 24, the processes should be distributed between socket 0 and socket 1 for best I/O performance.</p>"},{"location":"aurora/data-management/daos/daos-overview/#dragonfly-groups","title":"Dragonfly Groups","text":"<p>The next element is to consider how many dragonfly groups the job is running within. Each dragonfly groups has 2 links to each I/O group and the current DAOS servers are distributed amoung the full 8 I/O groups. * If a single compute group is used, that limits performance to 8 groups 2 links/group * 25 GB/s/link = 400 GB/s * Thus it requires at least 2 compute groups to reach max performance. * However, Slingshot support dynamic routing allowing traffic to use non-minimal routes via other compute groups which will result in performance greater that the theoretical peak of the number of compute groups being used.   * Dynamic routing performance will be sensitive to other workloads running on the system and not be consistent.</p> <p></p>"},{"location":"aurora/data-management/daos/daos-overview/#object-class","title":"Object Class","text":"<p>The object class selected for your container will influence the performance potential of I/O. An object class which is [SG]X is distributed across all of the targets in the system. * All pools in the test system are enabled to use 100% of the targets. * SX/GX will provide best performance for large data which distributes on all server targets but will lower IOp performance for metadata as each target must be communicated with. * S1/G1 will provide good performance for small data with need for high IOps as it places data only on 1 target."},{"location":"aurora/data-management/daos/daos-overview/#porting-io-to-daos","title":"Porting I/O to DAOS","text":"<p>There is no need to specifically modify code to use DAOS, however, DAOS can be used in several different modes so some consideration should be given on where to start. The diagram below provides a suggested path to testing an application beginning at the green downward arrow.</p> <p></p>"},{"location":"aurora/data-management/daos/daos-overview/#dfuse","title":"dFuse","text":"<p>The first suggested step to test with dFuse. dFuse utilizes the linux FUSE layer to provide a POSIX compatible API layer for any application that uses POSIX directly or indirectly though an I/O library. The dFuse component provides a simple method for using DAOS with no modifications. Once the DAOS container is mounted via dFuse, applications and utilities can access data as before. dFuse will scale as more compute nodes are added, but is not efficient on a per-node basis so will not provide ideal results at small scale. dFuse doesn't provide ideal metadata performance either but it does have the advatange of utilizing the Linux page cache, so workloads that benefit from caching may see better performance than other methods.</p>"},{"location":"aurora/data-management/daos/daos-overview/#interception-library","title":"Interception Library","text":"<p>The interception library (IL) is a next step in improving DAOS performance. The IL will intercept basic read and write POSIX calls while all metadata calls still go through dFuse. The IL can provide a large performance improvement for bulk I/O as it bypasses the kernel and communicates with DAOS directly in userspace. It will also take advantage of the multiple NICs on the node based on who many MPI processes are running on the node and which CPU socket they are on.</p>"},{"location":"aurora/data-management/daos/daos-overview/#mpi-io","title":"MPI-IO","text":"<p>The ROMIO MPI-IO layer provides multiple I/O backends including a custom DAOS backend. MPI-IO can be used with dFuse and the interception library when using the <code>ufs</code> backend but the <code>daos</code> backend will provide optimal performance. In order to use this, one can prefix the file names with <code>daos:</code> which will tell MPI-IO to use the DAOS backend.</p>"},{"location":"aurora/data-management/daos/daos-overview/#hdf-daos-vol","title":"HDF DAOS VOL","text":"<p>The HDF5 library can be used with POSIX or MPI-IO layers utilizing dFuse, IL or MPI-IO with DAOS. The first suggestion would be to start with dFuse and then move to MPI-IO. Once the performance of these methods has been evaluated, using the custom DAOS VOL can be attempted. The DAOS VOL will provide a performance improvement under certain types of HDF workloads. Using the VOL has other complications/benefits which should be considered as well. The VOL maps a single HDF file into a single container. This means a workload that tries to use multiple HDF files per checkpoint, will create one DAOS container for each one. This is not ideal and will likely lead to performance issues. The HDF code should be such that a single HDF file is used per checkpoint/analysis file/etc. An entire campaign might generate thousands of containers which might be some overhead on an individual to manage so many containers. As such, it might be beneficial to convert the code to write each checkpoint/time step into a HDF Group and then a single HDF file can be used for the entire campaign. This solution is more DAOS specific, as it will be functionally compatible on any system, however a traditinoal PFS may lose the entire contents of the file if a failure occurs during write while DAOS will be resilent to those failures and rollback to a previous good version.</p>"},{"location":"aurora/data-management/daos/daos-overview/#dfs","title":"DFS","text":"<p>DFS is the user level API for DAOS. This API is very similar to POSIX but still has many differences that would require code changes to utilize DFS directly. The DFS API can provide the best overall performance for any scenario other than workloads which benefit from caching.</p>"},{"location":"aurora/data-management/lustre/gecko/","title":"Gecko Filesystem","text":""},{"location":"aurora/data-management/lustre/gecko/#data-transfer","title":"Data Transfer","text":"<p>Currently, scp and SFTP are the only ways to transfer data to/from Aurora. </p>"},{"location":"aurora/data-management/lustre/gecko/#transferring-files-from-non-alcf-systems","title":"Transferring files from non-ALCF systems","text":"<p>As an expedient for initiating ssh sessions to Aurora login nodes via the bastion indirect nodes, and to enable scp from remote (non ALCF) hosts to Aurora login nodes, follow these steps:</p> <ol> <li>Create SSH keys on the laptop/desktop/remote machine. See \"Creating SSH Keys\" section on this page:</li> <li>Add the lines listed below to your ~/.ssh/config file on the remote host. That is, you should do this on your laptop/desktop, from which you are initiating ssh login sessions to Aurora via bastion, and on other non-ALCF host systems from which you want to copy files to Aurora login nodes using scp.</li> </ol> <pre><code>$ cat ~/.ssh/config\n\nHost *.aurora.alcf.anl.gov aurora.alcf.anl.gov\n    ProxyCommand ssh &lt;your_ALCF_username&gt;@bastion.alcf.anl.gov -q -W %h:%p\n</code></pre> <ol> <li>Copy the public key (*.pub) from ~/.ssh folder on the remote machine to ~/.ssh/authorized_keys file on Aurora (login.aurora.alcf.anl.gov)</li> </ol> <p>When you use an SSH proxy, it takes the authentication mechanism from the local host and applies it to the farthest-remote host, while prompting you for the \u201cmiddle host\u201d separately. So, when you run the ssh @login.aurora.alcf.anl.gov  command on your laptop/desktop, you'll be prompted for two ALCF authentication codes - first the Mobilepass+ or Cryptocard passcode for the bastion, and then the SSH passphrase for Aurora. Likewise, when you run scp from a remote host to copy files to Aurora login nodes, you'll be prompted for two ALCF authentication codes codes - first the Mobilepass+ or Cryptocard passcode and then the SSH passphrase."},{"location":"aurora/data-management/lustre/gecko/#transferring-files-from-other-alcf-systems","title":"Transferring files from other ALCF systems","text":"<p>With the bastion pass-through nodes currently used to access both Sunspot and Aurora, users will find it helpful to modify their .ssh/config files on Aurora appropriately to facilitate transfers to Aurora from other ALCF systems. These changes are similar to what Sunspot users may have already implemented. From an Aurora login-node, this readily enables one to transfer files from Sunspot's gila filesystem or one of the production filesystems at ALCF (home, grand, and eagle) mounted on an ALCF system's login node. With the use of ProxyJump below, entering the MobilePass+ or Cryptocard passcode twice will be needed (once for bastion and once for the other resource).  A simple example shows the .ssh/config entries for Polaris and the scp command for transferring from Polaris:</p> <pre><code>$ cat .ssh/config\nknight@aurora-uan-0009:~&gt; cat .ssh/config\nHost bastion.alcf.anl.gov\n    User knight\n\nHost polaris.alcf.anl.gov\n    ProxyJump bastion.alcf.anl.gov\n    DynamicForward 3142\n    user knight\n</code></pre> <pre><code>knight@aurora-uan-0009:~&gt; scp knight@polaris.alcf.anl.gov:/grand/catalyst/proj-shared/knight/test.txt ./\n---------------------------------------------------------------------------\n                            Notice to Users\n...\n[Password:\n---------------------------------------------------------------------------\n                            Notice to Users\n... \n[Password:\nknight@aurora-uan-0009:~&gt; cat test.txt \nfrom_polaris grand\n</code></pre>"},{"location":"aurora/data-science/julia/","title":"Julia on Aurora","text":""},{"location":"aurora/data-science/python/","title":"Python on Aurora","text":""},{"location":"aurora/data-science/python/#framework-modules","title":"Framework Modules","text":"<p>Frameworks on Aurora can be loaded into a users environment by loading the <code>frameworks</code> module as follows. The conda environment loaded with this module makes available TensorFlow, Horovod, and Pytorch with Intel extensions and optimizations. The following commands can be used both from an interactive session on a terminal and on a batch job script.</p> <p>Note that the framework modules may load a different oneAPI than the default module.  The frameworks are updated on approximately a quarterly cadence at the moment.</p> <p><pre><code>module use /soft/modulefiles\nmodule load frameworks\n</code></pre> These pre-built <code>conda</code> environments come with GPU-supported builds of PyTorch and TensorFlow. Both of these frameworks have <code>Horovod</code> support for multi-node calculations. Many other commonly used Python modules are available through these modules.</p> <p>For more information on pytorch and tensorflow please see their respective  pages: </p> <ul> <li>PyTorch</li> <li>TensorFlow</li> </ul> <p>From a login node we can do the following commands to list the available  modules:</p> <p><pre><code>module load /soft/modulefiles/\nmodule avail\n</code></pre> This shows a list of avilable modules including the frameworks module. There  are many frameworks modules available. The latest  frameworks release could be used using:</p> <p><pre><code>$ module load frameworks/2023.12.15.001\n\nThe following have been reloaded with a version change:\n  1) gcc/11.2.0 =&gt; gcc/12.2.0     2) intel_compute_runtime/release/agama-devel-551 =&gt; intel_compute_runtime/release/stable-736.25\n\n$ which python3\n/soft/datascience/aurora_nre_models_frameworks-2024.0/bin/python3\n\n$ which python\n/soft/datascience/aurora_nre_models_frameworks-2024.0/bin/python\n</code></pre> At the time of writing this module contains Python 3.9.18. Future modules will contain updated versions of Python, PyTorch, TensorFlow, etc.</p> <p>While the shared Anaconda environment encapsulated in the module contains many  of the most commonly used Python libraries for our users, you may still  encounter a scenario in which you need to extend the functionality of the  environment (i.e. install additional packages)</p> <p>You can use a virtual environment to extend/modify an existing frameworks  module. </p>"},{"location":"aurora/data-science/python/#virtual-environments-via-venv","title":"Virtual environments via <code>venv</code>","text":"<p>Creating your own (empty) virtual Python environment in a directory that is  writable to you is straightforward:</p> <pre><code>python3 -m venv /path/to/new/virtual/environment\n</code></pre> <p>This creates a new folder that is fairly lightweight folder (&lt;20 MB) with its  own Python interpreter where you can install whatever packages you'd like.  First, you must activate the virtual environment to make this Python  interpreter the default interpreter in your shell session.  By default, this environment will not have access to the framework packages but instead will be empty.</p> <p>You activate the new environment whenever you want to start using it via  running the activate script in that folder:</p> <pre><code>source /path/to/new/virtual/environment/bin/activate\n</code></pre> <p>In many cases, you do not want an empty virtual environment, but instead want  to start from the <code>conda</code> base environment's installed packages, only adding  and/or changing a few modules.</p> <p>To extend the base Anaconda environment with <code>venv</code> (e.g. <code>my_env</code> in the current  directory) and inherit the base enviroment packages, one can use the  <code>--system-site-packages</code> flag:</p> <p><pre><code>module use /soft/modulefiles/\nmodule load frameworks/2023.12.15.001\npython3 -m venv --system-site-packages my_env\nsource my_env/bin/activate\n\n# Install additional packages here\n</code></pre> You can always retroactively change the <code>--system-site-packages</code> flag state for  this virtual environment by editing <code>my_env/pyvenv.cfg</code> and changing the value  of the line <code>include-system-site-packages = false</code>.</p> <p>To install a different version of a package that is already installed in the  base environment, you can use:</p> <p><pre><code>pip install --ignore-installed ... # or -I\n</code></pre> The shared base environment is not writable, so it is impossible to remove or  uninstall packages from it. The packages installed with the above <code>pip</code> command  should shadow those installed in the base environment.</p>"},{"location":"aurora/data-science/python/#using-pip-install-user-not-recommended","title":"Using <code>pip install --user</code> (not recommended)","text":"<p>With the conda environment setup, one can install common Python modules using  <code>pip install --users &lt;module-name&gt;</code> which will install packages in  <code>$PYTHONUSERBASE/lib/pythonX.Y/site-packages</code>. The <code>$PYTHONUSERBASE</code>  environment variable is automatically set when you load the base <code>conda</code>  module, and is equal to <code>/home/$USER/.local/aurora/frameworks/2023.12.15.001</code></p> <p>Note, Python modules installed this way that contain command line binaries will  not have those binaries automatically added to the shell's <code>$PATH</code>. To manually  add the path:</p> <p><pre><code>export PATH=$PYTHONUSERBASE/bin:$PATH\n</code></pre> Be sure to remove this location from <code>$PATH</code> if you deactivate the base  Anaconda environment or unload the module.</p> <p>Cloning the Anaconda environment, or using <code>venv</code> are both more flexible and  transparent when compared to <code>--user</code> installs.</p>"},{"location":"aurora/data-science/applications/gpt-neox/","title":"gpt-neox","text":"<p>Instruction for gpt-neox on Aurora</p>"},{"location":"aurora/data-science/containers/containers/","title":"Containers on Aurora","text":""},{"location":"aurora/data-science/frameworks/deepspeed/","title":"DeepSpeed on Aurora","text":""},{"location":"aurora/data-science/frameworks/jax/","title":"Jax on Aurora","text":""},{"location":"aurora/data-science/frameworks/libtorch/","title":"LibTorch C++ Library","text":"<p>LibTorch is a C++ library for Torch, with many of the API that are available in PyTorch. Users can find more information on the PyTorch documentation. This is useful to integrate the Torch ML framework into traditional HPC simulation codes and therefore enable training and inferecing of ML models. On Aurora, the Intel Extension for PyTorch (IPEX) library is needed to access the Max 1550 GPU, which have the device name <code>kXPU</code> in LibTorch.  During compilation, Intel optimizations will be activated automatically once the IPEX dynamic library is linked.</p>"},{"location":"aurora/data-science/frameworks/libtorch/#environment-setup","title":"Environment Setup","text":"<p>To use LibTorch on Aurora, load the ML frameworks module <pre><code>module use /soft/modulefiles\nmodule load frameworks/2023.12.15.001\n</code></pre> which will also load the consistent oneAPI SDK and <code>cmake</code>.</p>"},{"location":"aurora/data-science/frameworks/libtorch/#torch-and-ipex-libraries","title":"Torch and IPEX libraries","text":"<p>With the ML frameworks module loaded as shown above, run <pre><code>python -c 'import torch; print(torch.__path__[0])'\npython -c 'import torch;print(torch.utils.cmake_prefix_path)'\n</code></pre> to find the path to the Torch libraries, include files, and CMake files.</p> <p>For the path to the IPEX dynamic library, run <pre><code>python -c 'import torch; print(torch.__path__[0].replace(\"torch\",\"intel_extension_for_pytorch\"))'\n</code></pre></p>"},{"location":"aurora/data-science/frameworks/libtorch/#model-inferencing-using-the-torch-api","title":"Model Inferencing Using the Torch API","text":"<p>This example shows how to perform inference on the ResNet50 model using only the LibTorch API. First, get a jit-traced version of the model running <code>resnet50_trace.py</code> below. <pre><code>import torch\nimport torchvision\nimport intel_extension_for_pytorch as ipex\nfrom time import perf_counter\n\ndevice = 'xpu'\n\nmodel = torchvision.models.resnet50()\nmodel.to(device)\nmodel.eval()\n\ndummy_input = torch.rand(1, 3, 224, 224).to(device)\n\nmodel_jit = torch.jit.trace(model, dummy_input)\ntic = perf_counter()\npredictions = model_jit(dummy_input)\ntoc = perf_counter()\nprint(f\"Inference time: {toc-tic}\")\n\ntorch.jit.save(model_jit, f\"resnet50_jit.pt\")\n</code></pre></p> <p>Then, use the source code in <code>inference-example.cpp</code> <pre><code>#include &lt;torch/torch.h&gt;\n#include &lt;torch/script.h&gt;\n#include &lt;iostream&gt;\n\nint main(int argc, const char* argv[]) {\n    torch::jit::script::Module model;\n    try {\n        model = torch::jit::load(argv[1]);\n        std::cout &lt;&lt; \"Loaded the model\\n\";\n    }\n    catch (const c10::Error&amp; e) {\n        std::cerr &lt;&lt; \"error loading the model\\n\";\n        return -1;\n    }\n    // Upload model to GPU\n    model.to(torch::Device(torch::kXPU));\n    std::cout &lt;&lt; \"Model offloaded to GPU\\n\\n\";\n\n    auto options = torch::TensorOptions()\n                      .dtype(torch::kFloat32)\n                      .device(torch::kXPU);\n    torch::Tensor input_tensor = torch::rand({1,3,224,224}, options);\n    assert(input_tensor.dtype() == torch::kFloat32);\n    assert(input_tensor.device().type() == torch::kXPU);\n    std::cout &lt;&lt; \"Created the input tesor on GPU\\n\";\n\n    torch::Tensor output = model.forward({input_tensor}).toTensor();\n    std::cout &lt;&lt; \"Performed inference\\n\\n\";\n\n    std::cout &lt;&lt; \"Predicted tensor is : \\n\";\n    std::cout &lt;&lt; output.slice(/*dim=*/1, /*start=*/0, /*end=*/10) &lt;&lt; '\\n';\n\n    return 0;\n}\n</code></pre></p> <p>and the <code>CMakeLists.txt</code> file</p> <pre><code>cmake_minimum_required(VERSION 3.5 FATAL_ERROR)\ncmake_policy(SET CMP0074 NEW)\nproject(inference-example)\n\nfind_package(Torch REQUIRED)\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS} -Wl,--no-as-needed\")\n\nadd_executable(inference-example inference-example.cpp)\ntarget_link_libraries(inference-example \"${TORCH_LIBRARIES}\" \"${INTEL_EXTENSION_FOR_PYTORCH_PATH}/lib/libintel-ext-pt-gpu.so\")\n\nset_property(TARGET inference-example PROPERTY CXX_STANDARD 17)\n</code></pre> <p>to build the inference example.</p> <p>Finally, execute the <code>doConfig.sh</code> script below <pre><code>#!/bin/bash\n\ncmake \\\n    -DCMAKE_PREFIX_PATH=`python -c 'import torch;print(torch.utils.cmake_prefix_path)'` \\\n    -DINTEL_EXTENSION_FOR_PYTORCH_PATH=`python -c 'import torch; print(torch.__path__[0].replace(\"torch\",\"intel_extension_for_pytorch\"))'` \\\n    ./\n\nmake\n./inference-example ./resnet50_jit.pt\n</code></pre></p>"},{"location":"aurora/data-science/frameworks/libtorch/#libtorch-interoperability-with-sycl-pipelines","title":"LibTorch Interoperability with SYCL Pipelines","text":"<p>The LibTorch API can be integrated with data pilelines using SYCL to offload and operate on the input and output data on the Intel Max 1550 GPU.  The code below extends the above example of performing inference with the ResNet50 model by first generating the input data on the CPU, then offloading it to the GPU with SYCL, and finally passing the device pointer to LibTorch for inference.</p> <p>The source code for <code>inference-example.cpp</code> is modified as follows <pre><code>#include &lt;torch/torch.h&gt;\n#include &lt;torch/script.h&gt;\n#include &lt;iostream&gt;\n#include \"sycl/sycl.hpp\"\n#include &lt;vector&gt;\n\nconst int N_BATCH = 1;\nconst int N_CHANNELS = 3;\nconst int N_PIXELS = 224;\nconst int INPUTS_SIZE = N_BATCH*N_CHANNELS*N_PIXELS*N_PIXELS;\n\nint main(int argc, const char* argv[]) {\n    torch::jit::script::Module model;\n    try {\n        model = torch::jit::load(argv[1]);\n        std::cout &lt;&lt; \"Loaded the model\\n\";\n    }\n    catch (const c10::Error&amp; e) {\n        std::cerr &lt;&lt; \"error loading the model\\n\";\n        return -1;\n    }\n    // Upload model to GPU\n    model.to(torch::Device(torch::kXPU));\n    std::cout &lt;&lt; \"Model offloaded to GPU\\n\\n\";\n\n    // Create the input data on the host\n    std::vector&lt;float&gt; inputs(INPUTS_SIZE);\n    srand(12345);\n    for (int i=0; i&lt;INPUTS_SIZE; i++) {\n      inputs[i] = static_cast &lt;float&gt; (rand()) / static_cast &lt;float&gt; (RAND_MAX);\n    }\n    std::cout &lt;&lt; \"Generated input data on the host \\n\\n\";\n\n    // Move input data to the device with SYCL\n    sycl::queue Q(sycl::gpu_selector_v);\n    std::cout &lt;&lt; \"SYCL running on \"\n            &lt;&lt; Q.get_device().get_info&lt;sycl::info::device::name&gt;()\n            &lt;&lt; \"\\n\\n\";\n    float *d_inputs = sycl::malloc_device&lt;float&gt;(INPUTS_SIZE, Q);\n    Q.memcpy((void *) d_inputs, (void *) inputs.data(), INPUTS_SIZE*sizeof(float));\n    Q.wait();\n\n    // Convert input array to Torch tensor\n    auto options = torch::TensorOptions()\n                      .dtype(torch::kFloat32)\n                      .device(torch::kXPU);\n    torch::Tensor input_tensor = at::from_blob(d_inputs, {N_BATCH,N_CHANNELS,N_PIXELS,N_PIXELS},\n                                               nullptr, at::device(torch::kXPU).dtype(torch::kFloat32),\n                                               torch::kXPU)\n                                               .to(torch::kXPU);\n    assert(input_tensor.dtype() == torch::kFloat32);\n    assert(input_tensor.device().type() == torch::kXPU);\n    std::cout &lt;&lt; \"Created the input tesor on GPU\\n\";\n\n    // Perform inference\n    torch::Tensor output = model.forward({input_tensor}).toTensor();\n    std::cout &lt;&lt; \"Performed inference\\n\\n\";\n\n    std::cout &lt;&lt; \"Predicted tensor is : \\n\";\n    std::cout &lt;&lt; output.slice(/*dim=*/1, /*start=*/0, /*end=*/10) &lt;&lt; '\\n';\n\n    return 0;\n}\n</code></pre></p> <p>and the CMake commands also change to include <pre><code>#!/bin/bash\n\ncmake \\\n    -DCMAKE_CXX_FLAGS=\"-std=c++17 -fsycl\" \\\n    -DCMAKE_PREFIX_PATH=`python -c 'import torch;print(torch.utils.cmake_prefix_path)'` \\\n    -DINTEL_EXTENSION_FOR_PYTORCH_PATH=`python -c 'import torch; print(torch.__path__[0].replace(\"torch\",\"intel_extension_for_pytorch\"))'` \\\n    ./\n\nmake\n./inference-example ./resnet50_jit.pt\n</code></pre></p>"},{"location":"aurora/data-science/frameworks/libtorch/#known-issues","title":"Known Issues","text":"<ul> <li>The LibTorch introspection API that are available for CUDA devices, such as <code>torch::cuda::is_available()</code>, are still under development for Intel Max 1550 GPU.</li> </ul>"},{"location":"aurora/data-science/frameworks/pytorch/","title":"PyTorch on Aurora","text":"<p>PyTorch is a popular, open source deep learning framework developed and  released by Facebook. The PyTorch home page, has more information about PyTorch, which you can refer to. For troubleshooting on  Aurora, please contact support@alcf.anl.gov.</p>"},{"location":"aurora/data-science/frameworks/pytorch/#installation-on-aurora","title":"Installation on Aurora","text":"<p>PyTorch is already installed on Aurora with GPU support and available through the frameworks module. To use it from a compute node, please load the following modules:</p> <p><pre><code>module use /soft/modulefiles/\nmodule load frameworks/2023.12.15.001\n</code></pre> Then you can <code>import</code> PyTorch as usual, the following is an output from the <code>frameworks/2023.12.15.001</code> module</p> <p><pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.__version__\n'2.0.1a0+cxx11.abi'\n</code></pre> A simple but useful check could be to use PyTorch to get device information on a compute node. You can do this the following way:</p> <pre><code>import torch\nimport intel_extension_for_pytorch as ipex\n\nprint(f\"GPU availability: {torch.xpu.is_available()}\")\nprint(f'Number of tiles = {torch.xpu.device_count()}')\ncurrent_tile = torch.xpu.current_device()\nprint(f'Current tile = {current_tile}')\nprint(f'Curent device ID = {torch.xpu.device(current_tile)}')\nprint(f'Device name = {torch.xpu.get_device_name(current_tile)}')\n</code></pre> <p><pre><code># output of the above code block\n\nGPU availability: True\nNumber of tiles = 12\nCurrent tile = 0\nCurent device ID = &lt;intel_extension_for_pytorch.xpu.device object at 0x1540a9f25790&gt;\nDevice name = Intel(R) Data Center GPU Max 1550\n</code></pre> Note that, along with importing the <code>torch</code> module, you need to import the <code>intel_extension_for_pytorch</code> module. The default mode in <code>ipex</code> for counting the available devices on a compute node treat each tile as a device, hence the code block above is expected to output <code>12</code>. If you want to get the number of \"cards\" as an output, you may declare the following environment variable:</p> <p><pre><code>export IPEX_TILE_AS_DEVICE=0\n</code></pre> With this environmental variable, we expect the output to be <code>6</code> -- the number of GPUs available on an Aurora compute node. All the <code>API</code> calls involving  <code>torch.cuda</code>, should be replaced with <code>torch.xpu</code>, as shown in the above  example.</p> <p>Important: It is highly recommended to import <code>intel_extension_for_pytorch</code>  right after <code>import torch</code>, prior to importing other packages, (from  Intel's getting started doc).</p> <p>Intel extension for PyTorch has been made publicly available as an open-source project at Github</p> <p>Please consult the following resources for additional details and useful  tutorials.</p> <ul> <li>PyTorch's webpage for Intel extension</li> <li>Intel's Github repository</li> <li>Intel's Documentation</li> </ul>"},{"location":"aurora/data-science/frameworks/pytorch/#pytorch-best-practices-on-aurora","title":"PyTorch Best Practices on Aurora","text":""},{"location":"aurora/data-science/frameworks/pytorch/#single-device-performance","title":"Single Device Performance","text":"<p>To expose one particular device out of the 6 available on a compute node, this environmental variable should be set</p> <p><pre><code>export ZE_AFFINITY_MASK=0.0,0.1\n\n# The values taken by this variable follows the syntax `Device.Sub-device`\n</code></pre> In the example given above, an application is targeting the <code>Device:0</code>  and <code>Sub-devices: 0, 1</code>, i.e. the two tiles of the GPU:0. This is  particularly important in setting a performance benchmarking baseline.</p> <p>More information and details are available through the  Level Zero Specification Documentation - Affinity Mask</p>"},{"location":"aurora/data-science/frameworks/pytorch/#single-node-performance","title":"Single Node Performance","text":"<p>When running PyTorch applications, we have found the following practices to be  generally, if not universally, useful and encourage you to try some of these  techniques to boost performance of your own applications.</p> <ol> <li> <p>Use Reduced Precision. Reduced Precision is available on Intel Max 1550 and  is supported with PyTorch operations. In general, the way to do this is via the  PyTorch Automatic Mixed Precision package (AMP), as descibed in the  mixed precision documentation. In  PyTorch, users generally need to manage casting and loss scaling manually,  though context managers and function decorators can provide easy tools to do  this.</p> </li> <li> <p>PyTorch has a <code>JIT</code> module as well as backends to support op fusion, similar  to TensorFlow's <code>tf.function</code> tools.  Please see TorchScript for more  information.</p> </li> <li> <p><code>torch.compile</code> will be available through the next framework release.</p> </li> </ol>"},{"location":"aurora/data-science/frameworks/pytorch/#multi-gpu-multi-node-scale-up","title":"Multi-GPU / Multi-Node Scale Up","text":"<p>PyTorch is compatible with scaling up to multiple GPUs per node, and across  multiple nodes. Good performance with PyTorch has been seen with both DDP and  Horovod. For details, please see the  Horovod documentation  or the Distributed Data Parallel documentation. Some of the Aurora specific details might be helpful to you:</p>"},{"location":"aurora/data-science/frameworks/pytorch/#environmental-variables","title":"Environmental Variables","text":"<p>The following environmental variables should be set on the batch submission  script (PBSPro script) in the case of attempting to run beyond 16 nodes.</p> <pre><code># This is a fix for running over 16 nodes:\nexport FI_CXI_DEFAULT_CQ_SIZE=131072\nexport FI_CXI_OVFLOW_BUF_SIZE=8388608\nexport FI_CXI_CQ_FILL_PERCENT=20\n\nexport FI_LOG_LEVEL=warn\n#export FI_LOG_PROV=tcp\nexport FI_LOG_PROV=cxi\n\nexport MPIR_CVAR_ENABLE_GPU=0\n# This is to disable certain GPU optimizations like the use of XeLinks between\n# GPUs, collectives with GPU-placed data etc., in order to reduce `MPI_Init`\n# overheads. Benefits are application dependent.\nexport CCL_KVS_GET_TIMEOUT=600\n</code></pre> <p>In order to run an application with <code>TF32</code> precision type, one must set the  following environmental parameter:</p> <p><pre><code>export IPEX_FP32_MATH_MODE=TF32\n</code></pre> This allows calculations using <code>TF32</code> as opposed to the default <code>FP32</code>, and  done through <code>intel_extension_for_pytorch</code> module.</p>"},{"location":"aurora/data-science/frameworks/pytorch/#cpu-affinity","title":"CPU Affinity","text":"<p>The CPU affinity should be set manually through mpiexec.  You can do this the following way:</p> <pre><code>export CPU_BIND=\"verbose,list:2-4:10-12:18-20:26-28:34-36:42-44:54-56:62-64:70-72:78-80:86-88:94-96\"\nmpiexec ... --cpu-bind=${CPU_BIND}\n</code></pre> <p>These bindings should be use along with the following oneCCL and Horovod  environment variable settings:</p> <pre><code>HOROVOD_THREAD_AFFINITY=\"4,12,20,28,36,44,56,64,72,80,88,96\"\nCCL_WORKER_AFFINITY=\"5,13,21,29,37,45,57,65,73,81,89,97\"\n</code></pre> <p>When running 12 ranks per node with these settings the <code>framework</code>s use 3 cores,  with Horovod tightly coupled with the <code>framework</code>s using one of the 3 cores, and  oneCCL using a separate core for better performance, eg. with rank 0 the  <code>framework</code>s would use cores 2,3,4, Horovod would use core 4, and oneCCL would  use core 5.</p> <p>Each workload may perform better with different settings.  The criteria for choosing the cpu bindings are:</p> <ul> <li>Binding for GPU and NIC affinity \u2013 To bind the ranks to cores on the proper      socket or NUMA nodes.</li> <li>Binding for cache access \u2013 This is the part that will change per application      and some experimentation is needed.</li> </ul> <p>Important: This setup is a work in progress, and based on observed  performance. The recommended settings are likely to changed with new <code>framework</code> releases.</p>"},{"location":"aurora/data-science/frameworks/pytorch/#distributed-training","title":"Distributed Training","text":"<p>Distributed training with PyTorch on Aurora is facilitated through both DDP and Horovod. DDP training is accelerated using oneAPI Collective Communications  Library Bindings for Pytorch (oneCCL Bindings for Pytorch).  The extension supports FP32 and BF16 data types.  More detailed information and examples are available at the  Intel oneCCL repo, formerly known as  <code>torch-ccl</code>.</p> <p>The key steps in performing distributed training using  <code>oneccl_bindings_for_pytorch</code> are the following:</p> <pre><code>import os\nimport torch\nimport intel_extension_for_pytorch as ipex\nimport torch.distributed as dist\nimport torch.nn as nn\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport oneccl_bindings_for_pytorch as torch_ccl\n\n...\n\n# perform the necessary transforms\n# set up the training data set\n# set up the data loader\n# set the master address, ports, world size and ranks through os.environ module\n\n...\n# Initialize the process group for distributed training with oneCCL backend\n\ndist.init_process_group(backend='ccl', ... # arguments)\n\nmodel = YOUR_MODEL().to(device)         # device = 'cpu' or 'xpu:{os.environ['MPI_LOCALRANKID']}'\ncriterion = torch.nn. ... .to(device)   # Choose a loss function \noptimizer = torch.optim. ...            # Choose an optimizer\n# model.train()                         # Optional, model dependent\n\n# Off-load the model to ipex for additional optimization \nmodel, optimizer = ipex.optimize(model, optimizer=optimizer)\n\n# Initialize DDP with your model for distributed processing\nif dist.get_world_size() &gt; 1:\n     model = DDP(model, device_ids=[device] if (device != 'cpu') else None)\n\nfor ...\n    # perform the training loop\n</code></pre> <p>A detailed example of the full procedure with a toy model is given here:</p> <ul> <li>Intel's oneCCL demo</li> </ul>"},{"location":"aurora/data-science/frameworks/pytorch/#a-simple-job-script","title":"A Simple Job Script","text":"<p>Below we give an example job script:</p> <pre><code>#!/bin/bash -l\n#PBS -l select=512                              # selecting 512 Nodes\n#PBS -l place=scatter\n#PBS -l walltime=1:59:00\n#PBS -q EarlyAppAccess                          # a specific queue\n#PBS -A Aurora_deployment                       # project allocation\n#PBS -l filesystems=home                        # specific filesystem, can be a list separated by :\n#PBS -k doe\n#PBS -e /home/$USER/path/to/errordir            \n#PBS -o /home/$USER/path/to/outdir              # path to `stdout` or `.OU` files\n#PBS -j oe                                      # output and error placed in the `stdout` file\n#PBS -N a.name.for.the.job\n\n#####################################################################\n# This block configures the total number of ranks, discovering\n# it from PBS variables.\n# 12 Ranks per node, if doing rank/tile\n#####################################################################\n\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=12\nlet NRANKS=${NNODES}*${NRANKS_PER_NODE}\n\n# This is a fix for running over 16 nodes:\nexport FI_CXI_DEFAULT_CQ_SIZE=131072\nexport FI_CXI_OVFLOW_BUF_SIZE=8388608\nexport FI_CXI_CQ_FILL_PERCENT=20\n# These are workaround for a known Cassini overflow issue\n\nexport FI_LOG_LEVEL=warn\n#export FI_LOG_PROV=tcp\nexport FI_LOG_PROV=cxi\n# These allow for logging from a specific provider (libfabric)\n\nexport MPIR_CVAR_ENABLE_GPU=0 \nexport CCL_KVS_GET_TIMEOUT=600\n\n#####################################################################\n# APPLICATION Variables that make a performance difference\n#####################################################################\n\n# Channels last is faster for pytorch, requires code changes!\n# More info here:\n# https://intel.github.io/intel-extension-for-pytorch/xpu/latest/tutorials/features.html#channels-last\n# https://pytorch.org/tutorials/recipes/recipes/intel_extension_for_pytorch.html\nDATA_FORMAT=\"channels_last\"\n\n#####################################################################\n# FRAMEWORK Variables that make a performance difference \n#####################################################################\n\n# Toggle tf32 on (or don't):\nexport IPEX_FP32_MATH_MODE=TF32\n\n#####################################################################\n# End of perf-adjustment section\n#####################################################################\n\n#####################################################################\n# Environment set up, using the latest frameworks drop\n#####################################################################\n\nmodule use /soft/modulefiles\nmodule load frameworks/2023.12.15.001\n\nexport NUMEXPR_NUM_THREADS=64\n# This is to resolve an issue due to a package called \"numexpr\". \n# It sets the variable \n# 'numexpr.nthreads' to available number of threads by default, in this case \n# to 208. However, the 'NUMEXPR_MAX_THREADS' is also set to 64 as a package \n# default. The solution is to either set the 'NUMEXPR_NUM_THREADS' to less than \n# or equal to '64' or to increase the 'NUMEXPR_MAX_THREADS' to the available \n# number of threads. Both of these variables can be set manually.\n\n#####################################################################\n# End of environment setup section\n#####################################################################\n\n#####################################################################\n# JOB LAUNCH\n######################################################################\n\nexport CCL_LOG_LEVEL=\"WARN\"\nexport CPU_BIND=\"verbose,list:2-4:10-12:18-20:26-28:34-36:42-44:54-56:62-64:70-72:78-80:86-88:94-96\"\nHOROVOD_THREAD_AFFINITY=\"4,12,20,28,36,44,56,64,72,80,88,96\"\nCCL_WORKER_AFFINITY=\"5,13,21,29,37,45,57,65,73,81,89,97\"\n\nulimit -c 0\n\n# Launch the script\nmpiexec -np ${NRANKS} -ppn ${NRANKS_PER_NODE} \\\n--cpu-bind ${CPU_BIND} \\\npython path/to/application.py\n</code></pre>"},{"location":"aurora/data-science/frameworks/tensorflow/","title":"TensorFlow on Aurora","text":"<p>TensorFlow is a popular, open-source deep learning framework developed and  released by Google. The  TensorFlow home page has more information about  TensorFlow, which you can refer to. For trouble shooting on Polaris, please  contact support@alcf.anl.gov.</p>"},{"location":"aurora/data-science/frameworks/tensorflow/#installation-on-aurora","title":"Installation on Aurora","text":"<p>TensorFlow is already pre-installed on Aurora, available in the <code>frameworks</code>  module. To use it from a compute node, please do:</p> <pre><code>module use /soft/modulefiles/\nmodule load frameworks/2023.12.15.001\n</code></pre> <p>Then you can <code>import</code> TensorFlow as usual, the following is an output from the  <code>frameworks/2023.12.15.001</code> module:</p> <p><pre><code>&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; tf.__version__\n'2.14.1'\n</code></pre> A simple but useful check could be to use TensorFlow to get device information  on a compute node. You can do this the following way:</p> <pre><code>&gt;&gt;&gt; tf.config.list_physical_devices()\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), \nPhysicalDevice(name='/physical_device:XPU:0', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:1', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:2', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:3', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:4', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:5', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:6', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:7', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:8', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:9', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:10', device_type='XPU'), \nPhysicalDevice(name='/physical_device:XPU:11', device_type='XPU')]\n</code></pre> <p>Note that, here <code>tf.config</code> return 12 tiles of 6 cards (the number of GPU  resources on an Aurora compute node), and treat each tile as a device. The user can choose to set the environmental variable <code>ZE_FLAT_DEVICE_HIERARCHY</code> with  appropriate values to achieve desired behavior, as described in the  Level Zero Specification documentation. This environment variable is equivalent to the <code>ITEX_TILE_AS_DEVICE</code>, which is to be deprecated soon.</p> <p>Intel extension for TensorFLow is has been made publicly available as an  open-source project at  Github.</p> <p>Please consult the following resources for additional details and useful tutorials.</p> <ul> <li>Intel's Documentation</li> <li>Intel's Examples</li> <li>Intel's ITEX Features Guide</li> <li>Intel's Practice Guide</li> </ul>"},{"location":"aurora/data-science/frameworks/tensorflow/#tensorflow-best-practices-on-aurora","title":"TensorFlow Best Practices on Aurora","text":""},{"location":"aurora/data-science/frameworks/tensorflow/#single-device-performance","title":"Single Device Performance","text":"<p>To expose one particular device out of the 6 available on a compute node,  this environmental variable should be set</p> <p><pre><code>export ZE_AFFINITY_MASK=0.0,0.1\n\n# The values taken by this variable follows the syntax `Device.Sub-device`\n</code></pre> In the example given above, an application is targeting the  Device:0 and Sub-devices: 0, 1, i.e. the two tiles of the GPU:0.  This is particularly important in setting a performance benchmarking baseline.</p> <p>More information and details are available through Level Zero Specification Documentation - Affinity Mask</p>"},{"location":"aurora/data-science/frameworks/tensorflow/#single-node-performance","title":"Single Node Performance","text":"<p>When running TensorFlow applications, we have found the following practices to  be generally, if not universally, useful and encourage you to try some of these  techniques to boost performance of your own applications.</p>"},{"location":"aurora/data-science/frameworks/tensorflow/#reduced-precision","title":"Reduced Precision","text":"<p>Use Reduced Precision, whenever the application allows. Reduced Precision is  available on Intel Max 1550 and is supported with TensorFlow operations. In  general, the way to do this is via the <code>tf.keras.mixed_precision</code> Policy, as  descibed in the  mixed precision documentation Intel's extension for TensorFlow is fully compatible with the Keras mixed  precision API in TensorFlow. It also provides an advanced auto mixed precision  feature. For example, you can just set two environment variables to get the  performance benefit from low-precision data type <code>FP16</code>/<code>BF16</code> without changing the  application code.</p> <p><pre><code>export ITEX_AUTO_MIXED_PRECISION=1\nexport ITEX_AUTO_MIXED_PRECISION_DATA_TYPE=\"BFLOAT16\" # or \"FLOAT16\"\n</code></pre> If you use a custom training loop (and not <code>keras.Model.fit</code>), you will also  need to apply loss scaling.</p>"},{"location":"aurora/data-science/frameworks/tensorflow/#tensorflows-graph-api","title":"TensorFlow's graph API","text":"<p>Use TensorFlow's graph API to improve efficiency of operations. TensorFlow is,  in general, an imperative language but with function decorators like  <code>@tf.function</code> you can trace functions in your code. Tracing replaces your  python function with a lower-level, semi-compiled TensorFlow Graph. More  information about the <code>tf.function</code> interface is available  here.  When possible, use <code>jit_compile</code>, but be aware of sharp bits when using  <code>tf.function</code>: python expressions that aren't tensors are often replaced as  constants in the graph, which may or may not be your intention.</p> <p>There is an experimental feature, which allows for aggressive fusion of kernels through  oneDNN Graph API. Intel's extension for TensorFlow can offload performance critical graph  partitions to oneDNN library to get more aggressive graph optimizations. It can be done by setting this environmental variable:</p> <p><pre><code>export ITEX_ONEDNN_GRAPH=1\n</code></pre> This feature is experimental, and actively under development.</p>"},{"location":"aurora/data-science/frameworks/tensorflow/#tf32-math-mode","title":"<code>TF32</code> Math Mode","text":"<p>The Intel Xe Matrix Extensions (Intel XMX) engines in Intel Max 1550 Xe-HPC  GPUs natively support <code>TF32</code> math mode. Through intel extension for tensorflow you can enable it by setting the following environmental variable:</p> <pre><code>export ITEX_FP32_MATH_MODE=\"TF32\"\n</code></pre>"},{"location":"aurora/data-science/frameworks/tensorflow/#xla-compilation-plannedupcoming","title":"XLA Compilation (Planned/Upcoming)","text":"<p>XLA is the Accelerated Linear Algebra library that is available in TensorFlow  and critical in software like JAX. XLA will compile a <code>tf.Graph</code> object,  generated with <code>tf.function</code> or similar, and perform optimizations like  operation-fusion. XLA can give impressive performance boosts with almost no  user changes except to set an environment variable <code>TF_XLA_FLAGS=--tf_xla_auto_jit=2</code>.  If your code is complex, or has dynamically sized tensors (tensors where the  shape changes every iteration), XLA can be detrimental: the overhead for  compiling functions can be large enough to mitigate performance improvements.  XLA is particularly powerful when combined with reduced precision,  yielding speedups &gt; 100% in some models. </p> <p>Intel provides initial intel GPU support for TensorFlow models with XLA  acceleration through  Intel Extension for OpenXLA. Full TensorFlow and PyTorch support is planned for development.</p>"},{"location":"aurora/data-science/frameworks/tensorflow/#a-simple-example","title":"A simple example","text":"<p>A simple example on how to use Intel GPU with TensorFlow is the following:</p> <pre><code>import tensorflow as tf   # TensorFlow registers PluggableDevices here.\ntf.config.list_physical_devices()  # XPU device is visible to TensorFlow.\n\n#Section 1 Run implicitly\na = tf.random.normal(shape=[5], dtype=tf.float32)  # Runs on XPU.\nb = tf.nn.relu(a)         # Runs on XPU .\n\n#Section 2 Run with explicit device setting\nwith tf.device(\"/XPU:0\"):  # Users can also use 'with tf.device' syntax.\n  c = tf.nn.relu(a)        # Runs on XPU.\nwith tf.device(\"/CPU:0\"):\n  c = tf.nn.relu(a)        # Runs on CPU.\n\n#Section 3 Run with graph mode\n@tf.function  # Defining a tf.function\ndef run():\n  d = tf.random.uniform(shape=[100], dtype=tf.float32)\n  e = tf.nn.relu(d)\nrun()  # PluggableDevices also work with tf.function and graph mode. Runs on XPU\n</code></pre>"},{"location":"aurora/data-science/frameworks/tensorflow/#multi-gpu-multi-node-scale-up","title":"Multi-GPU / Multi-Node Scale Up","text":"<p>TensorFlow is compatible with scaling up to multiple GPUs per node, and across  multiple nodes. Good performance with tensorFlow has been seen with horovod in  particular. For details, please see the  Horovod documentation. Some Aurora specific details might be helpful to you.</p>"},{"location":"aurora/data-science/frameworks/tensorflow/#environment-variables","title":"Environment Variables","text":"<p>The following environmental variables should be set on the batch submission  script (PBSPro script) in the case of attempting to run beyond 16 nodes.</p> <pre><code># This is a fix for running over 16 nodes:\nexport FI_CXI_DEFAULT_CQ_SIZE=131072\nexport FI_CXI_OVFLOW_BUF_SIZE=8388608\nexport FI_CXI_CQ_FILL_PERCENT=20\n\nexport FI_LOG_LEVEL=warn\n#export FI_LOG_PROV=tcp\nexport FI_LOG_PROV=cxi\n\nexport MPIR_CVAR_ENABLE_GPU=0\n# This is to disable certain GPU optimizations like the use of XeLinks between\n# GPUs, collectives with GPU-placed data etc., in order to reduce `MPI_Init`\n# overheads. Benefits are application dependent.\nexport CCL_KVS_GET_TIMEOUT=600\n</code></pre>"},{"location":"aurora/data-science/frameworks/tensorflow/#cpu-affinity","title":"CPU Affinity","text":"<p>The CPU affinity should be set manually through mpiexec.  You can do this the following way:</p> <pre><code>export CPU_BIND=\"verbose,list:2-4:10-12:18-20:26-28:34-36:42-44:54-56:62-64:70-72:78-80:86-88:94-96\"\nmpiexec ... --cpu-bind=${CPU_BIND}\n</code></pre> <p>These bindings should be use along with the following oneCCL and Horovod  environment variable settings:</p> <pre><code>HOROVOD_THREAD_AFFINITY=\"4,12,20,28,36,44,56,64,72,80,88,96\"\nCCL_WORKER_AFFINITY=\"5,13,21,29,37,45,57,65,73,81,89,97\"\n</code></pre> <p>When running 12 ranks per node with these settings the <code>framework</code>s use 3 cores,  with Horovod tightly coupled with the <code>framework</code>s using one of the 3 cores, and  oneCCL using a separate core for better performance, eg. with rank 0 the  <code>framework</code>s would use cores 2,3,4, Horovod would use core 4, and oneCCL would  use core 5.</p> <p>Each workload may perform better with different settings.  The criteria for choosing the cpu bindings are:</p> <ul> <li>Binding for GPU and NIC affinity \u2013 To bind the ranks to cores on the proper      socket or NUMA nodes.</li> <li>Binding for cache access \u2013 This is the part that will change per application      and some experimentation is needed.</li> </ul> <p>Important: This setup is a work in progress, and based on observed  performance. The recommended settings are likely to changed with new <code>framework</code> releases.</p>"},{"location":"aurora/data-science/frameworks/tensorflow/#distributed-training","title":"Distributed Training","text":"<p>Distributed training with TensorFlow  on Aurora is facilitated through Horovod, using Intel Optimization for Horovod.</p> <p>The key steps in performing distributed training are laid out in the following example:</p> <ul> <li>Tensorflow examples with Intel Optimization for Horovod</li> </ul> <p>Detailed implementation of the same example is here:</p> <ul> <li>TensorFlow with Keras and Horovod</li> </ul> <p>A suite of detailed and well documented examples is part of Intel's optimization for Horovod repository:</p> <ul> <li>Distributed Training Example Suite</li> </ul>"},{"location":"aurora/data-science/frameworks/tensorflow/#a-simple-job-script","title":"A simple Job Script","text":"<p>Below we give a simple job script:</p> <pre><code>#!/bin/bash -l\n#PBS -l select=512                              # selecting 512 Nodes\n#PBS -l place=scatter\n#PBS -l walltime=1:59:00\n#PBS -q EarlyAppAccess                          # a specific queue\n#PBS -A Aurora_deployment                       # project allocation\n#PBS -l filesystems=home                        # specific filesystem, can be a list separated by :\n#PBS -k doe\n#PBS -e /home/$USER/path/to/errordir\n#PBS -o /home/$USER/path/to/outdir              # path to `stdout` or `.OU` files\n#PBS -j oe                                      # output and error placed in the `stdout` file\n#PBS -N a.name.for.the.job\n\n#####################################################################\n# This block configures the total number of ranks, discovering\n# it from PBS variables.\n# 12 Ranks per node, if doing rank/tile\n#####################################################################\n\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=12\nlet NRANKS=${NNODES}*${NRANKS_PER_NODE}\n\n# This is a fix for running over 16 nodes:\nexport FI_CXI_DEFAULT_CQ_SIZE=131072\nexport FI_CXI_OVFLOW_BUF_SIZE=8388608\nexport FI_CXI_CQ_FILL_PERCENT=20\n# These are workaround for a known Cassini overflow issue\n\nexport FI_LOG_LEVEL=warn\n#export FI_LOG_PROV=tcp\nexport FI_LOG_PROV=cxi\n# These allow for logging from a specific provider (libfabric)\n\nexport MPIR_CVAR_ENABLE_GPU=0\nexport CCL_KVS_GET_TIMEOUT=600\n\n#####################################################################\n# FRAMEWORK Variables that make a performance difference\n#####################################################################\n\n# Toggle tf32 on (or don't):\nexport ITEX_FP32_MATH_MODE=TF32\n\n#####################################################################\n# End of perf-adjustment section\n#####################################################################\n\n#####################################################################\n# Environment set up, using the latest frameworks drop\n#####################################################################\n\nmodule use /soft/modulefiles\nmodule load frameworks/2023.12.15.001\n\nexport NUMEXPR_NUM_THREADS=64\n# This is to resolve an issue due to a package called \"numexpr\".\n# It sets the variable\n# 'numexpr.nthreads' to available number of threads by default, in this case\n# to 208. However, the 'NUMEXPR_MAX_THREADS' is also set to 64 as a package\n# default. The solution is to either set the 'NUMEXPR_NUM_THREADS' to less than\n# or equal to '64' or to increase the 'NUMEXPR_MAX_THREADS' to the available\n# number of threads. Both of these variables can be set manually.\n\n#####################################################################\n# End of environment setup section\n#####################################################################\n\n#####################################################################\n# JOB LAUNCH\n######################################################################\n\nexport CCL_LOG_LEVEL=\"WARN\"\nexport CPU_BIND=\"verbose,list:2-4:10-12:18-20:26-28:34-36:42-44:54-56:62-64:70-72:78-80:86-88:94-96\"\nHOROVOD_THREAD_AFFINITY=\"4,12,20,28,36,44,56,64,72,80,88,96\"\nCCL_WORKER_AFFINITY=\"5,13,21,29,37,45,57,65,73,81,89,97\"\n\nulimit -c 0\n\n# Launch the script\nmpiexec -np ${NRANKS} -ppn ${NRANKS_PER_NODE} \\\n--cpu-bind ${CPU_BIND} \\\npython path/to/application.py\n</code></pre>"},{"location":"aurora/data-science/libraries/openvino/","title":"Model Inference with OpenVINO","text":"<p>OpenVINO is a library developed by Intel specifically designed for accelerating inference of ML models on their CPU and GPU hardware.  This page contains build and run instructions for Python and C/C++ examples, but please refer to the full documentation for more information.</p>"},{"location":"aurora/data-science/libraries/openvino/#instlling-the-openvino-python-runtime-and-cli-tools","title":"Instlling the OpenVINO Python Runtime and CLI Tools","text":"<p>OpenVINO does not come with the default frameworks module on Aurora, but it can be installed manually within a virtual environment as shown below <pre><code>module use /soft/modulefiles\nmodule load frameworks/2023.12.15.001\npython -m venv --clear /path/to/_ov_env --system-site-packages\nsource /path/to/_ov_env/bin/activate\npip install openvino==2023.2\npip install openvino-dev==2023.2\npip install onnx\n</code></pre></p> <p>Note that <code>/path/to/</code> can either be a user's home or project directory.</p> <p>To use OpenVINO in the future, simply load the frameworks module and source the virtual environment. <pre><code>module use /soft/modulefiles\nmodule load frameworks/2023.12.15.001\nsource /path/to/_ov_env/bin/activate\n</code></pre></p>"},{"location":"aurora/data-science/libraries/openvino/#model-converter","title":"Model Converter","text":"<p>The first suggested step is to convert the model from one of the ML frameworks into OpenVINO's Intermediate Representation (IR).  This consists of an <code>.xml</code> file which describes the network topology and a <code>.bin</code> file which contains the weights and biases in binary format.  The conversion can be done from the command line with <code>ovc</code> or using the Pyrhon API <code>openvino.comvert_model()</code>. Note that PyTorch models cannot be converted directly with <code>ovc</code> and need to be converted to ONNX format first. You can find more information on the conversion process on OpenVINO's documentation page.</p> <p>The following code snippet demonstrates how to convert the ResNet50 model from TorchVision and save the OpenVINO IR.</p> <pre><code>import openvino as ov\nimport torch\nfrom torchvision.models import resnet50\n\nmodel = resnet50(weights='DEFAULT')\ninput_data = torch.rand(1, 3, 224, 224)\n\nov_model = ov.convert_model(model, example_input=input_data)\nov.save_model(ov_model, 'resnet50.xml')\n</code></pre> <p>Information on using the CLI conversion tool can be found running <code>ovc -h</code>, which will save the model in IR format by default.</p> <p>Note that by default, both <code>ovc</code> and <code>openvino.save_model()</code> perform compression of the model weights to FP16. This reduces the memory needed to store the model and can provide an increase in performance in many cases. To disable this feature, use <pre><code>ov.save_model(ov_model, 'resnet50.xml', compress_to_fp16=False)\n</code></pre></p> <p>or</p> <pre><code>ovc &lt;/path/to/model.onnx&gt; --compress_to_fp16=False\n</code></pre>"},{"location":"aurora/data-science/libraries/openvino/#benchmark-app","title":"Benchmark App","text":"<p>Before writing a script or program to perform inference with the OpenVINO runtime, the performance of the model can be tested with the CLI tool <code>benchmark_app</code>. </p> <p>A minimal example to run on a single PVC tile is shown below <pre><code>benchmark_app -m resnet50.xml -hint latency -d GPU.0 -data_shape [1,3,224,224]\n</code></pre></p> <p>which returns a series of information on the parameters set for the benchmark tests and the performance of the tests. The last few lines of the output are shown below.</p> <pre><code>[ INFO ] Execution Devices:['OCL_GPU.0']\n[ INFO ] Count:            6424 iterations\n[ INFO ] Duration:         60011.14 ms\n[ INFO ] Latency:\n[ INFO ]    Median:        9.23 ms\n[ INFO ]    Average:       9.25 ms\n[ INFO ]    Min:           9.00 ms\n[ INFO ]    Max:           11.69 ms\n[ INFO ] Throughput:   107.05 FPS\n</code></pre> <p>Note that <code>benchmark_app</code> takes a number of additional configuration options as described here and running <code>benchmark_app -h</code>. </p>"},{"location":"aurora/data-science/libraries/openvino/#inference-with-python-openvino-api","title":"Inference with Python OpenVINO API","text":"<p>Inference can be performed invoking the compiled model directly or using the OpenVINO Runtime API explicitly to create inference requests.</p> <p>An example of performing direct inference with the compiled model is shown below.  This leads to compact code, but it performs a single synchronous inference request.  Future calls to the model will reuse the same inference request created, thus will experience less overhead. <pre><code>import openvino as ov\nimport openvino.properties.hint as hints\nimport torch\n\ncore = ov.Core()\nconfig = {hints.inference_precision: 'f32'}\ncompiled_model = core.compile_model(\"resnet50.xml\",device_name='GPU.0', config=config)\ninput_data = torch.rand((1, 3, 224, 224))\nresults = compiled_model(input_data)[0]\n</code></pre></p> <p>Note:</p> <ul> <li>The output of the direct call to the compiled model is a NumPy array</li> <li>By default, OpenVINO performs inference with FP16 precision on GPU, therefore the precision type must be specified as a hint during model compilation if FP32 or other precisions are desired.</li> </ul> <p>Other than the direct call to the model, the Runtime API can be used to create inference requests and control their execution. For this approach we refer the user to the OpenVINO documentation page, which clearly outlines the steps involved. </p>"},{"location":"aurora/data-science/libraries/openvino/#inference-with-c-openvino-api","title":"Inference with C++ OpenVINO API","text":"<p>Currently, the C++ OpenVINO API on Aurora is enabled through a pre-built set of libraries. The environment is set as follows, with <code>/path/to/openvino</code> being a placeholder for the user to specify <pre><code>module use /soft/modulefiles\nmodule load spack-pe-gcc\nmodule load cmake\n\nexport OV_PATH=/path/to/openvino\ncp /home/balin/OpenVINO/SLES15.3/openvino-suse.tar.gz $OV_PATH\ntar -xzvf $OV_PATH/openvino-suse.tar.gz -C $OV_PATH\nsource $OV_PATH/openvino/setupvars.sh\n\n# Need to add a path to the libtbb.so.2 library needed by OpenVINO\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/datascience/llm_ds/basekit_2023_0_25537/vtune/2023.0.0/lib64\nexport ONEAPI_DEVICE_SELECTOR=opencl:gpu\nexport ZE_AFFINITY_MASK=0.0\n</code></pre></p> <p>An example performing inference with the C++ OpenVINO API is shown below. This simple program loads the ResNet50 model in OpenVINO IR format to the GPU (see instructions above on how to download and convert the model), creates an input vector and offloads it to the GPU with SYCL, and finally executes a single synchronous inference request on the GPU.</p> <pre><code>#include &lt;iostream&gt;\n#include &lt;cstdlib&gt;\n#include &lt;vector&gt;\n#include \"sycl/sycl.hpp\"\n#include \"openvino/openvino.hpp\"\n#include \"openvino/runtime/intel_gpu/ocl/ocl.hpp\"\n\nconst int N_BATCH = 1;\nconst int N_CHANNELS = 3;\nconst int N_PIXELS = 224;\nconst int INPUTS_SIZE = N_BATCH*N_CHANNELS*N_PIXELS*N_PIXELS;\n\nint main(int argc, const char* argv[])\n{\n  // Print some information about OpenVINO and start the runtime\n  std::cout &lt;&lt; \"Running with \" &lt;&lt; ov::get_openvino_version() &lt;&lt; \"\\n\\n\";\n  ov::Core core;\n  std::vector&lt;std::string&gt; availableDevices = core.get_available_devices();\n  char device_str[] = \"GPU\";\n  bool found_device = false;\n  for (auto&amp;&amp; device : availableDevices) {\n    if (strcmp(device.c_str(),device_str)==0) {\n      std::cout &lt;&lt; \"Found device \" &lt;&lt; device &lt;&lt; \" \\n\\n\";\n      found_device = true;\n    }\n  }\n  if (not found_device) {\n    std::cout &lt;&lt; \"Input device not found \\n\";\n    std::cout &lt;&lt; \"Available devices are: \\n\";\n    for (auto&amp;&amp; device : availableDevices) {\n      std::cout &lt;&lt; device &lt;&lt; std::endl;\n    }\n    return -1;\n  }\n\n  // Load the model\n  std::shared_ptr&lt;ov::Model&gt; model = core.read_model(\"./resnet50.xml\");\n  std::cout &lt;&lt; \"Loaded model \\n\\n\";\n\n  // Create the input data on the host\n  std::vector&lt;float&gt; inputs(INPUTS_SIZE);\n  srand(12345);\n  for (int i=0; i&lt;INPUTS_SIZE; i++) {\n    inputs[i] = static_cast &lt;float&gt; (rand()) / static_cast &lt;float&gt; (RAND_MAX);\n  }\n  std::cout &lt;&lt; \"Generated input data on the host \\n\\n\";\n\n  // Move input data to the device with SYCL\n  sycl::queue Q(sycl::gpu_selector_v, sycl::property::queue::in_order{}); // oneDNN needs in order queues\n  std::cout &lt;&lt; \"SYCL running on \"\n            &lt;&lt; Q.get_device().get_info&lt;sycl::info::device::name&gt;()\n            &lt;&lt; \"\\n\\n\";\n  float *d_inputs = sycl::malloc_device&lt;float&gt;(INPUTS_SIZE, Q);\n  Q.memcpy((void *) d_inputs, (void *) inputs.data(), INPUTS_SIZE*sizeof(float));\n  Q.wait();\n\n  // Share the SYCL queue and context with the GPU plugin and compile the model\n  auto queue = sycl::get_native&lt;sycl::backend::opencl&gt;(Q);\n  auto remote_context = ov::intel_gpu::ocl::ClContext(core, queue);\n  auto compiled_model = core.compile_model(model, remote_context,\n                                           ov::hint::inference_precision(\"f32\"));\n\n  // Convert input array to OpenVINO Tensor\n  ov::element::Type input_type = ov::element::f32;\n  ov::Shape input_shape = {N_BATCH, N_CHANNELS, N_PIXELS, N_PIXELS};\n  //ov::Tensor input_tensor = ov::Tensor(input_type, input_shape, d_inputs);\n  auto input_tensor = remote_context.create_tensor(input_type, input_shape, (void *) d_inputs);\n\n  // Run inference\n  ov::InferRequest infer_request = compiled_model.create_infer_request();\n  infer_request.set_input_tensor(input_tensor);\n  infer_request.infer();\n  std::cout &lt;&lt; \"Performed inference \\n\\n\";\n\n  // Output the predicted Torch tensor\n  ov::Tensor output_tensor = infer_request.get_output_tensor();\n  std::cout &lt;&lt; \"Size of output tensor \" &lt;&lt; output_tensor.get_shape() &lt;&lt; std::endl;\n  std::cout &lt;&lt; \"Predicted tensor is : \\n\";\n  for (int i=0; i&lt;10; i++) {\n    std::cout &lt;&lt; output_tensor.data&lt;float&gt;()[i] &lt;&lt; \"\\n\";\n  }\n  std::cout &lt;&lt; \"\\n\";\n\n  return 0;\n}\n</code></pre> <p>To build the example program, use the <code>CMakeLists.txt</code> file below <pre><code>cmake_minimum_required(VERSION 3.5 FATAL_ERROR)\nproject(inference_openvino_sycl_example)\n\nfind_package(OpenVINO REQUIRED COMPONENTS Runtime)\nset(ov_link_libraries openvino::runtime)\n\nadd_executable(inference_openvino_sycl inference_openvino_sycl.cpp)\ntarget_link_libraries(inference_openvino_sycl ${ov_link_libraries} -lOpenCL)\n\nset_property(TARGET inference_openvino_sycl PROPERTY CXX_STANDARD 17)\n</code></pre></p> <p>and execute <pre><code>cmake -DCMAKE_CXX_FLAGS=\"-std=c++17 -fsycl\" ./\nmake\n./inference_openvino_sycl\n</code></pre></p> <p>Note:</p> <ul> <li>OpenVINO does not currently support the Level Zero backend. OpenCL must be used instead, which can be set on Aurora with <code>export ONEAPI_DEVICE_SELECTOR=opencl:gpu</code></li> <li>The Remote Tensor API must be used to share the SYCL OpenCL context with OpenVINO</li> </ul>"},{"location":"aurora/debugging/debugging-overview/","title":"Debugging on Aurora","text":""},{"location":"aurora/debugging/debugging-overview/#hpe-gdb4hpc","title":"HPE gdb4hpc","text":"<p>The gdb4hpc is not a GPU-aware debugger but can be used to debug general code problems at scale. This debugger will apply commands to all threads in the MPI process group.</p>"},{"location":"aurora/debugging/debugging-overview/#attaching-to-a-running-job","title":"Attaching to a running job","text":"<p>Determine the jobid of interest.</p> <pre><code>  qstat -u $USER\n</code></pre> <pre><code>  harms@aurora-uan-0009:~/working/all2all&gt; qstat -u $USER\n\n  aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov: \n                                                            Req'd  Req'd   Elap\n  Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time\n  --------------- -------- -------- ---------- ------ --- --- ------ ----- - -----\n  127750.aurora-* harms    workq    all2all       --    4   4    --  00:30 R   -- \n</code></pre> <p>Next find a node the job is running on. Choose the first node in the list of vnodes.</p> <pre><code>  qstat -f 127750 | grep exec_vnode\n</code></pre> <pre><code>  harms@aurora-uan-0009:~/working/all2all&gt; qstat -f 127750 | grep exec_vnode\n      exec_vnode = (x4305c2s6b0n0:ncpus=1)+(x4305c2s7b0n0:ncpus=1)+(x4305c4s0b0n0\n</code></pre> <p>Login to this node, find your mpiexec process id and run gdb4hpc.</p> <pre><code>  ssh x4305c2s6b0n0\n  ps -eaf | grep mpiexec\n  module load gdb4hpc\n  CTI_WLM_IMPL=ssh gdb4hpc\n</code></pre> <pre><code>  harms@aurora-uan-0009:~/working/all2all&gt; ssh x4305c2s6b0n0\n  harms@x4305c2s6b0n0:~&gt; ps -eaf | grep mpiexec\n  harms    108581 108569  0 16:05 ?        00:00:00 mpiexec -l --no-transfer --line-buffer --np 16 -ppn 4 --cpu-bind core ./a2a-p2p\n  harms    109440 109354  0 16:11 pts/4    00:00:00 grep --color=auto mpiexec\n  harms@x4305c2s6b0n0:~&gt; module load gdb4hpc\n  harms@x4305c2s6b0n0:~&gt; CTI_WLM_IMPL=ssh gdb4hpc\n  gdb4hpc 4.14.7 - Cray Line Mode Parallel Debugger\n  With Cray Comparative Debugging Technology.\n  Copyright 2007-2022 Hewlett Packard Enterprise Development LP.\n  Copyright 1996-2016 University of Queensland. All Rights Reserved.\n\n  Type \"help\" for a list of commands.\n  Type \"help &lt;cmd&gt;\" for detailed help about a command.\n  dbg all&gt;\n</code></pre> <p>Now attach to the mpiexec process.</p> <pre><code>  dbg all&gt; attach $a &lt;pid&gt;\n</code></pre> <pre><code>  dbg all&gt; attach $a 108581\n  0/16 ranks connected... (timeout in 299 seconds)\n  0/16 ranks connected... (timeout in 298 seconds)\n  ..\n  12/16 ranks connected... (timeout in 300 seconds)\n  16/16 ranks connected.\n  Created network...\n  Connected to application...\n  Current rank location:\n  a{0}: #0  0x00001472aba12699 in MPIDI_progress_test\n  ... backtrace ...\n</code></pre>"},{"location":"aurora/debugging/gdb-oneapi-aurora/","title":"gdb-oneapi on Aurora","text":"<p>Placeholder</p>"},{"location":"aurora/hardware-overview/machine-overview/","title":"Aurora System Overview","text":"<p>Aurora is a 10,624 node HPE Cray-Ex based system. It has 166 racks with 21,248 CPUs and 63,744 GPUs. Each node consists of 2 Intel Xeon CPU Max Series (codename Sapphire Rapids or SPR)  with on-package HBM and 6 Intel Data Center GPU Max Series (codename Ponte Vecchio or PVC).  Each Xeon has 52 physical cores supporting 2 hardware threads per core and 64GB of HBM. Each CPU has 512 GB of DDR5. The GPUs are connected all-to-all with Intel XeLink interfaces. Each node has 8 HPE Slingshot-11 NICs, and the system is connected in a dragonfly topology. The GPUs may send messages directly to the NIC via PCIe, without the need to copy into CPU memory.</p> <p>Intel Data Center GPU Max Series is based on Xe Core.  Each Xe core consist of 8 vector engines and 8 matrix engines with 512 kb of L1 cache that  can be configured as cache or Shared Local Memroy (SLM). 16 Xe cores are grouped together to form a slice. 4 slicess are combined along with a large L2 cache, 4 HBM2E memory controllers to form s stack or tile. One or more stacks/tiles can then be combined on a socket to form a GPU.  More detailed information about node architecture can be found here:  https://www.intel.com/content/www/us/en/developer/articles/technical/intel-data-center-gpu-max-series-overview.html</p>"},{"location":"aurora/hardware-overview/machine-overview/#aurora-compute-node","title":"Aurora Compute Node","text":"NODE COMPONENT DESCRIPTION PER NODE AGGREGATE Processor 2000 MHz 2 21,248 Cores/Threads Intel Xeon CPU Max 9470C Series 104/208 1,104,896/2,209,792 CPU HBM HBM2e 64x2 GiB 1.328 PiB CPU DRAM DDR5 512x2 GiB 10.375 PiB GPUS Intel Data Center Max 1550 Series 6 63,744 GPU HBM HBM2e 768 GiB 7.968 PiB"},{"location":"aurora/hardware-overview/machine-overview/#aurora-pvc-gpu-components","title":"Aurora PVC GPU Components","text":"GPU COMPONENT DESCRIPTION COUNT CAPABILITY Stack a.k.a. Tile 2 Xe Vector Engine a.k.a. EU (execution unit) 512 per Stack (4?? active) 8 threads, 512b SIMD Xe Matrix Engine a.k.a  systolic part of EU 512 per Stack (4?? active) Register 512 bit register 128 per thread Xe Core a.k.a. subslice; unit of 8 EUs 64 per Stack 128 per GPU L1 cache 128 KiB Last Level cache a.k.a. RAMBO cache 384 MiB per GPU"},{"location":"aurora/node-performance-overview/node-performance-overview/","title":"Single node \"GPU-Peak\" benchmarks","text":"<p>This work was done on a pre-production supercomputer with early versions of the Aurora software development kit</p> <p>This page aims to give you a high-level overview of key performance numbers for a single Aurora node.</p> <ul> <li>We are providing both 1 Tile and Full Node numbers.</li> <li>The Full Node numbers are the Weak scaling version of the single node one.</li> <li>The Full Node numbers have been achieved by one Rank per Tile, 12 Ranks.</li> <li>All benchmarks' source code and launch options are included so you can tweak them as needed.</li> <li>We are not exhaustive. Please assume we cherry-picked the correct size to get the best numbers.</li> <li>We will not compare the results to some \u201ctheoretical\u201d value.  Theoretical values are full of assumptions, and we want to keep this page short.</li> <li>We will not compare the results to other hardware. Feel free to do it yourself \ud83d\ude42</li> <li>To improve reproducibility, only the \u201cbest\u201d numbers are reported (e.g., we take the minimum time of repetition step). When doing \"real\" science, please perform better statistical analysis.</li> <li>The code will use a mixture of OpenMP and SYCL in C++ (sorry, Fortran, Python, and Level Zero lovers).</li> </ul> <p>The asterisk (<code>*</code>) means that the data was collected on sunspot with older software stack...</p>"},{"location":"aurora/node-performance-overview/node-performance-overview/#micro-benchmarks","title":"Micro-benchmarks","text":"One Tile Full Node Scaling Single Precision Peak Flops 23 TFlop/s 267 TFlop/s 11.8 Double Precision Peak Flops 17 TFlop/s 187 TFlop/s 10.9 Memory Bandwidth (triad) 1 TB/s 12 TB/s 11.9 PCIe Unidirectional Bandwidth (H2D) 54 GB/s 329 GB/s 6.1 PCIe Unidirectional Bandwidth (D2H) 55 GB/s 263 GB/s 4.8 PCIe Bidirectional Bandwidth 76 GB/s 357 GB/s 4.7 Tile2Tile Unidirectional Bandwidth 196 GB/s 1 TB/s 6.0 Tile2Tile Bidirectional Bandwidth 287 GB/s 2 TB/s 5.9 GPU2GPU Unidirectional Bandwidth 15 GB/s 95 GB/s 6.3 GPU2GPU Bidirectional Bandwidth 23 GB/s 142 GB/s 6.2"},{"location":"aurora/node-performance-overview/node-performance-overview/#benchmark-description","title":"Benchmark description","text":"<ul> <li>Double Precision Peak Flops: Chain of FMA.</li> <li>Memory Bandwidth (triad): Triad, 2 load, 1 store</li> <li>PCIe Unidirectional Bandwidth (H2D): Host to Device data-transfert</li> <li>PCIe Unidirectional Bandwidth (H2D): Device to Host data-transfert</li> <li>PCIe Unidirectional Bandwidth (H2D): Concurent Host to Device and Device to Host data-transfert</li> <li>Tile2Tile Unidirectional Bandwidth: MPI Rank 0 (GPU N, Tile 0) will send a GPU buffer to Rank 1 (GPU N, Tile 1)</li> <li>Tile2Tile Unidirectional Bandwidth: MPI Rank 0 (GPU N, Tile 0) will send a GPU buffer to Rank 1 (GPU N, Tile 1). Concurently, Rank 1 will also send a buffer to Rank 0</li> <li>GPU2GPU Unidirectional Bandwidth: MPI Rank 0 (GPU 0,Tile 0) will send a GPU buffer to Rank 1 (GPU 1, Tile 0).</li> <li>GPU2GPU Bidirectional Bandwidth: MPI Rank 0 (GPU 0,Tile 0) will send a GPU buffer to Rank 1 (GPU 1, Tile 0). Concurently, Rank 1 will also send a buffer to Rank 0</li> </ul>"},{"location":"aurora/node-performance-overview/node-performance-overview/#gemm","title":"GEMM","text":"One Tile Full Node Scaling DGEMM 15 TFlop/s 179 TFlop/s 11.9 SGEMM 22 TFlop/s 258 TFlop/s 11.7 HGEMM 263 TFlop/s 2606 TFlop/s 9.9 BF16GEMM 273 TFlop/s 2645 TFlop/s 9.7 TF32GEMM 110 TFlop/s 1311 TFlop/s 11.9 I8GEMM 577 TFlop/s 5394 TFlop/s 9.4"},{"location":"aurora/node-performance-overview/node-performance-overview/#fft","title":"FFT","text":"One Tile Full Node Scaling Single-precision FFT C2C 1D 3 TFlop/s 34 TFlop/s 10.8 Single-precision FFT C2C 2D 3 TFlop/s 35 TFlop/s 10.4 <p>Don't hesitate to contact ALCF staff (via email or Slack) for complaints, bug reports, or praise.</p>"},{"location":"aurora/performance-tools/advisor/","title":"Advisor","text":""},{"location":"aurora/performance-tools/advisor/#references","title":"References","text":"<p>Intel Advisor User Guide</p> <p>Intel Advisor Performance Optimization Cookbook</p>"},{"location":"aurora/performance-tools/advisor/#introduction","title":"Introduction","text":"<p>Intel\u00ae Advisor is a design and analysis tool for developing performant code. The tool supports C, C++, Fortran, SYCL, OpenMP, OpenCL\u2122 code, and Python. It helps with the following:</p> <ul> <li>Performant CPU Code: Design your application for efficient threading, vectorization, and memory use.</li> <li>Efficient GPU Offload: Identify parts of the code that can be profitably offloaded. Optimize the code for compute and memory.</li> <li>Flow Graph Design and Analysis: Create, visualize, and analyze task and dependency computation for heterogeneous algorithms. </li> </ul>"},{"location":"aurora/performance-tools/advisor/#roofline-and-performance-insights-for-gpus","title":"Roofline and Performance Insights for GPUs","text":"<p>Get actionable advice for performant GPU code. In addition to the Roofline Analysis for kernels, you can:</p> <ul> <li>Get specific, actionable recommendations to design code that runs optimally on GPUs.</li> <li>See the CPU and GPU code performance side-by-side with a unified dashboard.</li> <li>Discover GPU application performance characterization, such as bandwidth sensitivity, instruction mix, and cache-line use.</li> </ul>"},{"location":"aurora/performance-tools/advisor/#offload-modeling","title":"Offload Modeling","text":"<p>Understand if your code benefits from GPU porting or how much performance acceleration your GPU code can get from moving to a next-generation GPU. You can:</p> <ul> <li>Pinpoint offload opportunities where it pays off the most.</li> <li>Project the performance on a GPU.</li> <li>Identify bottlenecks and potential performance gains.</li> <li>Get guidance for optimizing a data transfer between host and target devices.</li> </ul>"},{"location":"aurora/performance-tools/advisor/#a-quick-instruction-for-advisor-roofline-analysis-on-intel-gpus","title":"A quick instruction for Advisor roofline analysis on Intel GPUs","text":"<p>Step1: Setting the environments <pre><code>$ module load oneapi\n$ export PRJ=&lt;your_project_dir&gt;\n</code></pre></p> <p>Step 2-a: Collecting the GPU Roofline data on a single GPU (Survey analysis and Trip Count with FLOP analysis) <pre><code>$ advisor --collect=roofline --profile-gpu --project-dir=$PRJ -- &lt;your_executable&gt; &lt;your_arguments&gt;\n</code></pre></p> <p>Step 2-b: Collecting the GPU Roofline data on one of MPI ranks(Survey analysis and Trip Count with FLOP analysis) <pre><code>$ mpirun -n 1 gpu_tile_compact.sh advisor --collect=survey --profile-gpu --project-dir=$PRJ -- &lt;your_executable&gt; &lt;your_arguments&gt; : -n 1 gpu_tile_compact.sh &lt;your_executable&gt; &lt;your_arguments&gt;\n$ mpirun -n 1 gpu_tile_compact.sh advisor --collect=tripcounts --profile-gpu --flop --no-trip-counts -- project-dir=$PRJ -- &lt;your_executable&gt; &lt;your_arguments&gt; : -n 1 gpu_tile_compact.sh &lt;your_executable&gt; &lt;your_arguments&gt;\n</code></pre></p> <p>Step 3-a: Generate a GPU Roofline report, and then review the HTML report <pre><code>$ advisor --report=all --project-dir=$PRJ --report-output=${PRJ}/roofline_all.html\n</code></pre></p> <p>Step 3-b: Download the project folder to your local system and open it with the stand-alone Advisor Client</p>"},{"location":"aurora/performance-tools/advisor/#simple-examples","title":"Simple examples","text":""},{"location":"aurora/performance-tools/advisor/#advisor-roofline-analysis-for-one-mpi-rank-out-of-12-mpi-ranks","title":"Advisor roofline analysis for one MPI rank out of 12 MPI ranks","text":"<pre><code>$ mpiexec -n 1 gpu_tile_compact.sh advisor --collect=survey --profile-gpu --project-dir=Advisor_results -- ./Comp_GeoSeries_omp_mpicxx_DP 2048 1000 : -n 11 gpu_tile_compact.sh ./Comp_GeoSeries_omp_mpicxx_DP 2048 1000\n$ mpiexec -n 1 gpu_tile_compact.sh advisor --collect=tripcounts --profile-gpu --flop --no-trip-counts --project- dir=Advisor_results -- ./Comp_GeoSeries_omp_mpicxx_DP 2048 1000 : -n 11 gpu_tile_compact.sh ./Comp_GeoSeries_omp_mpicxx_DP 2048 1000\n$ advisor --report=all --project-dir=Advisor_results --report-output=Advisor_results/roofline_all.html\n</code></pre>"},{"location":"aurora/performance-tools/performance-overview/","title":"Performance Tools Overview","text":""},{"location":"aurora/performance-tools/vtune/","title":"VTune","text":""},{"location":"aurora/performance-tools/vtune/#references","title":"References","text":"<p>Intel VTune Profiler User Guide</p> <p>Downloadable documents for VTune Profiler</p>"},{"location":"aurora/performance-tools/vtune/#introduction","title":"Introduction","text":"<p>Intel VTune Profiler can be used to find and fix performance bottleneck quickly. There are several options (i.e., GPU Hotspots analysis, GPU Offload analysis, and HPC Performance Characterization analysis) available for Intel CPUs and GPUs on Aurora.</p> <p>Intel\u00ae VTune\u2122 Profiler is a performance analysis tool for serial, multithreaded, GPU-accelerated applications. Use VTune Profiler to analyze your choice of algorithm. Identify potential benefits for your application on Intel CPUs and GPUs on Aurora.</p> <p>Use VTune Profiler to locate or determine:</p> <ul> <li>The most time-consuming (hot) functions in your application and/or on the whole system</li> <li>Sections of code that do not effectively utilize available processor time</li> <li>The best sections of code to optimize for sequential performance and for threaded performance</li> <li>Synchronization objects that affect the application performance</li> <li>Whether, where, and why your application spends time on input/output operations</li> <li>Whether your application is CPU or GPU bound and how effectively it offloads code to the GPU</li> <li>The performance impact of different synchronization methods, different numbers of threads, or different algorithms</li> <li>Thread activity and transitions</li> <li>Hardware-related issues in your code such as data sharing, cache misses, branch misprediction, and others</li> </ul>"},{"location":"aurora/performance-tools/vtune/#vtune-analysis-types-for-intel-gpus","title":"VTune analysis types for Intel GPUs","text":""},{"location":"aurora/performance-tools/vtune/#gpu-offload","title":"GPU offload","text":"<p><code>$ vtune \u2013collect gpu-offload &lt;target&gt;</code></p> <p>This analysis enables you to: * Identify how effectively your application uses SYCL, OpenMP or OpenCL kernels and explore them further with GPU Compute/Media Hotspots analysis * Analyze execution of Intel Media SDK tasks over time * Explore GPU usage and analyze a software queue for GPU engines at each moment of time</p>"},{"location":"aurora/performance-tools/vtune/#gpu-computemeadia-hotspots","title":"GPU Compute/Meadia Hotspots","text":"<p><code>$ vtune \u2013collect gpu-hotspots &lt;target&gt;</code></p> <p>Use the GPU Compute/Media Hotspots analysis to: * Explore GPU kernels with high GPU utilization, estimate the effectiveness of this utilization, identify possible reasons for stalls or low occupancy and options. * Explore the performance of your application per selected GPU metrics over time. * Analyze the hottest SYCL* standards or OpenCL\u2122 kernels for inefficient kernel code algorithms or incorrect work item configuration.</p> <p>The GPU Compute/Media Hotspots analysis is a good next step if you have already run the GPU Offload analysis and identified: * a performance-critical kernel for further analysis and optimization; * a performance-critical kernel that it is tightly connected with other kernels in the program and may slow down their performance.</p> <p>For source level in-kernal profiling, applications should to be bulit with -fdebug-info-for-profiling -gline-tables-only.</p>"},{"location":"aurora/performance-tools/vtune/#a-quick-instruction-for-vtune-analysis-on-intel-gpus","title":"A quick instruction for VTune analysis on Intel GPUs","text":"<p>GPU hotspots analysis can be used as the first step. Without special knobs, its overhead is minimal and it provides useful performance data such as kernel time, instance count, SIMD width, EU Array active/stalled/idle ratio, EU occupancy, GPU barriers/atomic, and so on. The followings are simple instructions on Intel GPUs:</p>"},{"location":"aurora/performance-tools/vtune/#running-an-application-with-vtune-on-intel-gpus","title":"Running an application with VTune on Intel GPUs","text":"<pre><code>module load oneapi\n\n### To run an application on a single stack of a GPU\n$ ZE_AFFINITY_MASK=0.0 vtune -collect gpu-hotspots -r VTune_results_1S -- ./a.out\n\n### To run an application on two spacks of a single GPU\n$ ZE_AFFINITY_MASK=0 vtune -collect gpu-hotspots -r VTune_results_2S -- ./a.out\n\n### To run an MPI application (e.g., 24 MPI ranks on two Aurora nodes)\n$ mpirun -n 24 gpu_tile_compact.sh vtune -collect gpu-hotspots -r VTune_results_MPI -- ./a.out\n\n### To run an MPI application with VTune on a select MPI (e.g., MPI rank 5 out of 24 ranks)\n$ mpirun -n 5 gpu_tile_compact.sh ./a.out : -n 1 gpu_tile_compact.sh vtune -collect gpu-hotspots -r VTune_results_MPI_5 -- ./a.out : -n 18 ./a.out \n</code></pre>"},{"location":"aurora/performance-tools/vtune/#checking-if-vtune-collection-is-successful-or-not","title":"Checking if VTune collection is successful or not","text":"<p>After successful VTune analysis, VTune provides Hottest GPU Computing Tasks with High Sampler Usage with non-zero data. The following is an example from a GeoSeries benchmark:</p> <pre><code>Hottest GPU Computing Tasks with High Sampler Usage\nComputing Task                                                                                                                         Total Time\n-------------------------------------------------------------------------------------------------------------------------------------  ----------\nComp_Geo(cl::sycl::queue, double*, double*, int, int)::{lambda(cl::sycl::handler&amp;)#1}::operator()(cl::sycl::handler&amp;) const::Comp_Geo      0.627s\nzeCommandListAppendMemoryCopy         \n</code></pre>"},{"location":"aurora/performance-tools/vtune/#after-collecting-the-performance-data-vtune-profiler-web-server-can-be-used-for-the-post-processing","title":"After collecting the performance data, VTune profiler web server can be used for the post-processing.","text":"<p>Step 1: Open a new terminal and log into Sunspot login node (no X11 forwarding required) <pre><code>$ ssh &lt;username&gt;@bastion.alcf.anl.gov\n$ ssh &lt;username&gt;@login.aurora.alcf.anl.gov\n</code></pre> Step 2: Start VTune server on a Sunspot login node after loading oneapi module and setting corresponding environmental variables for VTune <pre><code>$ module load oneapi\n$ vtune-backend --data-directory=&lt;location of precollected VTune results&gt;\n</code></pre> Step 3: Open a new terminal with SSH port forwarding enabled (need 2 hops) <pre><code>$ ssh -L 127.0.0.1:&lt;port printed by vtune-backend&gt;:127.0.0.1:&lt;port printed by vtune-backend&gt; &lt;username&gt;@bastion.alcf.anl.gov\n$ ssh -L 127.0.0.1:&lt;port printed by vtune-backend&gt;:127.0.0.1:&lt;port printed by vtune-backend&gt; &lt;username&gt;@login.aurora.alcf.anl.gov\n</code></pre></p> <p>Step 4: Check if the login nodes of Step 2 and Step 3 are the same or not. If not (e.g., aurora-uan-0009 from Step 2 and aurora-uan-0010 from Step 3), do <code>ssh</code> on the terminal for Step 3 to the login node of Step 2 <pre><code>$ ssh -L 127.0.0.1:&lt;port printed by vtune-backend&gt;:127.0.0.1:&lt;port printed by vtune-backend&gt; aurora-uan-xxxx\n</code></pre></p> <p>Step 5: Open the URL printed by VTune server in firefox web browser on your local computer. For a security warning, click \"Advanced...\" and then \"Accept the Risk and Continue\".</p> <ul> <li> <p>Accept VTune server certificate: When you open VTune GUI, your web browser will complain about VTune self-signed certificate. You either need to tell web browser to proceed or install VTune server certificate on you client machine so that browser trusts it. To install the certificate note the path to the public part of the certificate printed by VTune server in the output, copy it to you client machine and add to the trusted certificates.</p> </li> <li> <p>Set the passphrase: When you run the server for the first time the URL that it outputs contains a one-time-token. When you open such URL in the browser VTune server prompts you to set a passphrase. Other users can't access your VTune server without knowing this passphrase. The hash of the passphase will be persisted on the server. Also, a secure HTTP cookie will be stored in your browser so that you do not need to enter the passphrase each time you open VTune GUI.</p> </li> </ul> <p></p> <p></p>"},{"location":"aurora/performance-tools/vtune/#simple-examples","title":"Simple examples","text":""},{"location":"aurora/performance-tools/vtune/#vtune-gpu-offload-analysis","title":"VTune gpu-offload analysis","text":"<pre><code>$ mpiexec -n 12 gpu_tile_compact.sh vtune -collect gpu-offload -r VTune_gpu-offload ./Comp_GeoSeries_omp_mpicxx_DP 2048 1000\n</code></pre>"},{"location":"aurora/performance-tools/vtune/#vtune-gpu-hotspots-analysis","title":"VTune gpu-hotspots analysis","text":"<pre><code>$ mpiexec -n 12 gpu_tile_compact.sh vtune -collect gpu-hotspots -r VTune_gpu-hotspots ./Comp_GeoSeries_omp_mpicxx_DP 2048 1000\n</code></pre>"},{"location":"aurora/performance-tools/vtune/#vtune-instruction-count-analysis","title":"VTune instruction count analysis","text":"<pre><code>$ mpiexec -n 12 gpu_tile_compact.sh vtune -collect gpu-hotspots -knob characterization-mode=instruction-count -r VTune_inst-count ./Comp_GeoSeries_omp_mpicxx_DP 2048 1000\n</code></pre>"},{"location":"aurora/performance-tools/vtune/#vtune-source-analysis","title":"VTune source analysis","text":"<pre><code>$ mpiexec -n 12 gpu_tile_compact.sh vtune -collect gpu-hotspots -knob profiling-mode=source-analysis -r VTune_source ./Comp_GeoSeries_omp_mpicxx_DP 2048 1000\n</code></pre>"},{"location":"aurora/performance-tools/vtune/#vtune-memory-latency-analysis","title":"VTune memory latency analysis","text":"<pre><code>$ mpiexec -n 12 gpu_tile_compact.sh vtune -collect gpu-hotspots -knob profiling-mode=source-analysis -knob source-analysis=mem-latency -r VTune_mem-latency ./Comp_GeoSeries_omp_mpicxx_DP 2048 1000\n</code></pre>"},{"location":"aurora/programming-models/kokkos-aurora/","title":"Kokkos","text":""},{"location":"aurora/programming-models/kokkos-aurora/#kokkos","title":"Kokkos","text":"<p>Kokkos Core implements a programming model in C++ for writing performance portable applications targeting all major HPC platforms. For that purpose it provides abstractions for both parallel execution of code and data management. Kokkos is designed to target complex node architectures with N-level memory hierarchies and multiple types of execution resources. It currently can use Serial and OpenMP (threads) for CPU execution spaces (\"backends\") and CUDA, HIP, SYCL, and OpenMPTarget for GPU execution spaces. By convention, Kokkos only allows one GPU backend at a time.</p>"},{"location":"aurora/programming-models/kokkos-aurora/#kokkos-documentation","title":"Kokkos Documentation","text":"<ul> <li>Kokkos-core Wiki</li> <li>Kokkos github</li> </ul>"},{"location":"aurora/programming-models/kokkos-aurora/#kokkos-on-aurora","title":"Kokkos on Aurora","text":"<p>The prebuilt Kokkos on Aurora includes 3 backends: Serial and OpenMP for CPU execution and SYCL for GPU execution (with ahead-of-time (AOT) compilation, not just-in-time (JIT) compilation. To use it, run</p> <p><pre><code>module use /soft/modulefiles\nmodule load kokkos\n</code></pre> This sets the following environment variables, some of which are used by <code>cmake</code>:</p> <ul> <li><code>KOKKOS_HOME</code> - path to the <code>lib64/</code>, <code>include/</code> files installed</li> <li><code>LIBRARY_PATH</code> - prepends <code>$KOKKOS_HOME/lib64</code> to this variable used by <code>cmake</code></li> <li><code>CPATH</code> - prepends <code>$KOKKOS_HOME/include</code> to this variable used by <code>cmake</code></li> <li><code>LD_LIBRARY_PATH</code> - prepends <code>$KOKKOS_HOME/lib64</code> to this variable</li> </ul>"},{"location":"aurora/programming-models/kokkos-aurora/#building-a-kokkos-application-using-cmake","title":"Building a Kokkos Application Using <code>cmake</code>","text":"<p>Add these lines to <code>CMakeLists.txt</code>:</p> <pre><code>find_package(Kokkos REQUIRED)\ntarget_link_libraries(myTarget Kokkos::kokkoscore)\n</code></pre> <p>Here is a simple example <code>CMakeLists.txt</code> to compile an example program:</p> <pre><code>cmake_minimum_required(VERSION 3.22)\nproject(buildExample)\nfind_package(Kokkos REQUIRED)\n\nset(buildExample_SOURCE_DIR \".\")\n\nset(top_SRCS\n  ${buildExample_SOURCE_DIR}/example1.cpp)\n\nset(SOURCE_FILES ${top_SRCS})\n\nadd_executable(example1_sycl_aot ${SOURCE_FILES})\ntarget_link_libraries(example1_sycl_aot Kokkos::kokkoscore)\ntarget_include_directories(example1_sycl_aot PUBLIC ${buildExample_SOURCE_DIR})\n</code></pre> <p>Configure and build it like this:</p> <pre><code>mkdir build\ncd build\ncmake -DCMAKE_CXX_COMPILER=CC -DCMAKE_C_COMPILER=cc ..\nmake\n</code></pre>"},{"location":"aurora/programming-models/kokkos-aurora/#building-a-kokkos-application-using-make","title":"Building a Kokkos Application Using <code>make</code>","text":"<p>Here's an example <code>Makefile</code>:</p> <pre><code># KOKKOS_HOME set via:\n#   module load kokkos\n\n# You can look at the first lines of $KOKKOS_HOME/KokkosConfigCommon.cmake to\n# see the flags used in cmake configuration of the kokkos library build. The\n# default Kokkos module on Aurora was built with PrgEnv-nvhpc and includes\n# Serial, OpenMP (threads) and CUDA backends. So you should have that\n# environment module loaded and include compiler flags for cuda and openmp:\n\n# Aurora MPICH wrapper for C++ and C compilers:\nCXX=\"mpic++ -cxx=icpx\"\nCC=\"mpicc -cc=icx\"\n\nSYCL_AOT_CPPFLAGS=-fsycl -fsycl-targets=spir64_gen -fno-sycl-id-queries-fit-in-int -fsycl-dead-args-optimization -fsycl-unnamed-lambda -std=c++17\nSYCL_AOT_LDFLAGS=-Xsycl-target-backend \"-device 12.60.7\"\n\nCPPFLAGS=-g -O2 -fiopenmp -I $(KOKKOS_HOME)/include $(SYCL_AOT_CPPFLAGS) -Wno-deprecated-declarations -Wno-tautological-constant-compare -Wno-unknown-attributes\n\nLDFLAGS=$(CPPFLAGS) $(SYCL_AOT_LDFLAGS)\nLDLIBS=-L$(KOKKOS_HOME)/lib64 -lkokkoscore -lkokkossimd -lpthread\n\nSRCS=example1.cpp\nOBJS=$(subst .cpp,.o,$(SRCS))\n\nall: example1_aurora\n\nexample1_aurora: $(OBJS)\n        $(CXX) $(LDFLAGS) -o example1_aurora $(OBJS) $(LDLIBS)\n\nexample1.o: example1.cpp\n\nclean:\n        rm -f $(OBJS)\n\ndistclean: clean\n        rm -f example1_aurora\n</code></pre>"},{"location":"aurora/programming-models/kokkos-aurora/#configuring-your-own-kokkos-build-on-aurora","title":"Configuring Your Own Kokkos Build on Aurora","text":"<p>Here are recommended environment settings and configuration to build your own kokkos libraries on Aurora:</p>"},{"location":"aurora/programming-models/kokkos-aurora/#environment","title":"Environment","text":"<p>To match what was done in the centrally-built kokkos associated with the modules discussed above, use the same oneAPI version as indicated in <code>module help kokkos</code> and use the Aurora MPICH wrapper <code>mpic++ -cxx=icpx</code> as the C++ compiler (or just use <code>icpx</code> if you're not using MPI). To build Kokkos, you'll need cmake.</p>"},{"location":"aurora/programming-models/kokkos-aurora/#cmake-configuration","title":"CMake Configuration","text":"<p>This example builds three backends: OpenMP, Serial, and SYCL.</p> <pre><code>git clone git@github.com:kokkos/kokkos.git\ncd kokkos\nmkdir build\ncd build\n\ncmake\\\n    -DCMAKE_BUILD_TYPE=RelWithDebInfo\\\n    -DCMAKE_CXX_COMPILER=icpx\\\n    -DCMAKE_CXX_EXTENSIONS=OFF\\\n    -DCMAKE_CXX_STANDARD=17\\\n    -DKokkos_ENABLE_TESTS=OFF\\\n    -DKokkos_ENABLE_SERIAL=ON\\\n    -DKokkos_ENABLE_OPENMP=ON\\\n    -DKokkos_ENABLE_SYCL=ON\\\n    -DKokkos_ARCH_INTEL_PVC=ON\\\n    -DBUILD_SHARED_LIBS=OFF\\\n    -DKokkos_ENABLE_DEPRECATED_CODE_3=ON\\\n    -DKokkos_ENABLE_DEBUG=OFF\\\n    -DKokkos_ENABLE_EXAMPLES=OFF\\\n    -DCMAKE_CXX_FLAGS=\"-Wno-deprecated-declarations -Wno-tautological-constant-compare\"\\\n    -DCMAKE_EXE_LINKER_FLAGS=\"-fsycl-max-parallel-link-jobs=5\"\\\n    -DCMAKE_VERBOSE_MAKEFILE=OFF\\\n    -DCMAKE_INSTALL_PREFIX=/path/to/your/install/directory\\\n    ..\n\nmake -j16 -l16 install\n</code></pre>"},{"location":"aurora/programming-models/level-0/","title":"Level 0","text":"<p>Placeholder</p>"},{"location":"aurora/programming-models/opencl-aurora/","title":"OpenCL","text":"<p>Placeholder</p>"},{"location":"aurora/programming-models/openmp-aurora/","title":"OpenMP on Aurora","text":""},{"location":"aurora/programming-models/openmp-aurora/#overview","title":"Overview","text":"<p>The OpenMP API is an open standard for parallel programming. The specification document can be found here: https://www.openmp.org. The specification describes directives, runtime routines, and environment variables that allow an application developer to express parallelism (e.g. shared memory multiprocessing and device offloading). Many compiler vendors provide implementations of the OpenMP specification (https://www.openmp.org/specifications).</p>"},{"location":"aurora/programming-models/openmp-aurora/#setting-the-environment-to-use-openmp-on-aurora","title":"Setting the environment to use OpenMP on Aurora","text":"<p>The Intel oneAPI Programming Environment is the main environment on Aurora to maximally use the hardware. oneAPI has OpenMP support for both CPU threads and GPU devices. The oneAPI module is loaded by default in your environment:</p> <pre><code>&gt; module list\n\nCurrently Loaded Modules:\n  1) gcc/11.2.0                    3) intel_compute_runtime/release/agama-devel-551   5) libfabric/1.15.2.0   7) cray-libpals/1.3.3\n  2) mpich/51.2/icc-all-pmix-gpu   4) *oneapi/eng-compiler/2022.12.30.003*              6) cray-pals/1.3.3\n</code></pre> <p>However, additional versions of oneAPI with newer compiler versions can be found by adding additional modules to your path:</p> <p><pre><code>&gt; module use /soft/modulefiles/\n&gt; module avail oneapi\n\n-------------------------------------------------------------------------------- /soft/modulefiles ---------------------------------------------------------------------------------\\\n   oneapi/eng-compiler/2023.05.15.003    oneapi/eng-compiler/2023.10.15.002        oneapi/release/2023.10.15.001        spack-pe-oneapi/0.5-rc1 (D)\n   oneapi/eng-compiler/2023.05.15.006    oneapi/eng-compiler/2023.12.15.002 (D)    oneapi/release/2023.12.15.001 (D)\n   oneapi/eng-compiler/2023.05.15.007    oneapi/release/2023.05.15.001             spack-pe-oneapi/0.4-rc1\n\n------------------------------------------------------------------------- /opt/aurora/23.073.0/modulefiles -------------------------------------------------------------------------\\\n   oneapi/eng-compiler/2022.12.30.003 (L)    oneapi/release/2022.12.30.001\n</code></pre> The additional oneAPI modules can be loaded with <code>module load oneapi/eng-compiler/2023.10.15.002</code>, for example.</p>"},{"location":"aurora/programming-models/openmp-aurora/#building-on-aurora","title":"Building on Aurora","text":"<p>The following table shows the compiler and flags</p> language MPI wrapper compiler (underlying compiler) flag to turn on OpenMP support and target CPU threads additional flags to target GPU devices Fortran mpifort (ifx) <code>-fiopenmp</code> <code>-fopenmp-targets=spir64_gen -Xopenmp-target-backend \"-device pvc\"</code> C mpicc (icx) <code>-fiopenmp</code> <code>-fopenmp-targets=spir64_gen -Xopenmp-target-backend \"-device pvc\"</code> C++ mpicxx (icpx) <code>-fiopenmp</code> <code>-fopenmp-targets=spir64_gen -Xopenmp-target-backend \"-device pvc\"</code>"},{"location":"aurora/programming-models/openmp-aurora/#running-on-aurora","title":"Running on Aurora","text":"<p>To run, you can run the produced executable or with mpiexec in a job script, and then submit the script to an Aurora queue, like:</p> <pre><code>$ cat submit.sh\n#!/bin/sh\n#PBS -l select=1\n#PBS -l walltime=0:30:00\n#PBS -q EarlyAppAccess \n#PBS -A Project\n\ncd ${PBS_O_WORKDIR}\n mpiexec -n 1 ./executable\n$ # submit to the queue:\n$ qsub -l select=1 -l walltime=0:30:00 -q EarlyAppAccess -A Project ./submit.sh\n</code></pre> <p>In the above, having the PBS options in the script and on the command line is redundant, but we put it there to show both ways of launching. This submits the script to one node in the <code>EarlyAppAccess</code> queue on Aurora, requesting 30 min. It will charge project <code>Project</code> for the time. You should replace it with your project name.</p> <p>More details for setting up the job script are in Job Scheduling and Execution section.</p>"},{"location":"aurora/programming-models/openmp-aurora/#example","title":"Example","text":"<pre><code>$ cat hello.cpp\n#include &lt;stdio.h&gt;\n#include &lt;omp.h&gt;\n\nint main( int argv, char** argc ) {\n\n  printf( \"Number of devices: %d\\n\", omp_get_num_devices() );\n\n  #pragma omp target\n  {\n    if( !omp_is_initial_device() )\n      printf( \"Hello world from accelerator.\\n\" );\n    else\n      printf( \"Hello world from host.\\n\" );\n  }\n  return 0;\n}\n\n$ cat hello.F90\nprogram  main\n  use omp_lib\n  implicit none\n  integer flag\n\n  write(*,*) \"Number of devices:\", omp_get_num_devices()\n\n  !$omp target map(from:flag)\n    if( .not. omp_is_initial_device() ) then\n      flag = 1\n    else\n      flag = 0\n   endif\n  !$omp end target\n\n   if( flag == 1 ) then\n      print *, \"Hello world from accelerator\"\n   else\n      print *, \"Hello world from host\"\n   endif\n\n end program main\n</code></pre>"},{"location":"aurora/programming-models/openmp-aurora/#to-compile","title":"To compile","text":"<pre><code>$ mpicxx -fiopenmp -fopenmp-targets=spir64_gen -Xopenmp-target-backend \"-device pvc\" hello.cpp -o c_test\n$ mpifort -fiopenmp -fopenmp-targets=spir64_gen -Xopenmp-target-backend \"-device pvc\" hello.F90 -o f_test\n</code></pre>"},{"location":"aurora/programming-models/openmp-aurora/#to-run","title":"To run","text":"<pre><code>$ mpiexec -n 1 ./c_test\nNumber of devices: 6\nHello world from accelerator.\n$ mpiexec -n 1 ./f_test\n Number of devices:            6\n Hello world from accelerator\n</code></pre>"},{"location":"aurora/programming-models/raja-aurora/","title":"Raja","text":"<p>Placeholder</p>"},{"location":"aurora/programming-models/sycl-aurora/","title":"SYCL","text":"<p>SYCL on Aurora</p>"},{"location":"aurora/services/gitlab-ci/","title":"Continuous Integration via Gitlab-CI For Sunspot/Aurora","text":""},{"location":"aurora/services/gitlab-ci/#changes-from-the-general-documentation-needed-for-aurorasunspot","title":"Changes from the general documentation needed for Aurora/Sunspot:","text":"<p>Instead of gitlab-ci.alcf.anl.gov use gitlab-sunspot.alcf.anl.gov.</p>"},{"location":"aurora/services/gitlab-ci/#alcf-specific-variables-for-aurora-and-sunspot","title":"ALCF Specific Variables for Aurora and Sunspot:","text":"Cluster Scheduler Variable Name Support docs Aurora PBS ANL_AURORA_SCHEDULER_PARAMETERS Aurora Getting Started Sunspot PBS ANL_SUNSPOT_SCHEDULER_PARAMETERS Sunspot Getting Started"},{"location":"aurora/services/gitlab-ci/#examples-which-have-been-modified-for-aurora-and-sunspot","title":"Examples which have been modified for Aurora and Sunspot:","text":"<p>Example: A <code>.gitlab-ci.yml</code> file for an Aurora project <pre><code># this include allows us to reference defaults in anl/ci-resource/defaults\ninclude:\n  - project: 'anl/ci-resources/defaults'\n    ref: main\n    file:\n      - '/runners.yml'\n\nvariables:\n  ANL_AURORA_SCHEDULER_PARAMETERS: \"-A ProjectName -l walltime=0:30:00  -q AuroraQueueName\"\nstages:\n  - stage1\n  - stage2\nshell_test1:\n  stage: stage1\n  extends: .aurora-shell-runner\n  script:\n    - echo \"Shell Job 1\"\nbatch_test:\n  stage: stage2\n  tags:\n  extends: .aurora-batch-runner\n  script:\n    - echo \"Job 2 start\"\n    - echo \"Job end\"\n</code></pre></p> <p>Example: Running a batch job on the Aurora HPC <pre><code># this include allows us to reference defaults in anl/ci-resource/defaults\ninclude:\n  - project: 'anl/ci-resources/defaults'\n    ref: main\n    file:\n      - '/runners.yml'\n\nvariables:\n ANL_AURORA_SCHEDULER_PARAMETERS: \"-A ProjectName -l walltime=0:30:00  -q AuroraQueueName\"\n\nbatch_test:\n  extends: .aurora-batch-runner\n  script:\n    - echo \"Job start\"\n    - echo \"Job end\"\n</code></pre></p> <p>Example: Aurora pipeline with custom stages <pre><code># this include allows us to reference defaults in anl/ci-resource/defaults\ninclude:\n  - project: 'anl/ci-resources/defaults'\n    ref: main\n    file:\n      - '/runners.yml'\n\nvariables:\n ANL_AURORA_SCHEDULER_PARAMETERS: \"-A ProjectName -l walltime=0:30:00  -q AuroraQueueName\"\n\nstages:\n  - stage1\n  - stage2\n\ntest1:\n  stage: stage1\n  extends: .aurora-shell-runner\n  script:\n    - export\n    - id\n    - hostname\n    - echo \"Running on aurora with shell runner\" \n    - echo test &gt; test.txt\ntest2:\n  stage: stage2\n  extends: .aurora-batch-runner\n  script:\n    - echo \"Job 2 start\"\n    - echo \"Job 2 end\"\n</code></pre></p> <p>Example: Gitlab job designed to only run on merge requests <pre><code># this include allows us to reference defaults in anl/ci-resource/defaults\ninclude:\n  - project: 'anl/ci-resources/defaults'\n    ref: main\n    file:\n      - '/runners.yml'\ntest1:\n  rules:\n    - if: $CI_COMMIT_TAG                    # Do not execute jobs for tag context\n      when: never\n    - if: $CI_COMMIT_BRANCH == \"master\"     # Do not run on master, since will run on the merge request just prior\n      when: never\n    - if: $CI_MERGE_REQUEST_IID             # CI_MERGE_REQUEST_IID exists, so run job\n  stage: stage1\n  extends: .aurora-batch-runner\n  script:\n    - echo \"Run test 1\"\n</code></pre></p>"},{"location":"aurora/services/jupyterhub/","title":"JupyterHub","text":"<p>Placeholder</p>"},{"location":"aurora/visualization/paraview/","title":"Paraview on Aurora","text":""},{"location":"aurora/workflows/balsam/","title":"Balsam on Aurora","text":""},{"location":"aurora/workflows/deephyper/","title":"DeepHyper","text":""},{"location":"aurora/workflows/libensemble/","title":"libEnsemble","text":"<p>libEnsemble on Aurora</p>"},{"location":"aurora/workflows/parsl/","title":"Parsl on Aurora","text":""},{"location":"aurora/workflows/smartsim/","title":"SmartSim and SmartRedis","text":"<p>SmartSim is an open source tool developed by the Hewlett Packard Enterprise (HPE) designed to facilitate the integration of traditional HPC simulation applications with machine learning workflows. There are two core components to SmartSim:</p> <ul> <li>Infrastructure library (IL)<ul> <li>Provides API to start, stop and monitor HPC applications from Python</li> <li>Interfaces with the PBSpro scheduler launch jobs</li> <li>Deploys a distributed in-memory database called the Orchestrator</li> </ul> </li> <li>SmartRedis client library<ul> <li>Provides clients that connect to the Orchestrator from Fortran, C, C++, Python code</li> <li>The client API library enables data transfer to/from database and ability to load and run JIT-traced Python and ML runtimes acting on stored data</li> </ul> </li> </ul> <p>For more resources on SmartSim, follow the links below:</p> <ul> <li>Source code</li> <li>Documentation</li> <li>Zoo of examples</li> <li>Fall 2023 ALCF User Hands-On Workshop</li> <li>NekRS-ML</li> </ul>"},{"location":"aurora/workflows/smartsim/#installation","title":"Installation","text":"<p>SmartSim on Aurora can be installed creating a virtual environment based on the ML frameworks module <pre><code>module use /soft/modulefiles\nmodule load frameworks/2023.10.15.001\npython -m venv --clear /path/to/_ssim_env --system-site-packages\nsource /path/to/_ssim_env/bin/activate\npip install --upgrade pip\n</code></pre> Note that <code>/path/to/</code> can either be a user's home or project directory.</p> <p>To use SmartSim in the future, simply load the frameworks module and source the virtual environment. <pre><code>module use /soft/modulefiles\nmodule load frameworks/2023.10.15.001\nsource /path/to/_ssim_env/bin/activate\n</code></pre></p> <p>Then install SmartSim and the CPU backend <pre><code>export SMARTSIM_REDISAI=1.2.7\ngit clone https://github.com/CrayLabs/SmartSim.git\ncd SmartSim\npip install -e .\nTORCH_PATH=$( python -c 'import torch;print(torch.utils.cmake_prefix_path)' )\nTF_PATH=$( python -c 'import tensorflow;print(\"/\".join(tensorflow.__file__.split(\"/\")[:-1]))' )\nsmart build -v --device cpu --torch_dir $TORCH_PATH --libtensorflow_dir $TF_PATH\ncd ..\n</code></pre></p> <p>Note:</p> <ul> <li>The <code>pip install -e .</code> command returns some warnings regarding the version of <code>protobuf</code> and errors about the installation of <code>cloud-volume</code>, but these can be ignored for now.</li> <li>The <code>smart build -v --device cpu</code> command builds the RedisAI backend for the CPU. This enables ML model inferencing with the SmartRedis library on the CPU hardware with models stored within the database. This feature is not enabled on the Intel Max 1550 GPU.</li> </ul> <p>Finally, install the SmartRedis library <pre><code>git clone https://github.com/CrayLabs/SmartRedis.git\ncd SmartRedis\npip install -e .\nmake lib\ncd ..\n</code></pre></p>"},{"location":"data-management/acdc/acdc-overview/","title":"ALCF Community Data Co-Op (ACDC)","text":""},{"location":"data-management/acdc/acdc-overview/#overview-of-the-alcf-community-data-co-op-acdc","title":"Overview of the ALCF Community Data Co-Op (ACDC)","text":"<p>The ALCF Community Data Co-Op (ACDC) powers data-driven research by providing a platform for data access and sharing, and value-added services for data discovery and analysis.</p> <p>A fundamental aspect of ACDC is a data fabric that allows programmatic data access, and straightforward large-scale data sharing with collaborators via Globus services.This provides a platform to build out different modalities for data access and use, such as indexing of data for discovery, data portals for interactive search and access, and accessible analysis services. ACDC will continue to be expanded to deliver ALCF users the platform to build customizable and accessible services towards the goal of supporting data-driven discoveries.</p>"},{"location":"data-management/acdc/acdc-overview/#data-access-and-sharing","title":"Data access and sharing","text":"<p>ALCF project PIs can share data on Eagle with their collaborators, making facility accounts unnecessary. With this service, the friction of data sharing amongst collaborators is eliminated \u2013 there is no need to create copies of data for sharing, or allocation and accounts just to access data. ALCF PIs can grant access to data, at read-only or read/write access levels. Non-ALCF users throughout the scientific community, who have been granted permissions, can access the data on Eagle filesystem using Globus.</p> <p>Access to the data for ALCF users and collaborators is supported via bulk transfer (Globus transfer) or direct browser-based access (HTTP/S). Direct connections to high-speed external networks permit data access at many gigabytes per second. Management of permissions and access is via a web application or command line clients, or directly via an Application Programming Interface (APIs). The interactivity permitted by the APIs distinguishes ACDC from the ALCF\u2019s previous storage systems and presents users with many possibilities for data control and distribution.</p>"},{"location":"data-management/acdc/acdc-overview/#data-portal-for-discovery-and-access","title":"Data portal for discovery and access","text":"<p>ACDC\u2019s fully supported production environment is the next step in the expansion of edge services that blur the boundaries between experimental laboratories and computing facilities. The use and prominence of such services at the ALCF are only expected to increase as they become more integral to the facility\u2019s ability to deliver data-driven scientific discoveries.</p> <p>ACDC includes several project-specific data portals that enable search and discovery of the data hosted on Eagle. The portals allow users to craft queries and filters to find specific sets of data that match their criteria and use faceted search for the discovery of data. Portals also provide the framework for other interfaces including data processing capabilities, all secured with authentication and configured authorization policy.</p> <p>The ACDC portal is a deployment of Django Globus Portal Framework customized for a variety of different projects For most of these projects, the search metadata links directly to data on Eagle, with browser-based download, preview, and rendering of files, and bulk data access.</p>"},{"location":"data-management/acdc/acdc-overview/#getting-started","title":"Getting Started","text":"<ol> <li>Request an allocation: Researchers or PIs request an allocation on Eagle, and a project allocation is created upon request acceptance.</li> <li>Manage Access: PIs can manage the space independently or assign other users to manage the space, as well as provide other users with read or read/write access for folders in the space. Globus groups and identities are used to manage such access.</li> <li>Authentication: Globus is used for authentication and identity needed to access the system. As Globus has built-in support for federated logins, users can access ACDC using their campus or institution federated username and passcode</li> </ol> <p>If you are new to the ALCF, follow these instructions on how to transfer your data to ACDC: Transferring Data to Eagle</p> <p>If you already have an ALCF account, follow these instructions on how to share your data: Sharing Data to Eagle</p>"},{"location":"data-management/acdc/eagle-data-sharing/","title":"Sharing Data on Grand/Eagle Using Globus Guest Collections","text":""},{"location":"data-management/acdc/eagle-data-sharing/#overview","title":"Overview","text":"<p>Collaborators throughout the scientific community have the ability to write data to and read scientific data from the Eagle filesystem using Globus sharing capability. This capability provides PIs with a natural and convenient storage space for collaborative work.</p> <p>Note: The project PI needs to have an active ALCF account to set up Globus guest collections on Eagle, and set permissions for collaborators to access data. If the PI does not have an account or has an inactive account, they will not be able to create a Globus guest collectively. If a PI's account goes inactive after the Globus guest collection was created and shared, the collection will become inaccessible until the PI's account is reactivated. Only the project PI has the ability to create a collection; project proxies cannot create a collection.</p> <p>Globus is a service that  provides research data management, including managed transfer and sharing. It makes it easy to move, sync, and share large amounts of data. Globus will manage file transfers, monitor performance, retry failures, recover from faults automatically when possible, and report the status of your data transfer. Globus supports GridFTP for bulk and high-performance file transfer, and direct HTTPS for download. The service allows the user to submit a data transfer request, and performs the transfer asynchronously in the background. For more information, see Globus data transfer and Globus data sharing.</p>"},{"location":"data-management/acdc/eagle-data-sharing/#logging-into-globus-with-your-alcf-login","title":"Logging into Globus with your ALCF Login","text":"<p>ALCF researchers can use their ALCF Login username and password to access Globus. Go to the Globus website and click on Log In in the upper right corner of the page.</p> <p> </p> Logging into Globus <p>Type or scroll down to \"Argonne LCF\" in the \"Use your existing organizational login\" box, and then click \"Continue\".</p> <p> </p> Select Organization Argonne LCF <p>You will be taken to a familiar-looking page for ALCF login. Enter your ALCF login username and password.</p>"},{"location":"data-management/acdc/eagle-data-sharing/#accessing-your-grandeagle-project-directory","title":"Accessing your Grand/Eagle Project Directory","text":"<p>Note: Specifically for PIs with Eagle 'Data-Only' projects (no  compute allocations), logging in through Globus is the only way to access the project directory. </p> <p>PIs with data and compute allocations will have access to the required compute-system login nodes (along with the Globus Web Interface) to  access their project directory. </p>"},{"location":"data-management/acdc/eagle-data-sharing/#creating-a-guest-collection","title":"Creating a Guest Collection","text":"<p>A project PI needs to have an 'active' ALCF account in place to create and share guest collections with collaborators. Please note that ONLY a PI has the ability to create guest collections. </p> <p>PIs with an \"Inactive/Deleted\" ALCF account, should submit a reactivation request by filling out this form: Re-activation Form</p> <p>PIs without an ALCF account should submit an ALCF account request by filling out this form: Account Request Form </p>"},{"location":"data-management/acdc/eagle-data-sharing/#navigate-to-the-collections-tab","title":"Navigate to the Collections tab","text":"<p>There are multiple ways to Navigate to the Collections tab in \"Endpoints\": 1. Click the link to get started. It will take you to the Collections tab for Eagle. OR 2. Click on 'Endpoints' located in the left panel of the Globus web app. Type \"alcf#dtn_eagle\" (for Eagle) or \"alcf#dtn_grand\" (for Grand) in the search box located at the top of the page and click the magnifying glass to search. Click on the Managed Public Endpoint \"alcf#dtn_eagle\" or \"alcf#dtn_grand\" from the search results. Click on the  Collections tab. OR 3. Click on 'File Manager' located in the left panel of the Globus web app. Search for 'alcf#dtn_Eagle' (or \"alcf#dtn_grand\") and select it in the Collection field. Select your project directory or a sub directory that you would like to share with collaborators as a Globus guest collection. Click on 'Share' on the right side of the panel, which will take you to the Collections tab.</p> <p>Note:  When you select an endpoint to transfer data to/from, you may be asked to authenticate  with  that endpoint. Follow the instructions on screen to activate the endpoint and to authenticate. You may also have to provide Authentication/Consent for the Globus web app to manage collections on this endpoint</p>"},{"location":"data-management/acdc/eagle-data-sharing/#adding-a-guest-collection","title":"Adding a Guest Collection","text":"<p>In the Collections tab, click 'Add a Guest Collection' located at the top right hand corner.</p> <ol> <li> <p>Fill out the form:</p> <ol> <li> <p>If the path to the directory is not pre-populated, click the browse button, navigate and select the directory.  Note that you can create a single guest collection and set permissions for folders within a guest collection. There is no reason to create multiple guest collections to share for a single project.</p> </li> <li> <p>Give the collection a Display Name (choose a descriptive name)</p> </li> </ol> </li> <li> <p>Click \"Create Collection\"</p> </li> </ol> <p> </p> Create New Guest Collection"},{"location":"data-management/acdc/eagle-data-sharing/#sharing-data-with-collaborators-using-guest-collections","title":"Sharing Data with Collaborators Using Guest Collections","text":"<p>Your data in the Guest Collections can be easily shared with collaborators at ALCF or elsewhere. You have full control over which files your collaborators can access, and whether they have read-only or read-write permissions. </p> <p>To share data with collaborators (that either have a Globus account or an ALCF account), click on 'Endpoints', select your newly created Guest Collection (as described in the section above), and go to the 'Permissions' tab. Click on 'Add Permissions - Share With':</p> <p> </p> Add Permissions <p>You can share with other Globus users or Globus Groups (for more information on Groups, scroll down to Groups). You can give the collaborators read, write or read+write permissions. Once the options have been selected, click 'Add Permission'.</p> <p> </p> Add Permissions - Share With <p>PI can also choose to share their data with 'Public' with anonymous read access (and anonymous write disabled).  This allows anyone that has access to the data read and/or download it without authorizing the request.</p> <p> </p> Add Permissions - Share With <p>You should then see the share and the people you have shared it with. You can repeat this process for any number of collaborators. At any time, you can terminate access to the directory by clicking the trash can next to the user.</p> <p> </p> List of people that you have shared with"},{"location":"data-management/acdc/eagle-data-sharing/#additional-information-on-globus-guest-collections","title":"Additional information on Globus Guest Collections","text":"<ol> <li> <p>ONLY a project PI can create guest collections and make them accessible to collaborators. Project proxies cannot create guest collections. </p> </li> <li> <p>You can only share directories, not individual files.</p> </li> <li> <p>Globus allows directory trees to be shared as either read or read/write. This means that any subdirectories within that tree also have the same permissions. Globus supports setting permissions at a folder level, so there is no need to create multiple guest collections for a project. You can create a guest collection at the top level and share sub-directories with the collaborators by assigning the appropriate permissions.</p> </li> <li> <p>When you create a guest collection endpoint and give access to one or more Globus users, you can select whether each person has read or read/write access. If they have write access, they can also delete files within that directory tree, so you should be careful about providing write access.</p> </li> <li> <p>Globus guest collections are created and managed by project PIs. If the PI of a project changes, the new PI will have to create a new guest collection and share them with the users. Contact ALCF Support (support@alcf.anl.gov) in such cases. Globus guest collections' ownership cannot be transferred.</p> </li> <li> <p>Guest collections are active as long as the project directory is available and the PI's ALCF account is active. If the PI's ALCF account goes inactive, the collections become inaccessible to all its collaborators. Access is restored once the PI's account is reactivated.</p> </li> <li> <p>All RW actions are performed as the PI, when using Guest Collections. If a PI does not have permissions to read or write a file or a directory, then the Globus guest collection users won't either.</p> </li> </ol>"},{"location":"data-management/acdc/eagle-data-sharing/#creating-a-group","title":"Creating a group","text":"<ol> <li>Go to Groups on the left panel</li> <li>Click on \u2018Create a new group\u2019 at the top</li> <li>Give the group a descriptive name and add Description for more information</li> <li>Make sure you select \u2018group members only\u2019 radio button</li> <li>Click on \u2018Create Group\u2019</li> </ol> Create new group"},{"location":"data-management/acdc/eagle-data-sharing/#transferring-data-from-grandeagle","title":"Transferring data from Grand/Eagle","text":"<p>Log in to Globus using your ALCF credentials. After authenticating, you will be taken to the Globus File Manager tab. In the 'Collection' box, type the name of Eagle/Grand managed endpoint (<code>alcf#dtn_eagle</code> or <code>alcf#dtn_grand</code>). Navigate to the folder/file you want to transfer. HTTPS access (read-only) is enabled so you can download files by clicking the \"Download\" button.</p> <p>Click on 'Download' to download the required file. </p> <p> </p> Download the required file <p>To transfer files to another Globus endpoint, in the \"collection\" search box in the RHS panel, enter the destination endpoint (which could also be your Globus Connect Personal endpoint). </p> <p> </p> Transferring files to another Globus endpoint <p>To transfer files, select a file or directory on one endpoint, and click the blue 'Start' button.</p> <p> </p> Transferring files <p>If the transfer is successful, you should see the following message:</p> <p> </p> A Successful Transfer <p>Click on 'View details' to display task detail information.</p> <p> </p> Transfer completed <p>You will also receive an email when the transfer is complete.</p> <p> </p> Email confirmation"},{"location":"data-management/acdc/eagle-data-sharing/#deleting-a-guest-collection","title":"Deleting a guest collection","text":"<p>To see all guest collections you have shared, go to 'Endpoints' in the left hand navigation bar, then 'Administered by You'. Select the guest collection endpoint you wish to delete, and click on 'Delete endpoint'.</p> <p> </p> Deleting a guest collection"},{"location":"data-management/acdc/eagle-data-sharing/#what-to-tell-your-collaborators","title":"What to tell your Collaborators","text":"<p>If you set up a shared endpoint and want your collaborator to download the data, this is what you need to tell them.</p> <p>First, the collaborator needs to get a Globus account. The instructions for setting up a Globus account are as described above. This account is free. They may already have Globus access via their institution.</p> <p>If the collaborator is downloading the data to his/her personal workstation, they need to install the Globus Connect client. Globus connect clients are available for Mac, Windows or Linux systems and are free.</p> <p>If you clicked on the 'notify users via email' button when you added access for this user, they should have received a message that looks like this:</p> <p> </p> Click on the 'notify users via email' button for collaborators to receive an email <p>You can, of course, also send email to your collaborators yourself, telling them you've shared a folder with them. The collaborator should click on the link, which will require logging in with their institutional or Globus login username and password. They should then be able to see the files you shared with them. External collaborator's view of the shared collection is shown below: </p> <p> </p> Collaborator transfer or sync to <p>They should click on the files they want to transfer, then 'Transfer or Sync to', enter their own endpoint name and desired path and click the 'Start' button near the bottom to start the transfer.</p> <p> </p> Chossing transfer path"},{"location":"data-management/acdc/eagle-data-sharing/#encryption-and-security","title":"Encryption and Security","text":"<p>Data can be encrypted during Globus file transfers. In some cases encryption cannot be supported by an endpoint, and Globus Online will signal an error.</p> <p>For more information, see How does Globus ensure my data is secure?</p> <p>In the Transfer Files window, click on 'More options' at the bottom of the 2 panes. Check the 'encrypt transfer' checkbox in the options.</p> <p> </p> Encrypting the transfer <p>Alternatively, you can encrypt the files before transfer using any method on your local system, then transfer them using Globus, then unencrypt on the other end.</p> <p>Note: Encryption and verification will slow down the data transfer.</p>"},{"location":"data-management/acdc/eagle-data-sharing/#faqs","title":"FAQs","text":""},{"location":"data-management/acdc/eagle-data-sharing/#general-faqs","title":"General FAQs:","text":"<p>1. What are Eagle and Grand file systems?</p> <p>They are Lustre file systems residing on an HPE ClusterStor E1000 platform equipped with 100 Petabytes of usable capacity across 8480 disk drives. Each ClusterStor platform also provides 160 Object Storage Targets and 40 Metadata Targets with an aggregate data transfer rate of 650GB/s. </p> <p>2. What is the difference between a Guest, Shared, and Mapped collection? </p> <ul> <li>Guest collections: A Guest collection is a logical construct that a PI sets up on their project directory in Globus that makes it accessible to collaborators. The PI creates a guest collection at or below their project and shares it with the Globus account holders.</li> <li>Shared collection: A guest collection becomes a shared collection when it is shared with a user/group.</li> <li>Mapped Collections: Mapped Collections are created by the endpoint administrators. In the case of Eagle/Grand, these are created by ALCF.</li> </ul> <p>3. Who can create Guest collections?</p> <p>ONLY a project PI (or project owner) can create guest collections and make them accessible to collaborators. </p> <p>Project Proxy (on the POSIX side) or Access Manager (on the Globus side) do not have the ability to create guest collections. </p> <p>4. Who is an Access Manager? </p> <p>Access Manager is someone who can act as a Proxy on behalf of the PI to manage the collection. The Access Manager has the ability to add users, remove users, grant or revoke read/write access privileges for those users on that particular guest collection. However, Access Managers DO NOT have permissions to create guest collections. </p> <p>5. What are Groups? </p> <p>Groups are constructs that enable multi-user data collaboration. A PI (and an Access Manager) can create new groups, add members to them and share a guest collection with a group of collaborators. </p> <p>Note Members of groups do not need to have ALCF accounts.</p> <p>6. What are some of the Common Errors you see and what do they mean?</p> <pre><code>- EndpointNotFound   -  Wrong endpoint name \n- PermissionDenied    -  If you do not have permissions to view or modify the collection on &lt;endpoint&gt;\n- ServiceUnavailable  -  If the service is down for maintenance\n</code></pre>"},{"location":"data-management/acdc/eagle-data-sharing/#pi-faqs","title":"PI FAQs:","text":"<p>1. How can a PI request for a data-only, Eagle storage allocation? </p> <p>A project PI can request an allocation by filling out the Director\u2019s Discretionary Allocation Request form: Request an allocation. The allocations committee reviews the proposals and provides its decision in 1-2 weeks. </p> <p>2. Does a PI need to have an ALCF account to create a Globus guest collection?</p> <p>Yes. The PI needs to have an 'active' ALCF account in place to create and share guest collections with collaborators. </p> <ul> <li>PIs with an \"Inactive/Deleted\" ALCF account, should submit a reactivation request by filling out this form: Re-activation Form</li> <li>PIs without an ALCF account should submit an ALCF account request by filling out this form: Account Request Form </li> </ul> <p>3. What endpoint should the PI use?</p> <p><code>alcf#dtn_eagle</code> (project on Eagle) or <code>alcf#dtn_eagle</code> (project on Grand)</p> <p>4. What are the actions a PI can perform?</p> <ul> <li>Create and delete guest collections, groups</li> <li>Create, delete and share the data with ALCF users and external collaborators</li> <li>Specify someone as a Proxy (Access Manager) for the guest collections</li> <li>Transfer data between the guest collection on Eagle/Grand and other Globus endpoints/collections</li> </ul> <p>5. How can a PI specify someone as a Proxy on the Globus side?</p> <p>Go to alcf#dtn_eagle (or alcf#dtn_grand) -&gt; collections -&gt; shared collection -&gt; roles -&gt; select 'Access Manager'</p> <p> </p> To specify someone as a Proxy, click on \"Roles\" <p> </p> Choose Access Manager and \"Add Role\" <p>6. What is the high-level workflow for setting up a guest collection? </p> <ol> <li>PI requests a compute or data-only allocation project.</li> <li>Once the request is approved, ALCF staff sets up a project, unixgroup, and project directory.</li> <li>A Globus sharing policy is created for the project with appropriate access controls, provided the PI has an active ALCF account.</li> <li> <p>PI creates a guest collection for the project, using the Globus mapped collection for the file system (alcf#dtn_eagle or alcf#dtn_grand).</p> <ul> <li>Note: PI needs to have an active ALCF Account and will need to log in to Globus using their ALCF credentials.</li> <li>If PI has an existing Globus account, it needs to be linked to their ALCF account.</li> </ul> </li> <li> <p>PI adds collaborators to the guest collection. Collaborators can be ALCF users and external collaborators and can be added with Read only or Read-Write permissions</p> </li> </ol> <p>7. How can project members with ALCF accounts access the project directory via Globus?</p> <p>Users that have active ALCF accounts and are part of the project in the ALCF Account and Project Management system will automatically have access to the project directory which they can access by browsing the Globus endpoint <code>alcf#dtn_eagle or alcf#dtn_grand</code>. If they want to access the files using the Globus guest collection set up by the PI, the PI will need to explicitly give them permissions to that guest collection. The purpose of Globus guest collections is to share the data with collaborators that don't have ALCF accounts or are not part of the project in the ALCF Account and Project Management system. </p> <p>8. Who has the permissions to create a guest collection?</p> <p>Only the PI has the ability to create a guest collection. The Access Manager, along with the PI, has permissions to share it with collaborators (R-only or R-W permissions as needed). </p> <p>9. I am the project PI. Why do I see a \"Permission Denied\" error when I try to CREATE a guest collection?</p> <p>If you are a PI and you see this error, it could mean that a sharing policy for the project is missing. Please contact support@alcf.anl.gov so they can set one up.</p> <p>10. If a PI added a member as a project proxy on the POSIX-side, is it safe to assume that the Proxy can create guest collections?</p> <p>No, project proxies cannot create guest collections, only the PI can.</p> <p>11. Who can create groups? A PI (and an Access Manager) can create new groups, add members to them and share a guest collection with a group of collaborators. For more information, refer to: Creating a Group</p> <p>12. What happens when the PI of a project changes? What happens to the guest collection endpoint?</p> <p>The new PI will need to create new guest collections and share it with collaborators again. Guest collections are tied to a PI's account and cannot be transferred.</p> <p>13. I noticed that I am the owner of all the files that were transferred by external collaborators using the guest collection. Why is that?</p> <p>When collaborators read files from or write files to the guest collection, they do so on behalf of the PI. All writes show up as having been carried by the PI. Additionally, if the PI does not have permission to read or write to a file or folder in the directory, then the collaborators will not have those permissions either. </p> <p>14. What happens to the guest collections when the PI's account goes inactive?</p> <p>The collections goes inactive and will remain in that state until the PI's account is re-activated. </p> <p>15. How long does it take for the endpoint to become accessible to collaborators after a PI's account is re-activated?</p> <p>Right away. The page needs to be refreshed and sometimes you may have to log out and log back in.</p>"},{"location":"data-management/acdc/eagle-data-sharing/#access-manager-faqs","title":"Access Manager FAQs:","text":"<p>1. What are the actions an Access Manager can perform?</p> <p>Access Manager should be able to see the collection under \"Shared with you\" and \"Shareable by you\" tabs. They have permissions to add and/or delete collaborators on the shared collection and restrict their R-W access as needed.</p> <p>2. Does an Access Manager need to have an ALCF account?</p> <p>Not necessary. However, if they need to manage the membership on the POSIX side (or in the ALCF Account and Project Management system), they will need an ALCF account and be a Proxy on the project.</p> <p>3. What is the difference between an ALCF project Proxy and a guest collection Access Manager?</p> <p>An ALCF Project Proxy has permissions to manage project membership on the POSIX side whereas a guest collection Access Manager has permissions to manage the project membership specific to that guest collection, created by the PI, on the Globus side.</p> <p>4. I am an 'Access Manager' on the collection. Why do I see a 'Permission Denied' error when I try to SHARE a guest collection created by the PI? </p> <p>If you are a non-PI who is able to access the guest collection but unable to share it, it means that your role on this guest collection is limited to a \"Member\". If you want the ability to share folders and sub-folders from the collections that are shared with you, please talk to the PI. They will need to set your role to an \"Access Manager\" for the collection within Globus.</p> <p>5. Can an Access Manager give external collaborators access to the collections that are shared with them?</p> <p>Yes, an Access Manager will see \"Permissions\" tab at the top of the shared collection page and can share it with collaborators and/or a group.</p> <p>6. Can an Access Manager create collections using the shared endpoint?</p> <p>No. An access manager cannot create a collection, only a PI can do that. The access manager can however share folders and sub-folders from the collections that are shared with them.</p> <p>7. Can an Access Manager leave a globus group or withdraw membership request for collaborators?</p> <p>Yes.[Go to alcf#dtn_eagle (or alcf#dtn_grand)-&gt; Groups &gt; group_name -&gt; Members -&gt; click on specific user -&gt; Role &amp; Status -&gt; Set the appropriate status]</p> <p> </p> If you get thie error, you do not have read permissions. <p>8. Can an Access Manager delete guest collections created by PI? No. Access managers cannot delete guest collections.</p>"},{"location":"data-management/acdc/eagle-data-sharing/#guest-collection-collaborators-faqs","title":"Guest Collection Collaborators FAQs:","text":"<p>1. What actions can collaborators perform?</p> <ol> <li>Collaborators can read files from a collection*</li> <li>Collaborators can write to a collection**</li> <li>Collaborators can delete files in a collection**</li> </ol> <p>*If the PI has read permissions for those files on the POSIX side and the collaborator is given read permissions in Globus for the guest collection.</p> <p>**If the PI has write permissions for those files on the POSIX side and the collaborator is given write permissions in Globus  for the guest collection.</p> <p>2. I am a collaborator. Why do I see a 'Permission Denied' error when I try to ACCESS a guest collection created by the PI?</p> <p>If you are a non-PI and you see this error while trying to access the collection, it means that you do not have read permissions to access the quest collection. Please contact the PI for required access.</p> <p> </p> If you get thie error, you do not have read permissions."},{"location":"data-management/acdc/transferring-data-to-eagle/","title":"Transferring Data to Eagle","text":""},{"location":"data-management/acdc/transferring-data-to-eagle/#evolution-of-the-petrel-data-service-to-the-alcf-community-data-co-op","title":"Evolution of the Petrel Data Service to the ALCF Community Data Co-Op","text":"<p>The Petrel data service is evolving into a more mature service called the ALCF Community Data Co-Op (ACDC) which will be launched later this year. </p> <p>In preparation for this shift, all current Petrel project PIs will need to move their project data to ALCF's Eagle filesystem by December 2021.</p> <p>For detailed instructions on how to move your data, please follow the steps outlined below. You will need to follow the order of the steps as listed.</p> <p>If you have any questions, please email: support@alcf.anl.gov.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#transferring-data-to-eagle_1","title":"Transferring data to Eagle","text":""},{"location":"data-management/acdc/transferring-data-to-eagle/#1-request-a-dd-project-on-eagle-filesystem","title":"1. Request a DD project on Eagle Filesystem","text":"<p>All Petrel project owners/PIs should request for a Director's Discretionary project on the Eagle filesystem by filling out the form at https://accounts.alcf.anl.gov/allocationRequests. Select \"New Project\" and then \"Eagle\" as the resource and fill out the rest of the form. In the \"Project and Justification Summary\" section, along with the requested details you should also state that you are migrating your data from Petrel.</p> <p>Once the submission is reviewed and approved by the allocations committee, your project will be created on the Eagle filesystem and you will be notified via email. The approval process may take 1-2 weeks. Once the project is approved, proceed to the next step.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#2-apply-for-an-alcf-account","title":"2. Apply for an ALCF account","text":"<p>A project PI will need an active ALCF account to: - Transfer their data from Petrel to the Eagle filesystem - Enable data sharing on their Eagle project (See section \"4 Share your data on Eagle using Globus Guest Collections\" for more details)</p> <p>NOTE: A collaborator does not need an ALCF account to access data that is shared on Eagle (as a Globus Guest Collection). They can sign into Globus with their institutional identity to access the data. The first time they log in, they will need to accept terms and conditions.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#to-apply-for-an-alcf-account","title":"To apply for an ALCF account:","text":"<ul> <li>Visit https://accounts.alcf.anl.gov and click on \"Request An Account\".</li> <li>When prompted for project name, please select the project on Eagle that was created for your Petrel data as a result of Step 1: Request a DD project on Eagle (you have to wait for your project to be created before you can apply for an account)</li> <li>If you don't have one, please follow the directions under \"Step 1: Request a DD project on Eagle\" (above)</li> <li>For more details on the ALCF account request process, visit the webpage Request an account</li> <li>Once your account is created and you have the cryptocard/mobile token to login to Eagle, proceed to the next step to transfer the data from Petrel to Eagle</li> </ul>"},{"location":"data-management/acdc/transferring-data-to-eagle/#3-transfer-data-from-your-source-endpoint-to-eagle-using-globus","title":"3. Transfer data from your source endpoint to Eagle using Globus","text":"<p>You can use the Globus web app to transfer data or the CLI. See Using CLI for instructions on how to use the CLI to transfer data. The following set of instructions use the Globus web app, using alcf#dtn_eagle (path /projectname) as the destination to transfer data from your source endpoint.</p> <p>NOTE: Anonymous HTTPS read access is enabled on Eagle.</p> <p>Step 1: Log into https://app.globus.org/file-manager?destination_id=05d2c76a-e867-4f67-aa57-76edeb0beda0 which opens two panes in the Globus File Manager, with ALCF Eagle on the right-hand side. - Enter the name of your source endpoint in the pane on the left-hand side.</p> <p> </p> File Manager <p> </p> Enter the name of your source endpoint <p>Step 2: You may have to log in and link your ALCF identity to your Globus account.</p> <p> </p> Log in and link your ALCF identity to your Globus account <p>Step 3: Log in using your ALCF credentials.</p> <p> </p> Use ALCF credentials <p>Step 4: If the login is successful, the folders and files on the Eagle file system will be displayed in the project/file viewer.</p> <p> </p> Eagle file system in the project/file viewer <p>Step 5: Navigate to the correct destination (project folder) on the Eagle file system. Choose the files/folders to transfer in the left-hand side panel (Petrel endpoint).</p> <p>NOTE: Before clicking the \"Start\" button, click on the Transfer and Sync Options and check the \"sync\" checkbox and then click start.</p> <p> </p> Choose the files/folders <p>Step 6: Click on the \"Activity\" tab on the left-hand side navigation panel to view the status and details of your transfers.</p> <p> </p> Activity tab <p>Step 7: Once the transfer is successful, you should see the files and folders on the Eagle file system. You will also receive an email notification from Globus letting you know that your transfer was successful.</p> <p> </p> Files and folders on the Eagle file system"},{"location":"data-management/acdc/transferring-data-to-eagle/#migrating-permissions-from-petrel-to-eagle","title":"Migrating permissions from Petrel to Eagle:","text":"<p>For PIs who had previously stored data on Petrel, and are migrating to Eagle, the following tool automates the step of copying the permissions set on Petrel to Eagle. The tool, migrate_permissions.py at https://github.com/globus/globus-tool-examples takes the source endpoint (your shared endpoint on Petrel in this case), and destination endpoint (the guest collection on Eagle that has the data), and copies over all the permissions. The tool assumes the data was coped over as is from source to destination.</p> <p>If you have any questions on the tool, or need further support, please contact support@globus.org.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#4-share-your-data-on-eagle-using-globus-guest-collections","title":"4. Share your data on Eagle using Globus Guest Collections","text":"<p>Your data on the Eagle file system can easily be shared with collaborators who are at ALCF or elsewhere. You have full control over which files your collaborator can access, and whether they have read-only or read-write permissions.</p> <p>See below for step-by-step instructions on how to share data from Eagle using Globus Guest Collections:</p> <p>https://docs.alcf.anl.gov/data-management/acdc/eagle-data-sharing/</p> <p>NOTE: Guest Collections are tied to the project PI's account so if the PI's account becomes inactive, the Guest Collections will also become inactive. Once the PI's account is reactivated, access to the Guest Collections is restored.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#using-globus-cli-tool","title":"Using Globus CLI tool:","text":"<p>To copy data and permissions from a source collection, PIs can use a Globus CLI tool that automates the step of copying the permissions set on the source collection and applies them to the collection on Eagle. This is especially useful for PIs who had previously stored data on Petrel. See https://github.com/globus/globus-tool-examples for more information.</p> <p>The tool, migrate_permissions.py in the github repo takes the source endpoint (the shared endpoint on Petrel for example), and destination endpoint (the guest collection on Eagle that has the data), and copies over all the permissions. The tool assumes the data was coped over as is from source to destination. Note that you need to have a guest collection set up for your project on Eagle to use the CLI command and tool. See this page for instructions on how to set up guest collections.</p> <p>If you have any questions on the tool, or need further support, please contact support@globus.org.</p> <p>Existing data portals: To reconfigure and update your existing data portals to point to your guest collections on Eagle, please work directly with developer/maintainer of the portal.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#faqs-for-migrating-petrel-data-to-eagle","title":"FAQs for migrating Petrel data to Eagle:","text":""},{"location":"data-management/acdc/transferring-data-to-eagle/#1-is-it-important-for-a-petrel-project-ownerpi-to-obtain-an-alcf-account","title":"1. Is it important for a Petrel project owner/PI to obtain an ALCF account?","text":"<p>Yes, the data from Petrel needs to be moved to an ALCF project directory on the Eagle filesystem. The PI will need an ALCF account to log into Globus and move the data to their Eagle project directory.</p>"},{"location":"data-management/acdc/transferring-data-to-eagle/#2-what-is-the-workflow-for-migrating-data-from-petrel-and-giving-access-to-collaborators-on-eagle","title":"2. What is the workflow for migrating data from Petrel and giving access to collaborators on Eagle?","text":"<ol> <li>PI requests an Eagle allocation project</li> <li>Allocations Committee reviews and approves requests</li> <li>Once the allocation request is approved, the project is created and associated with a UNIX group and project directory on Eagle</li> <li>PI requests an ALCF account (if they don't have one)</li> <li>Once the ALCF account is created and tied to the project on Eagle, the PI moves the data from Petrel to Eagle using Globus</li> <li>PI creates guest collections for the project on Eagle, using the Globus web app using the mapped collection/endpoint for Eagle (alcf#dtn_eagle). Note that:</li> <li>The PI needs to have an active ALCF Account and will need to log in to Globus using their ALCF credentials</li> <li>Only the PI (and not a proxy) can create guest collections</li> <li>If the PI already has a Globus account, it needs to be linked to their ALCF account</li> <li>PI adds collaborators to the guest collection. </li> <li>Added with read-only or read-write permissions.</li> <li>Note: Anonymous HTTPS write is disabled and only anonymous HTTPS read is allowed.</li> <li>Existing data portals on Petrel should be updated to point to the new guest collection on Eagle. Please work directly with developer/maintainer of the portal.</li> </ol>"},{"location":"data-management/acdc/transferring-data-to-eagle/#3-what-endpoints-should-the-pi-use-to-move-data-from-petrel","title":"3. What endpoints should the PI use to move data from Petrel?**","text":"<ul> <li>Source: Globus endpoint on Petrel for the Petrel allocation</li> <li>Destination: Globus endpoint on the Eagle filesystem and the path to the directory  (alcf#dtn_eagle, path /) OR the name of the    guest collection on Eagle"},{"location":"data-management/data-transfer/sftp-scp/","title":"SFTP and SCP","text":"<p>These standard utilities are available for local area transfers of small files; they are not recommended for use with large data transfers due to poor performance and excess resource utilization on the login nodes.</p> <p>See Globus for performing large data transfers.</p>"},{"location":"data-management/data-transfer/using-globus/","title":"Using Globus","text":"<p>Globus addresses the challenges faced by researchers in moving, sharing, and archiving large volumes of data among distributed sites. With Globus, you hand off data movement tasks to a hosted service that manages the entire operation. It monitors performance and errors, retries failed transfers, corrects problems automatically whenever possible, and reports status to keep you informed and keep you focused on your research. </p> <p>Command line and Web-based interfaces are available. The command line interface, which requires only ssh to be installed on the client, is the method of choice for script-based workflows. Globus also provides a REST-style transfer API for advanced-use cases that require scripting and automation.</p>"},{"location":"data-management/data-transfer/using-globus/#getting-started","title":"Getting Started","text":"<p>Basic documentation for getting started with Globus can be found at the following URL: https://docs.globus.org/how-to/</p>"},{"location":"data-management/data-transfer/using-globus/#data-transfer-node","title":"Data Transfer Node","text":"<p>Several data transfer nodes (DTNs) for <code>/home</code>, Grand, Eagle, and HPSS  are available to ALCF users, allowing users to perform wide and local area data transfers. Access to the DTNs is provided via the following Globus endpoints.</p>"},{"location":"data-management/data-transfer/using-globus/#alcf-globus-endpoints","title":"ALCF Globus Endpoints","text":"<p>The Globus endpoint and the path to use depends on where your data resides. If your data is on:</p> <ul> <li><code>/home</code> which is where your home directory resides: <code>alcf#dtn_home</code> for accessing <code>/home</code> (i.e. home directories on swift-home filesystem). Use the path <code>/&lt;username\\&gt;</code></li> <li>HPSS: <code>alcf#dtn_hpss</code></li> <li>Grand filesystem: <code>alcf#dtn_grand</code> for accessing <code>/lus/grand/projects</code> or <code>/grand</code> (i.e. project directories on Grand filesystem). Use the path <code>/grand/&lt;project name\\&gt;</code></li> <li>Eagle filesystem: <code>alcf#dtn_eagle</code> for accessing /<code>lus/eagle/projects</code> or <code>/eagle</code> (i.e project directories on Eagle filesystem). Use the path <code>/eagle/&lt;project name\\&gt;</code></li> </ul> <p>After registering, simply use the appropriate ALCF endpoint, as well as other sources or destinations. Use your ALCF credentials (your OTP generated by the CryptoCARD token with PIN or Mobilepass app) to activate the ALCF endpoint.</p> <p>Globus Connect Personal allows users to add laptops or desktops as an endpoint to Globus, in just a few steps. After you set up Globus Connect Personal, Globus can be used to transfer files to and from your computer.</p>"},{"location":"data-management/data-transfer/using-globus/#references","title":"References","text":"<p>Research Data Management with Globus (2019)</p>"},{"location":"data-management/filesystem-and-storage/data-storage/","title":"ALCF Data Storage","text":""},{"location":"data-management/filesystem-and-storage/data-storage/#disk-storage","title":"Disk Storage","text":"<p>The ALCF operates a number of file systems that are mounted globally across all of our production systems.</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#home","title":"Home","text":"<p>A Lustre file system residing on a DDN AI-400X NVMe Flash platform. It has 24 NVMe drives with 7 TB each with 123 TB of usable space. It provides 8 Object Storage Targets and 4 Metadata Targets.</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#grand","title":"Grand","text":"<p>A Lustre file system residing on an HPE ClusterStor E1000 platform equipped with 100 Petabytes of usable capacity across 8480 disk drives. This ClusterStor platform provides 160 Object Storage Targets and 40 Metadata Targets with an aggregate data transfer rate of 650GB/s.  The primary use of grand is compute campaign storage.</p> <p>Also see ALCF Data Policies and Data Transfer</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#eagle","title":"Eagle","text":"<p>A Lustre file system residing on an HPE ClusterStor E1000 platform equipped with 100 Petabytes of usable capacity across 8480 disk drives. This ClusterStor platform provides 160 Object Storage Targets and 40 Metadata Targets with an aggregate data transfer rate of 650GB/s.  The primary use of eagle is data sharing with the research community.  Eagle has community sharing community capabilities which allow PIs to share their project data with external collabortors using Globus.  Eagle can also be used for compute campaign storage.</p> <p>Also see ALCF Data Policies and Data Transfer</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#tape-storage","title":"Tape Storage","text":"<p>ALCF operates three 10,000 slot Spectralogic tape libraries.  We are currently running a combination of LTO6 and LTO8 tape technology.  The LTO tape drives have built-in hardware compression which typically achieve compression ratios between 1.25:1 and 2:1 depending on the data yielding an effective capacity of approximately 65PB.</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#hpss","title":"HPSS","text":"<p>HPSS is a data archive and retrieval system that manages large amounts of data on disk and robotic tape libraries. It provides hierarchical storage management services that allow it to migrate data between those storage platforms.</p> <p>HPSS is currently configured with a disk and tape tier. The disk tier has a capacity of 1.2PB on a DataDirect Networks SFA12K-40 storage array. By default, all archived data is initially written to the disk tier. The tape tier consists of 3 SpectraLogic T950 robotic tape libraries containing a total of 72 LTO6 tape drives with total uncompressed capacity 64 PB. Archived data is migrated to the tape tier at regular intervals, then deleted from the disk tier to create space for future archives.</p> <p>Access to HPSS is provided by various client components. Currently, ALCF supports access through two command-line clients, HSI and HTAR. In order for the client to authenticate with HPSS, the user must have a keytab file that should be located in their home directory under subdirectory .hpss. The file name will be in the format .ktb_."},{"location":"data-management/filesystem-and-storage/data-storage/#hsi-general-usage","title":"HSI General Usage","text":"<p>HSI can be invoked by simply entering hsi at your normal shell prompt. Once authenticated, you will enter the hsi command shell environment:</p> <pre><code>&gt; hsi\n[HSI]/home/username-&gt;\n</code></pre> <p>You may enter \"help\" to display a brief description of available commands.</p> <p>If archiving from or retrieving to grand or eagle you must disable the Transfer Agent. -T off</p> <p>Example archive <pre><code>[HSI]/home/username-&gt; put mydatafile                # same name on HPSS\n[HSI]/home/username-&gt; put local.file : hpss.file    # different name on HPSS\n[HSI]/home/username-&gt; put -T off mydatafile\n</code></pre></p> <p>Example retrieval <pre><code>[HSI]/home/username-&gt; get mydatafile\n[HSI]/home/username-&gt; get local.file : hpss.file\n[HSI]/home/username-&gt; get -T off mydatafile\n</code></pre></p> <p>Most of the usual shell commands will work as expected in the HSI command environment. For example, checking what files are archived:</p> <p><code>[HSI]/home/username-&gt; ls -l</code></p> <p>And organizing your archived files:</p> <pre><code>[HSI]/home/username-&gt; mkdir dataset1\n[HSI]/home/username-&gt; mv hpss.file dataset1\n[HSI]/home/username-&gt; ls dataset1\n[HSI]/home/username-&gt; rm dataset1/hpss.file\n</code></pre> <p>It may be necessary to use single or double quotes around metacharacters to avoid having the shell prematurely expand them.  For example:</p> <pre><code>[HSI]/home/username-&gt; get *.c\n</code></pre> <p>will not work, but</p> <pre><code>[HSI]/home/username-&gt; get \"*.c\"\n</code></pre> <p>will retrieve all files ending in .c.</p> <p>Following normal shell conventions, other special characters in filenames such as whitespace and semicolon also need to be escaped with \"\\\" (backslash).   For example:</p> <pre><code>       [HSI]/home/username-&gt; get \"data\\ file\\ \\;\\ version\\ 1\"\n</code></pre> <p>retrieves the file named \"data file ; version 1\".</p> <p>HSI can also be run as a command line or embedded in a script as follows:</p> <pre><code>hsi -O log.file \"put local.file\"\n</code></pre>"},{"location":"data-management/filesystem-and-storage/data-storage/#htar-general-usage","title":"HTAR General Usage","text":"<p>HTAR is a tar-like utility that creates tar-format archive files directly in HPSS. It can be run as a command line or embedded in a script.</p> <p>Example archive <pre><code>htar -cf hpssfile.tar localfile1 localfile2 localfile3\n</code></pre></p> <p>Example retrieval</p> <pre><code>htar -xf hpssfile.tar localfile2\n</code></pre> <p>NOTE:  The current version of HTAR has a 64GB file size limit as well as a path length limit.  The recommended client is HSI.</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#globus","title":"Globus","text":"<p>In addition, HPSS is accessible through the Globus endpoint <code>alcf#dtn_hpss</code>.  As with HSI and HTAR, you must have a keytab file before using this endpoint.  For more information on using Globus, please see [Using Globus].</p>"},{"location":"data-management/filesystem-and-storage/data-storage/#keytab-file-missing","title":"Keytab File Missing","text":"<p>If you see an error like this:</p> <pre><code>*** HSI: (KEYTAB auth method) - keytab file missing or inaccessible: /\n home/username/.hpss/.ktb_username\n Error - authentication/initialization failed\n</code></pre> <p>it means that your account is not enabled to use the HPSS yet. Please contact support to have it set up.</p>"},{"location":"data-management/filesystem-and-storage/disk-quota/","title":"Disk Quota","text":""},{"location":"data-management/filesystem-and-storage/disk-quota/#overview","title":"Overview","text":"<p>Disk quotas are enabled on project directories. ALCF's HPC systems use the swift-home file system located at /lus/swift/home where quotas are also enforced. Details on the home file system are listed in file systems. Following are descriptions and examples for the home file system, as well as the grand and eagle project filesystems.</p>"},{"location":"data-management/filesystem-and-storage/disk-quota/#home-directory-quotas","title":"Home Directory Quotas","text":"<p>By default, each home directory is assigned a default of 50GB. File ownership determines disk space usage.</p> <p>To check the home directory usage, enter this command: <pre><code>&gt; myquota\nName                           Type     Filesystem        Used               Quota          Grace\n=========================================================================================================\nuserX                         User     /lus/swift         44.13G          50.00G             none\n</code></pre></p>"},{"location":"data-management/filesystem-and-storage/disk-quota/#project-directory-quotas","title":"Project Directory Quotas","text":"<p>Grand and Eagle. The amount of data stored under /lus//projects/PROJECT_NAME cannot exceed the approved project quota limit approved during the allocation period. The total data usage under the project directory is used to calculate the disk quota. <p>To check project quota usage on the file systems, enter this command: <pre><code>&gt; myprojectquotas\n\nLustre : Current Project Quota information for projects you're a member of:\n\nName                       Type        Filesystem          Used             Quota           Grace\n==============================================================================================================\nprojectZ                  Project      grand                  8k              1000T            -\nprojectX                  Project      eagle                1.87T             1000T            -\n</code></pre></p>"},{"location":"data-management/filesystem-and-storage/disk-quota/#requesting-a-new-eagle-allocation","title":"Requesting a New Eagle Allocation","text":"<p>For requesting a new project having an allocation on Eagle (with or without a compute allocation), please make a request by filling out the Director's Discretionary allocation form. Note that all new compute projects will have Grand as the default file system.</p>"},{"location":"data-management/filesystem-and-storage/disk-quota/#quota-increases","title":"Quota Increases","text":"<p>If you need a quota increase for Director's Discretionary allocations, please make a request by filling out the Director's Discretionary allocation form.</p> <p>If you need a quota increase for your INCITE/ALCC/ALCC/ESP project directory, please send an email to support@alcf.anl.gov with the machine, project name, new quota amount and reason for the increase.</p>"},{"location":"data-management/filesystem-and-storage/file-systems/","title":"ALCF File Systems","text":"<p>Our HPC systems have discrete file systems for project data: Grand and Eagle.  Grand and Eagle are 100 PB Lustre file systems mounted as /grand and /eagle respectively.  For more information on the Lustre file system, here is a document on Lustre File Striping Basics.</p> <ul> <li>Lustre File Striping Basics</li> </ul> <p>For information on the AI Testbed storage systems, refer to the AI Testbed storage page: https://argonne-lcf.github.io/ai-testbed-userdocs/common/storage/</p> <p>Our HPC systems also share a Lustre home file system, called swift-home. The home file system is mounted as /home, and should generally be used for small files and any binaries to be run on Polaris. The performance of this file system is reasonable, but using it for intensive I/O from the compute nodes is discouraged because I/O from the compute nodes uses the project data file systems, which are fast parallel systems and have far more storage space and greater I/O performance than the home directory space.</p> <p>The swift-home file system is regularly backed up to tape. The data file system is not backed up. It is the user\u2019s responsibility to ensure that copies of any critical data on the data file system have either been archived to tape or stored elsewhere.</p> Name Accessible From Type Path Production Backed-up Usage swift-home Polaris Lustre /home or /lus/swift/home Yes Yes General use Grand Polaris Lustre /grand or /lus/grand/projects Yes No Intensive job output, large files Eagle Polaris Lustre /eagle or /lus/eagle/projects Yes No Community sharing via Globus;  Intensive job output, large files Node SSD  (Compute node only) Polaris xfs /local/scratch (Polaris) Yes No Local node scratch during run"},{"location":"data-management/filesystem-and-storage/file-systems/#available-directories","title":"Available Directories","text":""},{"location":"data-management/filesystem-and-storage/file-systems/#home-directories","title":"Home Directories","text":"<ul> <li>Created when an account is created</li> <li>Located under /home</li> <li>Each home directory is subject to a quota based on user file ownership. The default quota is 50 GB</li> </ul>"},{"location":"data-management/filesystem-and-storage/file-systems/#sharing-home-directory-files-or-subdirectories-with-others","title":"Sharing Home Directory Files or Subdirectories with Others","text":"<p>If you need to share files or subdirectories (folders) under your home directory with collaborators (other ALCF users), you need to change file permissions from their defaults. You must change permissions of your top-level /home/username directory, even if you only want to share certain files/directories within it. Using normal linux file permissions control is good enough to give access to all other users, and is simple. For more fine-grained control over specific users, you need to use linux access control list (ACL) commands.</p>"},{"location":"data-management/filesystem-and-storage/file-systems/#simple-method-permission-to-all-users","title":"Simple Method: Permission to All Users","text":"<p>First, a one-time-only change to your top-level /home/username directory.</p> <pre><code>chmod o+x /home/username\n</code></pre> <p>Then you may permission individual files and/or subdirectories with read access. For example, to recursively change permissions on /home/username/subdirectoryname so that all files in that subdirectory and any subdirectory trees within it are world-readable, you would use</p> <pre><code>chmod -R o+Xr /home/username/subdirectoryname\n</code></pre>"},{"location":"data-management/filesystem-and-storage/file-systems/#refined-method-use-acl-to-give-permission-to-specific-users","title":"Refined Method: Use ACL to Give Permission to Specific Users","text":"<p>First, a one-time-only change to your top-level /home/username directory. To share files/directories with user gilgamesh, for example:</p> <pre><code>setfacl -m u:gilgamesh:X /home/username\n</code></pre> <p>Then you may permission individual files and/or subdirectories with read access. For example, to recursively change permissions on /home/username/subdirectoryname so that all files in that subdirectory and any subdirectory trees within it are readable to user gilgamesh, you would use</p> <pre><code>setfacl -R -m u:gilgamesh:rX /home/username/subdirectoryname\n</code></pre>"},{"location":"data-management/filesystem-and-storage/file-systems/#project-directories","title":"Project Directories","text":"<ul> <li>Directories on Grand or Eagle are created when an allocation (INCITE, ALCC, Discretionary, etc.) is awarded. Eagle directories can be created as stand-alone allocations. Use the allocation request form to submit requests for an allocation on Eagle. </li> <li>Directory paths:<ul> <li>Grand: /grand or /lus/grand/projects</li> <li>Eagle: /eagle or /lus/eagle/projects</li> </ul> </li> </ul> <p>These project spaces do not have user quotas but a directory quota, meaning that ALL files contained within a project directory, regardless of the username, cannot exceed the disk space allocation granted to the project. For more information on quotas, see the Disk Quota page.</p>"},{"location":"data-management/filesystem-and-storage/file-systems/#local-node-ssd","title":"Local Node SSD","text":"<p>Access to SSDs is enabled by default on Polaris.</p>"},{"location":"data-management/filesystem-and-storage/file-systems/#ssd-information","title":"SSD Information","text":"<ul> <li>Local scratch SSD storage on compute nodes for running jobs</li> <li>Completely local non-parallel filesystem</li> <li>Located at /local/scratch on Polaris computes</li> <li>Wiped between Cobalt/PBS Pro jobs</li> <li>No automatic backups provided</li> <li>Information on the current SSD drives in use is below:</li> </ul> <p>Polaris SSD Specs</p> <p>Model PM1725a drives specifications</p> Model PM1725a drives ------- Capacity 1.6 TB Sequential Read 3300 MB/s Sequential Write    3300 MB/s"},{"location":"data-management/filesystem-and-storage/hpss/","title":"Using HPSS","text":""},{"location":"data-management/filesystem-and-storage/hpss/#overview","title":"Overview","text":"<p>HPSS is a data archive and retrieval system that manages large amounts of data on disk and robotic tape libraries. It provides hierarchical storage management services that allow it to migrate data between those storage platforms.</p> <p>HPSS is currently configured with a disk and tape tier. The disk tier has a capacity of 1.2PB on a DataDirect Networks SFA12K-40 storage array. By default, all archived data is initially written to the disk tier. The tape tier consists of 3 SpectraLogic T950 robotic tape libraries containing a total of 72 LTO6 tape drives with total uncompressed capacity 64 PB. Archived data is migrated to the tape tier at regular intervals, then deleted from the disk tier to create space for future archives.</p> <p>Access to HPSS is provided by various client components. Currently, ALCF supports access through two command-line clients: HSI and HTAR.  These are installed on the login nodes of Theta, Cooley, and Polaris. In order for the client to authenticate with HPSS, the user must have a keytab file that should be located in their home directory under subdirectory <code>.hpss</code>. The file name will be in the format <code>.ktb_&lt;userid&gt;</code>.</p>"},{"location":"data-management/filesystem-and-storage/hpss/#hsi-general-usage","title":"HSI General Usage","text":"<p>HSI can be invoked by simply entering hsi at your normal shell prompt. Once authenticated, you will enter the hsi command shell environment: <pre><code>&gt; hsi\n[HSI]/home/username-&gt;\n</code></pre></p> <p>You may enter \"help\" to display a brief description of available commands.</p> <p>Example archive: <pre><code>[HSI]/home/username-&gt; put mydatafile                # same name on HPSS\n[HSI]/home/username-&gt; put local.file : hpss.file    # different name on HPSS\n</code></pre></p> <p>Example retrieval: <pre><code>[HSI]/home/username-&gt; get mydatafile\n[HSI]/home/username-&gt; get local.file : hpss.file\n</code></pre></p> <p>Most of the usual shell commands will work as expected in the HSI command environment. </p> <p>For example, checking what files are archived: <pre><code>[HSI]/home/username-&gt; ls -l\n</code></pre></p> <p>And organizing your archived files: <pre><code>[HSI]/home/username-&gt; mkdir dataset1\n[HSI]/home/username-&gt; mv hpss.file dataset1\n[HSI]/home/username-&gt; ls dataset1\n[HSI]/home/username-&gt; rm dataset1/hpss.file\n</code></pre></p> <p>It may be necessary to use single or double quotes around metacharacters to avoid having the shell prematurely expand them.  </p> <p>For example: <pre><code>[HSI]/home/username-&gt; get *.c\n\nwill not work, but\n\n[HSI]/home/username-&gt; get \"*.c\"\n\nwill retrieve all files ending in .c.  \n</code></pre></p> <p>Following normal shell conventions, other special characters in filenames such as whitespace and semicolon also need to be escaped with \"\\\" (backslash).   For example:</p> <pre><code>   [HSI]/home/username-&gt; get \"data\\ file\\ \\;\\ version\\ 1\"\n</code></pre> <p>retrieves the file named \"data file ; version 1\".</p> <p>HSI can also be run as a command line or embedded in a script as follows: <pre><code>hsi -O log.file \"put local.file\"\n</code></pre></p>"},{"location":"data-management/filesystem-and-storage/hpss/#htar-general-usage","title":"HTAR General Usage","text":"<p>HTAR is a tar-like utility that creates tar-format archive files directly in HPSS. It can be run as a command line or embedded in a script.</p> <p>Example archive: <pre><code>htar -cf hpssfile.tar localfile1 localfile2 localfile3\n</code></pre></p> <p>Example retrieval: <pre><code>htar -xf hpssfile.tar localfile2\n</code></pre></p> <p>Note: - On Theta you must first load the HSI module to make HSI and HTAR available. \"module load hsi\" - The current version of HTAR has a 64GB file size limit as well as a path length limit.  The recommended client is HSI</p>"},{"location":"data-management/filesystem-and-storage/hpss/#globus","title":"Globus","text":"<p>In addition, HPSS is accessible through the Globus endpoint alcf#dtn_hpss.  As with HSI and HTAR, you must have a keytab file before using this endpoint.  For more information on using Globus, please see Using Globus.</p>"},{"location":"data-management/filesystem-and-storage/hpss/#common-problems","title":"Common Problems","text":""},{"location":"data-management/filesystem-and-storage/hpss/#keytab-file-missing","title":"Keytab File Missing","text":"<p>If you see an error like this: <code>*** HSI: (KEYTAB auth method) - keytab file missing or inaccessible: /  home/username/.hpss/.ktb_username  Error - authentication/initialization failed</code>  it means that your account is not enabled to use the HPSS yet. Please contact support to have it set up.</p>"},{"location":"polaris/getting-started/","title":"Getting Started on Polaris","text":""},{"location":"polaris/getting-started/#logging-into-polaris","title":"Logging Into Polaris","text":"<p>To log into Polaris: <pre><code>ssh &lt;username&gt;@polaris.alcf.anl.gov\n</code></pre> Then, type in the password from your CRYPTOCard/MobilePASS+ token.</p>"},{"location":"polaris/getting-started/#hardware-overview","title":"Hardware Overview","text":"<p>An overview of the Polaris system including details on the compute node architecture is available on the Machine Overview page.</p>"},{"location":"polaris/getting-started/#compiling-applications","title":"Compiling Applications","text":"<p>Users are encouraged to read through the Compiling and Linking Overview page and corresponding pages depending on the target compiler and programming model.</p>"},{"location":"polaris/getting-started/#accessing-additional-software","title":"Accessing Additional Software","text":"<p>In addition to the Cray PE, ALCF installs software in <code>/soft</code> which can be accessed via module commands by altering your <code>$MODULEPATH</code>: <pre><code>module use /soft/modulefiles\n</code></pre> The available software can then be queried with <code>module avail</code>.</p> <p>Additionally, a suite of software packages are provided via Spack deployments, detailed on the Spack PE page.</p>"},{"location":"polaris/getting-started/#submitting-and-running-jobs","title":"Submitting and Running Jobs","text":"<p>Users are encouraged to read through the Running Jobs with PBS at the ALCF page for information on using the PBS scheduler and preparing job submission scripts. Some example job submission scripts are available on the Example Job Scripts page as well.</p>"},{"location":"polaris/getting-started/#lustre-file-striping","title":"Lustre File Striping","text":"<p>In addition to the content above, here is a document on Lustre File Striping Basics. </p> <ul> <li>Lustre File Striping Basics</li> </ul>"},{"location":"polaris/getting-started/#proxy","title":"Proxy","text":"<p>If the node you are on doesn\u2019t have outbound network connectivity, add the following to your ~/.bash_profile file to access the proxy host. </p> <pre><code># proxy settings\nexport HTTP_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport HTTPS_PROXY=\"http://proxy.alcf.anl.gov:3128\"\nexport http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport no_proxy=\"admin,polaris-adminvm-01,localhost,*.cm.polaris.alcf.anl.gov,polaris-*,*.polaris.alcf.anl.gov,*.alcf.anl.gov\"\n</code></pre>"},{"location":"polaris/getting-started/#getting-assistance","title":"Getting Assistance","text":"<p>Please direct all questions, requests, and feedback to support@alcf.anl.gov.</p>"},{"location":"polaris/known-issues/","title":"Known Issues","text":"<p>This is a collection of known issues that have been encountered on Polaris. Documentation will be updated as issues are resolved. Users are encouraged to email support@alcf.anl.gov to report issues.</p>"},{"location":"polaris/known-issues/#submitting-jobs","title":"Submitting Jobs","text":"<ol> <li> <p>For batch job submissions, if the parameters within your submission script do not meet the parameters of any of the execution queues (<code>small</code>, ..., <code>backfill-large</code>) you might not receive the \"Job submission\" error on the command line at all, and the job will never appear in history <code>qstat -xu &lt;username&gt;</code> (current bug in PBS). E.g. if a user submits a script to the <code>prod</code> routing queue requesting 10 nodes for 24 hours, exceeding \"Time Max\" of 6 hrs of the <code>small</code> execution queue (which handles jobs with 10-24 nodes), then it may behave as if the job was never submitted. </p> </li> <li> <p>Job scripts are copied to temporary locations after <code>qsub</code> and any changes to the original script while the job is queued will not be reflected in the copied script. Furthermore, <code>qalter</code> requires <code>-A &lt;allocation name&gt;</code> when changing job properties. Currently, there is a request for a <code>qalter</code>-like command to trigger a re-copy of the original script to the temporary location.</p> </li> </ol>"},{"location":"polaris/known-issues/#compiling-running-applications","title":"Compiling &amp; Running Applications","text":"<ol> <li>If your job fails to start with an <code>RPC launch</code> message like below, please forward the complete messages to support@alcf.anl.gov.</li> </ol> <pre><code>launch failed on x3104c0s1b0n0: Couldn't forward RPC launch(ab751d77-e80a-4c54-b1c2-4e881f7e8c90) to child x3104c0s31b0n0.hsn.cm.polaris.alcf.anl.gov: Resource temporarily unavailable\n</code></pre>"},{"location":"polaris/known-issues/#sshing-between-polaris-compute-nodes","title":"<code>ssh</code>'ing between Polaris Compute Nodes","text":"<ol> <li> <p>You should be able to <code>ssh</code> freely (without needing a password) between your assigned compute nodes on Polaris. If you are running into <code>ssh</code> issues check for the following causes:</p> </li> <li> <p>Your <code>/home/&lt;username&gt;</code> directory permissions should be set to <code>700</code> (<code>chmod 700 /home/&lt;username&gt;</code>)</p> </li> <li>Confirm the following files exist in your <code>.ssh</code> directory and the permissions are set to the following:          1. <code>-rw-------  (600)  authorized_keys</code>          2. <code>-rw-r--r--  (644)  config</code>          3. <code>-rw-------  (600)  id_rsa</code>          4. <code>-rw-r--r--  (644)  id_rsa.pub</code></li> <li>Copy the contents of your <code>.ssh/id_rsa.pub</code> file to <code>.ssh/authorized_keys</code></li> </ol>"},{"location":"polaris/running-jobs/","title":"Running Jobs on Polaris","text":""},{"location":"polaris/running-jobs/#queues","title":"Queues","text":"<p>There are five production queues you can target in your qsub (<code>-q &lt;queue name&gt;</code>):</p> Queue Name Node Min Node Max Time Min Time Max Notes debug 1 2 5 min 1 hr max 24 nodes in use by this queue ay any given time; Only 8 nodes are exclusive (see Note below) debug-scaling 1 10 5 min 1 hr max 1 job running/accruing/queued per-user prod 10 496 5 min 24 hrs Routing queue; See below preemptable 1 10 5 min 72 hrs Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue. Max 20 jobs running/accruing/queued per-project; see Note below demand 1 56 5 min 1 hr By request only; max 100 jobs running/accruing/queued per-project <p>Note: Please be aware that jobs in the preemptable queue can be killed at any time if jobs are submitted to the demand queue. Jobs in the demand queue take priority over jobs in the preemptable queue. This means jobs in the preemptable queue may be preempted (killed without any warning) if there are jobs in the demand queue. Unfortunately, there's always an inherent risk of jobs being killed when using the preemptable queue.  Please use the following command to view details of a queue: <code>qstat -Qf &lt;queuename&gt;</code></p> <p>To make your job re-runable add the following PBS directive: <code>#PBS -r y</code> This will ensure your job will restart once the demand job is complete. </p> <p>Note: The debug queue has 8 exclusively dedicated nodes. If there are free nodes in production, then debug jobs can take another 16 nodes for a total of 24.</p> <p><code>prod</code> is routing queue and routes your job to one of the following six execution queues:</p> Queue Name Node Min Node Max Time Min Time Max Notes small 10 24 5 min 3 hrs medium 25 99 5 min 6 hrs large 100 496 5 min 24 hrs backfill-small 10 24 5 min 3 hrs low priority, negative project balance backfill-medium 25 99 5 min 6 hrs low priority, negative project balance backfill-large 100 496 5 min 24 hrs low priority, negative project balance <ul> <li>Note 1: You cannot submit to these queues directly, you can only submit to the routing queue \"<code>prod</code>\".</li> <li>Note 2: All of these queues have a limit of ten (10) jobs running/accruing per-project</li> <li>Note 3: All of these queues have a limit of one hundred (100) jobs queued (not accruing score) per-project</li> <li>Note 4: As of January 2023, it is recommended to submit jobs with a maximum node count of 476-486 nodes given current rates of downed nodes (larger jobs may sit in the queue indefinitely).</li> </ul>"},{"location":"polaris/running-jobs/#running-mpiopenmp-applications","title":"Running MPI+OpenMP Applications","text":"<p>Once a submitted job is running calculations can be launched on the compute nodes using <code>mpiexec</code> to start an MPI application. Documentation is accessible via <code>man mpiexec</code> and some helpful options follow.</p> <ul> <li><code>-n</code> total number of MPI ranks</li> <li><code>-ppn</code> number of MPI ranks per node</li> <li><code>--cpu-bind</code> CPU binding for application</li> <li><code>--depth</code> number of cpus per rank (useful with <code>--cpu-bind</code>)</li> <li><code>--env</code> set environment variables (<code>--env OMP_NUM_THREADS=2</code>)</li> <li><code>--hostfile</code> indicate file with hostnames (the default is <code>--hostfile $PBS_NODEFILE</code>)</li> </ul> <p>A sample submission script with directives is below for a 4-node job with 8 MPI ranks on each node and 8 OpenMP threads per rank. Each hardware thread runs a single OpenMP thread since there are 64 hardware threads on the CPU (2 per core). You can download and compile <code>hello_affinity</code> from this link.</p> <pre><code>#!/bin/bash -l\n#PBS -N AFFINITY\n#PBS -l select=4:ncpus=256\n#PBS -l walltime=0:10:00\n#PBS -q debug-scaling\n#PBS -A Catalyst  # Replace with your project\n\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS=8 # Number of MPI ranks to spawn per node\nNDEPTH=8 # Number of hardware threads per rank (i.e. spacing between MPI ranks)\nNTHREADS=8 # Number of software threads per rank to launch (i.e. OMP_NUM_THREADS)\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS} THREADS_PER_RANK= ${NTHREADS}\"\n\n# Change the directory to work directory, which is the directory you submit the job.\ncd $PBS_O_WORKDIR\nmpiexec --np ${NTOTRANKS} -ppn ${NRANKS} -d ${NDEPTH} --cpu-bind depth -env OMP_NUM_THREADS=${NTHREADS} ./hello_affinity\n</code></pre>"},{"location":"polaris/running-jobs/#running-gpu-enabled-applications","title":"Running GPU-enabled Applications","text":"<p>GPU-enabled applications will similarly run on the compute nodes using the above example script. - The environment variable <code>MPICH_GPU_SUPPORT_ENABLED=1</code> needs to be set if your application requires MPI-GPU support whereby the MPI library sends and receives data directly from GPU buffers. In this case, it will be important to have the <code>craype-accel-nvidia80</code> module loaded both when compiling your application and during runtime to correctly link against a GPU Transport Layer (GTL) MPI library. Otherwise, you'll likely see <code>GPU_SUPPORT_ENABLED is requested, but GTL library is not linked</code> errors during runtime. - If running on a specific GPU or subset of GPUs is desired, then the <code>CUDA_VISIBLE_DEVICES</code> environment variable can be used. For example, if one only wanted an application to access the first two GPUs on a node, then setting <code>CUDA_VISIBLE_DEVICES=0,1</code> could be used.</p>"},{"location":"polaris/running-jobs/#binding-mpi-ranks-to-gpus","title":"Binding MPI ranks to GPUs","text":"<p>The Cray MPI on Polaris does not currently support binding MPI ranks to GPUs. For applications that need this support, this instead can be handled by use of a small helper script that will appropriately set <code>CUDA_VISIBLE_DEVICES</code> for each MPI rank. One example is available here where each MPI rank is similarly bound to a single GPU with round-robin assignment.</p> <p>A example <code>set_affinity_gpu_polaris.sh</code> script follows where GPUs are assigned round-robin to MPI ranks.</p> <p><pre><code>#!/bin/bash -l\nnum_gpus=4\n# need to assign GPUs in reverse order due to topology\n# See Polaris Device Affinity Information:\n# https://www.alcf.anl.gov/support/user-guides/polaris/hardware-overview/machine-overview/index.html\ngpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\nexport CUDA_VISIBLE_DEVICES=$gpu\necho \u201cRANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}\u201d\nexec \"$@\"\n</code></pre> This script can be placed just before the executable in the <code>mpiexec</code> command like so. <pre><code>mpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./set_affinity_gpu_polaris.sh ./hello_affinity\n</code></pre> Users with different needs, such as assigning multiple GPUs per MPI rank, can modify the above script to suit their needs.</p>"},{"location":"polaris/running-jobs/#interactive-jobs-on-compute-nodes","title":"Interactive Jobs on Compute Nodes","text":"<p>Here is how to submit an interactive job to, for example, edit/build/test an application Polaris compute nodes: <pre><code>qsub -I -l select=1 -l filesystems=home:eagle -l walltime=1:00:00 -q debug -A &lt;project_name&gt;\n</code></pre></p> <p>This command requests 1 node for a period of 1 hour in the debug queue, requiring access to the /home and eagle filesystems. After waiting in the queue for a node to become available, a shell prompt on a compute node will appear. You may then start building applications and testing gpu affinity scripts on the compute node.</p> <p>NOTE: If you want to <code>ssh</code> or <code>scp</code> to one of your assigned compute nodes you will need to make sure your <code>$HOME</code> directory and your <code>$HOME/.ssh</code> directory permissions are both set to <code>700</code>.</p>"},{"location":"polaris/running-jobs/#running-multiple-mpi-applications-on-a-node","title":"Running Multiple MPI Applications on a node","text":"<p>Multiple applications can be run simultaneously on a node by launching several <code>mpiexec</code> commands and backgrounding them. For performance, it will likely be necessary to ensure that each application runs on a distinct set of CPU resources and/or targets specific GPUs. One can provide a list of CPUs using the <code>--cpu-bind</code> option, which when combined with <code>CUDA_VISIBLE_DEVICES</code> provides a user with specifying exactly which CPU and GPU resources to run each application on. In the example below, four instances of the application are simultaneously running on a single node. In the first instance, the application is spawning MPI ranks 0-7 on CPUs 24-31 and using GPU 0. This mapping is based on output from the <code>nvidia-smi topo -m</code> command and pairs CPUs with the closest GPU.</p> <pre><code>export CUDA_VISIBLE_DEVICES=0\nmpiexec -n 8 --ppn 8 --cpu-bind list:24:25:26:27:28:29:30:31 ./hello_affinity &amp;\n\nexport CUDA_VISIBLE_DEVICES=1\nmpiexec -n 8 --ppn 8 --cpu-bind list:16:17:18:19:20:21:22:23 ./hello_affinity &amp;\n\nexport CUDA_VISIBLE_DEVICES=2\nmpiexec -n 8 --ppn 8 --cpu-bind list:8:9:10:11:12:13:14:15 ./hello_affinity &amp;\n\nexport CUDA_VISIBLE_DEVICES=3\nmpiexec -n 8 --ppn 8 --cpu-bind list:0:1:2:3:4:5:6:7 ./hello_affinity &amp;\n\nwait\n</code></pre>"},{"location":"polaris/running-jobs/#compute-node-access-to-the-internet","title":"Compute Node Access to the Internet","text":"<p>Currently, the only access the internet is via a proxy.  Here are the proxy environment variables for Polaris:</p> <pre><code>export http_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport https_proxy=\"http://proxy.alcf.anl.gov:3128\"\nexport ftp_proxy=\"http://proxy.alcf.anl.gov:3128\"\n</code></pre> <p>In the future, though we don't have a timeline on this because it depends on future features in slingshot and internal software development, we intend to have public IP addresses be a schedulable resource.  For instance, if only your head node needed public access your select statement might looks something like: <code>-l select=1:pubnet=True+63</code>.</p>"},{"location":"polaris/running-jobs/#controlling-where-your-job-runs","title":"Controlling Where Your Job Runs","text":"<p>If you wish to have your job run on specific nodes form your select like this: <code>-l select=1:vnode=&lt;node name1&gt;+1:vnode=&lt;node name2&gt;...</code> . Obviously, that gets tedious for large jobs.</p> <p>If you want to control the location of a few nodes, for example 2 out of 64, but the rest don't matter, you can do something like this: <code>-l select=1:vnode=&lt;node name1&gt;+1:vnode=&lt;node name2&gt;+62:system=foo</code></p> <p>Every node has a PBS resource called <code>tier0</code> with a rack identifier and <code>tier1</code> with a dragonfly group identifieer.  If you want all your nodes grouped in a rack, you can add the group specifier <code>-l select=8:system=foo,place=scatter:group=tier0</code>.  If you wanted everything in the same dragonfly group, replace <code>tier0</code> with <code>tier1</code>.  Note that you have to also explicitly specify the place when you use group.  If you wanted a specific rack or dragonfly group instead of any of them, you are back to the select: <code>-l select 10:tier0=x3001-g0</code>.</p>"},{"location":"polaris/running-jobs/#network-rack-and-dragonfly-group-mappings","title":"Network: Rack and Dragonfly Group Mappings","text":"<ul> <li>Racks contain (7) 6U chassis; each chassis has 2 nodes for 14 nodes per rack</li> <li>The hostnames are of the form xRRPPc0sUUb[0|1]n0 where:<ul> <li>RR is the row {30, 31, 32}</li> <li>PP is the position in the row {30 goes 1-16, 31 and 32 go 1-12}</li> <li>c is chassis and is always 0</li> <li>s stands for slot, but in this case is the RU in the rack and values are {1,7,13,19,25,31,37}</li> <li>b is BMC controller and is 0 or 1 (each node has its own BMC)</li> <li>n is node, but is always 0 since there is only one node per BMC</li> </ul> </li> <li>So, 16+12+12 = 40 racks * 14 nodes per rack = 560 nodes.</li> <li>Note that in production group 9 (the last 4 racks) will be the designated on-demand racks</li> <li>The management racks are x3000 and X3100 and are dragonfly group 10</li> <li>The TDS rack is x3200 and is dragonfly group 11</li> <li>Each compute node will have a PBS resource named <code>tier0</code> which will be equal to the values in the table below.  This allows you to group your jobs within a rack if you wish.  There is also a resource called <code>tier1</code> which will be equal to the column headings.  This allows you to group your jobs within a dragonfly group if you wish.</li> </ul> g0 g1 g2 g3 g4 g5 g6 g7 g8 g9 x3001-g0 x3005-g1 x3009-g2 x3013-g3 x3101-g4 x3105-g5 x3109-g6 x3201-g7 x3205-g8 x3209-g9 x3002-g0 x3006-g1 x3010-g2 x3014-g3 x3102-g4 x3106-g5 x3110-g6 x3202-g7 x3206-g8 x3210-g9 x3003-g0 x3007-g1 x3011-g2 x3015-g3 x3103-g4 x3107-g5 x3111-g6 x3203-g7 x3207-g8 x3211-g9 x3004-g0 x3008-g1 x3012-g2 x3016-g3 x3104-g4 x3108-g5 x3112-g6 x3204-g7 x3208-g8 x3212-g9"},{"location":"polaris/system-updates/","title":"Polaris System Updates","text":""},{"location":"polaris/system-updates/#2024-04-22","title":"2024-04-22","text":"<p>The management software on Polaris has been upgraded to HPCM 1.10 The following version changes are in place with the upgrade to HPCM 1.10:</p> <ul> <li>HPE Cray Programming Environment (CPE) 23.12</li> <li>SlingShot version 2.1.2</li> <li>NVIDIA SDK 23.9</li> <li>NVIDIA driver version 535.154.05</li> <li>CUDA 12.2</li> <li>SUSE 15 SP5</li> </ul>"},{"location":"polaris/system-updates/#releasing-jobs","title":"Releasing jobs","text":"<p>Jobs that were queued before the upgrade have been restored to the appropriate queues but are placed on user hold.  Jobs are not expected to complete successfully due to the changes made to the system and software environments resulting from the upgrade.  We recommend you review your jobs and either release the hold (<code>qrls &lt;jobid&gt;</code>) or delete it (<code>qdel &lt;jobid&gt;</code>) and resubmit as appropriate.</p> <ul> <li>Users need to rebuild for the new PE environment and major OS upgrade. Existing binaries are unlikely to run successfully.</li> <li>We have held all jobs submitted prior to the upgrade as a user hold. Users may release their existing jobs with <code>qrls</code> to run after they have rebuilt their binaries.</li> <li>PBS does cache the job execution script.  If a change to the script is required due to a path changing post rebuild, the job will have to be resubmitted.</li> <li>All application binaries should be rebuilt prior to further job submissions.</li> </ul>"},{"location":"polaris/system-updates/#re-building-user-codes","title":"Re-building user codes","text":"<p>Many user codes will need to be re-built and/or re-linked against the newer version of the programming environment (23.12) and Spack provided dependencies.</p>"},{"location":"polaris/system-updates/#changes-to-the-user-software-environment","title":"Changes to the user software environment","text":"<p>In addition to the system upgrades, several changes have been made to the user software environment which may impact user workflows.</p>"},{"location":"polaris/system-updates/#older-pe-versions-removed","title":"Older PE versions removed","text":"<p>Older versions of the Cray PE (older than 23.12) are deprecated as they are incompatible with the upgraded system stack and are no longer available for use.</p>"},{"location":"polaris/system-updates/#datascience-anaconda-module-updates","title":"Datascience Anaconda Module Updates","text":"<p>We have updated the datascience Anaconda module and built various packages and libraries with CUDA 12.4.1 to be compatible with  the new Polaris NVIDIA GPU hardware driver (CUDA 12.2) and to use the latest MPI, NCCL, cuDNN, TensorRT, etc. libraries.  PyTorch 2.3.0 and TensorFlow 2.16.1 are now available as part of this module.</p> <p>To use the new environment, type: <pre><code>module use /soft/modulefiles \nmodule load conda; conda activate\n</code></pre></p>"},{"location":"polaris/system-updates/#soft-refresh-and-default-modulepath-change","title":"<code>/soft</code> refresh and default <code>$MODULEPATH</code> change","text":"<p>Due to the new system software stack, <code>/soft</code> has been purged to allow for software to be rebuilt. In addition, <code>/soft/modulefiles</code> is no longer in the default <code>$MODULEPATH</code>. To access modules installed in <code>/soft</code>, users should run <code>module use /soft/modulefiles</code>. </p> <p>Adding <code>module use /soft/modulefiles</code> to your profile should approximate the old behavior.</p>"},{"location":"polaris/system-updates/#modules-removed","title":"Modules removed","text":"<p>The following modules have been removed:</p> <pre><code>   aocl/3.2.0                                                        hpctoolkit/2022.07.27\n   aocl/4.0                                                   (D)    hpctoolkit/2023.03.27                                                    (D)\n   ascent/develop/2024-01-08-492f9b0                                 imagemagick/imagemagick-7.1.1-11\n   boost/1.80.0                                                      kokkos/kokkos-3.6.01\n   boost/1.81.0                                               (D)    kokkos/3.7.00-cuda\n   cabana/cabana-20220723                                            kokkos/3.7.00-sycl\n   cabana/PrgEnv-gnu/8.3.3/gnu/11.2.0/cuda_cudatoolkit_11.8.0 (D)    kokkos/3.7.01-cuda\n   cmake/3.23.2                                                      kokkos/4.2.00/shared/PrgEnv-gnu/8.3.3/gnu/11.2.0/cuda_cudatoolkit_11.8.0 (D)\n   conda/2022-07-19                                                  llvm/release-15.0.0\n   conda/2022-09-08-hvd-nccl                                         llvm/release-16.0.0\n   conda/2022-09-08                                                  magma/2.6.2\n   conda/2023-01-10-unstable                                         magma/2.7.0                                                              (D)\n   conda/2023-10-04-openmpi                                          mpiwrappers/cray-mpich-oneapi                                            (D)\n   conda/2023-10-04                                           (D)    oneapi/release/2023.2.1\n   cudatoolkit-standalone/11.2.2                                     oneapi/release/2024.0\n   cudatoolkit-standalone/11.4.4                                     oneapi/upstream\n   cudatoolkit-standalone/11.6.2                                     paraview/paraview-5.11.1-mesa\n   cudatoolkit-standalone/11.7.1                                     paraview/paraview-5.11.2-EGL-test\n   cudatoolkit-standalone/12.0.0                                     paraview/paraview-5.11.2-mesa\n   e4s/22.05/mvapich2                                                paraview/paraview-5.12.0-RC1-mesa\n   e4s/22.05/PrgEnv-gnu                                       (D)    paraview/paraview-5.12.0-mesa                                            (D)\n   e4s/22.08/mvapich2                                                singularity/3.8.7\n   e4s/22.08/PrgEnv-gnu                                       (D)    tau/2.31.1\n   ffmpeg/ffmpeg-6.0                                                 tau/2.32\n   forge/23.0.4                                                      tau/2.33.1                                                               (D)\n   ginkgo/mpi/20230314/ginkgo                                        visit/visit\n   ginkgo/20230314/ginkgo                                     (D)    vmd/vmd-1.9.4a55\n   gnu-parallel/2021-09-22                                           xalt/3.0.1-202308151751\n   gsl/2.7                                                           xalt/3.0.1-202308261842                                                  (D)\n</code></pre>"},{"location":"polaris/system-updates/#modules-newly-installed","title":"Modules newly installed","text":"<p>The following modules have been newly installed:</p> <pre><code>   cabana/dev-9a1ad605/kokv/4.2.01/PrgEnv-gnu/8.5.0/gnu/12.3/cuda_cudatoolkit_12.2.91\n   cuda-PrgEnv-nvidia/12.2.91\n   cudatoolkit-standalone/12.2.2                                                      (D)\n   cudatoolkit-standalone/12.3.2\n   cudatoolkit-standalone/12.4.0\n   cudnn/9.0.0\n   forge/23.1.2\n   kokkos/4.2.01/shared/PrgEnv-gnu/8.5.0/gnu/12.3/cuda_cudatoolkit_12.2.91\n   spack-pe-base/0.6.1\n   spack-pe-gnu/0.6.1\n</code></pre> <p>Note that <code>spack-pe-base</code> and <code>spack-pe-gnu</code> are metamodules which contain further software offerings. See the Spack section below for details.</p>"},{"location":"polaris/system-updates/#spack","title":"Spack","text":"<p>We have newly installed Spack deployments in <code>/soft</code>. Spack is an HPC-oriented package manager which ALCF uses to install software for the user environment. However, no knowledge of Spack is necessary to use these software offerings. All ALCF-managed software is accessible to users via modules.</p> <p>The base suite of software tools and libraries can be accessed by loading the <code>spack-pe-base</code> module. This adds a path to <code>$MODULEPATH</code> which contains numerous modules. </p> <p>For example, to load <code>cmake</code> starting from the default environment, a user should run the following commands: <pre><code>module use /soft/modulefiles\nmodule load spack-pe-base\nmodule load cmake\n</code></pre> Other modules in <code>spack-pe-base</code> can be browsed by running <code>module avail</code> or <code>module --show-hidden avail</code>. The latter shows hidden modules which are installed as dependencies of the un-hidden modules.</p> <p>In addition to the base stack, a suite of higher-level libraries are installed in the <code>spack-pe-gnu</code> module. These are built with and are dependent on <code>PrgEnv-gnu</code>. A <code>PrgEnv-nvidia</code>-compatible stack will be available in the future.</p> <p>Note that not all software is installed through Spack; many applications and libraries are installed as standalone packages in <code>/soft</code>. Users are encouraged to browse the available modules with <code>module avail</code> to see what software is installed on the system.</p>"},{"location":"polaris/system-updates/#paraview-and-visit","title":"ParaView and Visit","text":"<p>ParaView module has been updated. For more information, see https://docs.alcf.anl.gov/polaris/visualization/paraview/ and https://docs.alcf.anl.gov/polaris/visualization/paraview-manual-launch/</p> <p>Visit module is in the process of being updated.</p>"},{"location":"polaris/system-updates/#changes-to-memory-limits-on-login-nodes","title":"Changes to Memory Limits on Login Nodes","text":"<p>Memory limits were lowered on the logins due to resource contention to 8GB of memory, and 8 cores per user.  This might result in error messages indicating abnormal process termination for user processes run on logins.</p> <p>Examples of the error messages people might see are:</p> <ul> <li><code>nvcc error   : 'cudafe++' died due to signal 9 (Kill signal)</code></li> <li><code>g++-12: fatal error: Killed signal terminated program cc1plus</code></li> </ul> <p>These errors are likely due to exhausting the per-user resources on a login node as each user is allocated 8 cores and 8GB memory. To avoid this you can either:</p> <ul> <li>Reduce the parallelism of your compile, such as using <code>-j</code> or <code>-j4</code> flags</li> <li>Request a debug node and run your compile there where you will have the full resources of the node at your disposal</li> </ul>"},{"location":"polaris/applications-and-libraries/applications/QMCPACK/","title":"QMCPACK","text":""},{"location":"polaris/applications-and-libraries/applications/QMCPACK/#qmcpack-on-polaris","title":"QMCPACK on Polaris","text":"<p>QMCPACK, is a modern high-performance open-source Quantum Monte Carlo (QMC) simulation code. Its main applications are electronic structure calculations of molecular, quasi-2D and solid-state systems. Variational Monte Carlo (VMC), diffusion Monte Carlo (DMC) and a number of other advanced QMC algorithms are implemented. Orbital space auxiliary field QMC (AFQMC) has recently been added. By directly solving the Schrodinger equation, QMC methods offer greater accuracy than methods such as density functional theory, but at a trade-off of much greater computational expense.</p> <p>Prebuilt executables are provided at <code>/soft/applications/qmcpack</code>. The directory of each installation also includes a job submission script example <code>qmcpack-polaris.job</code>. Update build recipe is provided on GitHub.</p>"},{"location":"polaris/applications-and-libraries/applications/QuantumESPRESSO/","title":"Quantum ESPRESSO","text":""},{"location":"polaris/applications-and-libraries/applications/QuantumESPRESSO/#quantum-espresso-on-polaris","title":"Quantum ESPRESSO on Polaris","text":"<p>Quantum ESPRESSO is an integrated suite of Open-Source computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials.</p> <p>Prebuilt executables are provided at <code>/soft/applications/quantum_espresso</code>. The directory of each installation also includes a job submission script example <code>job.sub</code> and a <code>README</code> file documenting the actual build recipe. We only support building QE using CMake.</p>"},{"location":"polaris/applications-and-libraries/applications/gromacs/","title":"Gromacs on Polaris","text":""},{"location":"polaris/applications-and-libraries/applications/gromacs/#what-is-gromacs","title":"What is Gromacs?","text":"<p>GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids, and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers.</p>"},{"location":"polaris/applications-and-libraries/applications/gromacs/#using-gromacs-at-alcf","title":"Using GROMACS at ALCF","text":"<p>ALCF offers assistance with building binaries and compiling instructions for GROMACS. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"polaris/applications-and-libraries/applications/gromacs/#building-gromacs","title":"Building Gromacs","text":"<ol> <li>Download latest source code: http://manual.gromacs.org/documentation/2022.1/download.html</li> <li>tar -xzf gromacs-2022.1.tar.gz</li> <li>module swap PrgEnv-nvhpc PrgEnv-gnu</li> <li>module load cudatoolkit-standalone/11.2.2</li> <li>module load gcc/10.3.0</li> <li>module load cmake</li> <li>cd gromacs-2022.1</li> <li>mkdir build</li> <li><pre><code>cmake -DCMAKE_C_COMPILER=cc -DCMAKE_CXX_COMPILER=CC \\\n      -DBUILD_SHARED_LIBS=OFF -DGMX_BUILD_OWN_FFTW=ON \\\n      -DCMAKE_INSTALL_PREFIX=/path-to/gromacs-2022.1/build \\\n      -DGMX_MPI=ON -DGMX_OPENMP=ON -DGMX_GPU=CUDA \\\n      -DCUDA_TOOLKIT_ROOT_DIR=/soft/compilers/cudatoolkit/cuda-11.2.2\n</code></pre></li> <li>make \u2013j 8</li> <li>make install</li> <li>The installed binary is <code>build/bin/gmx_mpi</code>.</li> </ol>"},{"location":"polaris/applications-and-libraries/applications/gromacs/#running-gromacs-on-polaris","title":"Running Gromacs on Polaris","text":"<p>Prebuilt Gromacs binaries can be found in the directory <code>/soft/applications/Gromacs/gromacs-2022.1</code>.</p> <p>A sample pbs script follows that will run GROMACS on two nodes, using 4 MPI ranks per node, and each rank with four OpenMP threads. The PME kernel owns one MPI rank and one GPU per node, while the nonbonded kernel uses 3 MPI ranks and 3 GPUs per node.</p> <pre><code>#!/bin/sh\n#PBS -l select=2:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q debug\n#PBS -A PROJECT\n#PBS -l filesystems=home:grand:eagle\n\ncd ${PBS_O_WORKDIR}\n\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\nmodule load cudatoolkit-standalone/11.2.2\n\nexport OMP_NUM_THREADS=4\n\nmpirun --np 8 /soft/applications/Gromacs/gromacs-2022.1/gmx_mpi \\\n      mdrun -gputasks 0123 -nb gpu -pme gpu -npme 1 -ntomp 4 \\\n      -dlb yes -resethway -pin on -v deffnm step5_1 -g test.log\n</code></pre> <p>We strongly suggest that users try combinations of different numbers of nodes, MPI ranks per node, number of GPU tasks/devices, GPU task decomposition between nonbonded and PME kernels, and OMP threads per rank to find the optimal throughput for their particular workload.</p>"},{"location":"polaris/applications-and-libraries/applications/lammps/","title":"LAMMPS","text":""},{"location":"polaris/applications-and-libraries/applications/lammps/#overview","title":"Overview","text":"<p>LAMMPS is a general-purpose molecular dynamics software package for massively parallel computers. It is written in an exceptionally clean style that makes it one of the more popular codes for users to extend and it currently has dozens of user-developed extensions.</p> <p>For details about the code and its usage, see the LAMMPS home page. This page provides information specific to running on Polaris at the ALCF.</p>"},{"location":"polaris/applications-and-libraries/applications/lammps/#using-lammps-at-alcf","title":"Using LAMMPS at ALCF","text":"<p>ALCF provides assistance with build instructions, compiling executables, submitting jobs, and providing prebuilt binaries (upon request). A collection of Makefiles and submission scripts are available in the ALCF GettingStarted repo here. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"polaris/applications-and-libraries/applications/lammps/#how-to-obtain-the-code","title":"How to Obtain the Code","text":"<p>LAMMPS is an open-source code, which can be downloaded from the LAMMPS website.</p>"},{"location":"polaris/applications-and-libraries/applications/lammps/#building-on-polaris","title":"Building on Polaris","text":"<p>After LAMMPS has been downloaded and unpacked on an ALCF filesystem, users should see a directory whose name is of the form <code>lammps-&lt;version&gt;</code>. One should then see the Makefile <code>lammps-&lt;version&gt;/src/MAKE/MACHINES/Makefile.polaris</code> in recent versions that can be used as a starting point for compilation on Polaris. Copies of Makefiles for building with the GPU/KOKKOS package using CUDA for GPU support with the GNU/NVHPC compiler are available in the ALCF GettingStarted repo here. For older versions of LAMMPS, you may need to take an existing Makefile (e.g. Makefile.mpi) for your specific version of LAMMPS and edit the top portion appropratiately to create a new Makefile.polaris.</p>"},{"location":"polaris/applications-and-libraries/applications/lammps/#kokkos-package-and-gnu-compilers","title":"KOKKOS package and GNU compilers","text":"<p>The following modules are useful for this particular build. Note, the <code>cmake</code> module is not required if using the GNU Makefile procedure to build LAMMPS. The initial <code>module restore</code> is just setting the default environment as the starting point. Users may find it useful to copy these module commands into a small helper script to assist with compiling and running LAMMPS (e.g. <code>setup_lammps_gnu.sh</code>). </p> <pre><code>module restore\nmodule load craype-accel-nvidia80\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\n\nmodule use /soft/modulefiles\nmodule load cudatoolkit-standalone\n\nmodule load spack-pe-base cmake\n</code></pre> <p>The top portion of <code>Makefile.polaris_gnu_kokkos</code> used to build LAMMPS with the KOKKOS package using GNU as the host-side compiler is shown as an example.</p> <pre><code># polaris_nvhpc_kokkos = Flags for NVIDIA A100, GNU Compiler, MPICH, CUDA\n# module load craype-accel-nvidia80\n# module swap PrgEnv-nvhpc PrgEnv-gnu\n# module use /soft/modulefiles\n# module load cudatoolkit-standalone\n# make polaris_gnu_kokkos -j 32\n\nSHELL = /bin/sh\n\n# ---------------------------------------------------------------------\n# compiler/linker settings\n# specify flags and libraries needed for your compiler\n\nKOKKOS_DEVICES = Cuda,OpenMP\nKOKKOS_ARCH = Ampere80\nKOKKOS_ABSOLUTE_PATH = $(shell cd $(KOKKOS_PATH); pwd)\nKOKKOS_CUDA_OPTIONS = \"enable_lambda,disable_malloc_async\"\nexport NVCC_WRAPPER_DEFAULT_COMPILER = nvc++\n\nCC =        $(KOKKOS_ABSOLUTE_PATH)/bin/nvcc_wrapper\nCCFLAGS =  -g -O3 -mp -DLAMMPS_MEMALIGN=64\n#CCFLAGS += -DLAMMPS_BIGBIG\nSHFLAGS =   -fPIC\nDEPFLAGS =  -M\n\nLINK =      $(CC)\nLINKFLAGS = $(CCFLAGS)\nLIB =\nSIZE =      size\n</code></pre> <p>With the appropriate LAMMPS Makefile in place an executable can be compiled as in the following example. Note, per-user limits on the login nodes will reduce the maximum parallelism for compilation. Users are encouraged to compile on a compute node in an interactive session if necessary.</p> <pre><code>cd lammps-&lt;version&gt;/src\nmake yes-KOKKOS\nmake polaris_gnu_kokkos -j 32\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/lammps/#kokkos-package-and-nvhpc-compilers","title":"KOKKOS package and NVHPC compilers","text":"<p>The following modules are useful for this particular build. Note, the <code>cmake</code> module is not required if using the GNU Makefile procedure to build LAMMPS. The initial <code>module restore</code> is just setting the default environment as the starting point. Users may find it useful to copy these module commands into a small helper script to assist with compiling and running LAMMPS (e.g. <code>setup_lammps_nvhpc.sh</code>). </p> <pre><code>module restore\nmodule load craype-accel-nvidia80\n\nmodule use /soft/modulefiles\nmodule load spack-pe-base cmake\n</code></pre> <p>The top portion of <code>Makefile.polaris_nvhpc_kokkos</code> used to build LAMMPS with the KOKKOS package using the NVHPC compilers is shown as an example.</p> <pre><code># polaris_nvhpc_kokkos = Flags for NVIDIA A100, NVIDIA Compiler, MPICH, CUDA\n# make polaris_nvhpc_kokkos -j 16\n\nSHELL = /bin/sh\n\n# ---------------------------------------------------------------------\n# compiler/linker settings\n# specify flags and libraries needed for your compiler\n\nKOKKOS_DEVICES = Cuda,OpenMP\nKOKKOS_ARCH = Ampere80\nKOKKOS_ABSOLUTE_PATH = $(shell cd $(KOKKOS_PATH); pwd)\nKOKKOS_CUDA_OPTIONS = \"enable_lambda,disable_malloc_async\"\nexport NVCC_WRAPPER_DEFAULT_COMPILER = nvc++\n\nCRAY_INC = $(shell CC --cray-print-opts=cflags)\nCRAY_LIB = $(shell CC --cray-print-opts=libs)\n\n#$(info CRAY_INC = ${CRAY_INC})\n#$(info CRAY_LIB = ${CRAY_LIB})\n\nCC =        $(KOKKOS_ABSOLUTE_PATH)/bin/nvcc_wrapper\nCCFLAGS =  -g -O3 -mp -DLAMMPS_MEMALIGN=64\nCCFLAGS += $(CRAY_INC)\nSHFLAGS =   -fPIC\nDEPFLAGS =  -M\n\nLINK =      $(CC)\nLIB =\nLIB += $(CRAY_LIB)\nSIZE =      size\n</code></pre> <p>With the appropriate LAMMPS Makefile in place an executable can be compiled as in the following example. Note, per-user limits on the login nodes will reduce the maximum parallelism for compilation. Users are encouraged to compile on a compute node in an interactive session if necessary.</p> <pre><code>cd lammps-&lt;version&gt;/src\nmake yes-KOKKOS\nmake polaris_nvhpc_kokkos -j 32\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/lammps/#running-jobs-on-polaris","title":"Running Jobs on Polaris","text":"<p>An example submission script for running a 64-node KOKKOS-enabled LAMMPS executable is below as an example. Additional information on LAMMPS application flags and options is described on the LAMMPS website.</p> <pre><code>#!/bin/sh\n#PBS -l select=64:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:15:00\n#PBS -l filesystems=home:grand:eagle\n#PBS -q prod\n#PBS -A Catalyst\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\nNNODES=`wc -l &lt; $PBS_NODEFILE`\n\n# per-node settings\nNRANKS=4\nNDEPTH=8\nNTHREADS=1\nNGPUS=4\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\n. ./setup_lammps_gnu.sh\n\nEXE=/home/knight/bin/lmp_polaris_gnu_kokkos\nEXE_ARG=\"-in in.reaxc.hns -k on g ${NGPUS} -sf kk -pk kokkos neigh half neigh/qeq full newton on \"\n\n# OMP settings mostly to quiet Kokkos messages\n\nMPI_ARG=\" -n ${NTOTRANKS} --ppn ${NRANKS} --depth=${NDEPTH} --cpu-bind depth \"\nOMP_ARG=\" --env OMP_NUM_THREADS=${NTHREADS} --env OMP_PROC_BIND=spread --env OMP_PLACES=cores \"\n\nCOMMAND=\"mpiexec ${MPI_ARG} ${OMP_ARG} ${EXE} ${EXE_ARG}\"\necho \"COMMAND= ${COMMAND}\"\n${COMMAND}\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/lammps/#gpu-package","title":"GPU package","text":"<p>The module environments above can be used to build LAMMPS with the GPU package as well. Copies of Makefiles for building with the GPU package using CUDA for GPU support with the GNU and NVHPC compilers are available in the ALCF GettingStarted repo here. </p>"},{"location":"polaris/applications-and-libraries/applications/lammps/#performance-notes","title":"Performance Notes","text":"<p>Some useful information on accelerator packages and expectations can be found on the LAMMPS website here.</p>"},{"location":"polaris/applications-and-libraries/applications/openmm/","title":"OpenMM on Polaris","text":""},{"location":"polaris/applications-and-libraries/applications/openmm/#what-is-openmm","title":"What is OpenMM?","text":"<p>OpenMM is a high-performance toolkit for molecular simulations that can be used as a stand-alone application or as a library. It provides a combination of flexibility (through custom forces and integrators), openness, and high-performance (especially on recent GPUs). </p>"},{"location":"polaris/applications-and-libraries/applications/openmm/#using-openmm-at-alcf","title":"Using OpenMM at ALCF","text":"<p>ALCF offers assistance with building binaries and compiling instructions for OpenMM. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"polaris/applications-and-libraries/applications/openmm/#building-openmm-using-conda-module","title":"Building OpenMM using Conda module","text":"<ol> <li>Update environment <pre><code>$ module load conda/2022-07-19\n</code></pre></li> <li>Install OpenMM <pre><code>$ mkdir conda\n$ conda create --prefix /path-to/conda/openmm_env\n$ conda activate /path-to/conda/openmm_env\n$ conda install -c conda-forge openmm cudatoolkit=11.4\n$ conda deactivate /path-to/conda/openmm_env\n</code></pre></li> <li> <p>Validate installation: if successful, then info on code version, platform types, CUDA initialization, and force error tolerance will be shown.</p> <pre><code>$ cd /path-to/conda/openmm_env/share/openmm/examples\n$ python -m openmm.testInstallation\n</code></pre> </li> <li> <p>Benchmark testing using PBS job script below.</p> <pre><code>$ cd /path-to/conda/openmm_env/share/openmm/examples\n$ qsub ./submit.sh\n</code></pre> </li> </ol>"},{"location":"polaris/applications-and-libraries/applications/openmm/#running-openmm-benchmark-on-polaris","title":"Running OpenMM Benchmark on Polaris","text":"<p>A sample pbs script follows that will run OpenMM benchmark on one node.</p> <pre><code>#!/bin/sh\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q debug\n#PBS -A PROJECT\n#PBS -l filesystems=home:grand:eagle\n\ncd ${PBS_O_WORKDIR}\n\nmodule load cudatoolkit-standalone/11.4.4\n\npython benchmark.py --platform=CUDA --test=pme --precision=mixed --seconds=30 --heavy-hydrogens &gt; test.output\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/openmm/#building-openmm-from-source","title":"Building OpenMM from Source","text":"<ol> <li>Update environment <pre><code>$ module load cudatoolkit-standalone/11.4.4\n$ module load cray-python/3.9.12.1\n</code></pre></li> <li>Download OpenMM <pre><code>$ git checkout https://github.com/openmm/openmm.git\n$ cd openmm ; mkdir build\n</code></pre></li> <li>Download and build doxygen <pre><code>$ git clone https://github.com/doxygen/doxygen.git\n$ cd doxygen ; cmake ; make ; make install ; cd ../\n</code></pre></li> <li>Download and install swig in OpenMM directory. <pre><code>$ tar xzf swig-4.0.2.tar.gz\n$ cd swig-4.0.2\n$ ./configure --prefix=/path-to/openmm/swig-4.0.2 ; make -j 8 ; make install\n</code></pre></li> <li>Build OpenMM <pre><code>$ cmake -DDOXYGEN_EXECUTABLE=/path-to/openmm/doxygen/bin/doxygen \\\n        -DSWIG_EXECUTABLE=/path-to/openmm/swig-4.0.2/bin/swig \\\n        -DCMAKE_INSTALL_PREFIX=/path-to/openmm/build \\\n         -DCUDA_HOME=/soft/compilers/cudatoolkit/cuda-11.4.4 \\\n         -DCUDA_INCLUDE_DIR=/soft/compilers/cudatoolkit/cuda-11.4.4/include \\\n         -DCUDA_LIB_DIR=/soft/compilers/cudatoolkit/cuda-11.4.4/lib64\n$ make -j 8\n$ make install\n</code></pre></li> <li> <p>Validate installation: if successful, then info on code version, platform types, CUDA initialization, and force error tolerance will be shown. </p> <pre><code>$ cd /path-to/openmm/examples\n$ python -m openmm.testInstallation\n</code></pre> </li> <li> <p>Benchmark testing using the PBS job script above.</p> <pre><code>$ cd /path-to/openmm/examples\n$ qsub ./submit.sh\n</code></pre> </li> </ol>"},{"location":"polaris/applications-and-libraries/applications/vasp/","title":"VASP","text":""},{"location":"polaris/applications-and-libraries/applications/vasp/#what-is-vasp","title":"What is VASP?","text":"<p>The Vienna Ab initio Simulation Package (VASP) is a software package for performing electronic structure calculations with periodic boundary conditions. It is most commonly used that to perform density functional theory (DFT) calculations in a planewave basis using the projector augemented wave (PAW) method. A more complete description of VASP can be found here: https://www.vasp.at</p>"},{"location":"polaris/applications-and-libraries/applications/vasp/#using-vasp-at-alcf","title":"Using VASP at ALCF","text":"<p>VASP is commercial software. Access to binaries compiled by ALCF can only be accessed after the user requesting access has been verified to be on the VASP license by an official VASP license distributor. </p> <p>To access the VASP binary at ALCF, please email the details listed directly below to support@alcf.anl.gov. It can take up to 5 - 10 business days to verify a VASP license.</p> <p>Information to provide: - User\u2019s full name: - User\u2019s ALCF username: - Name of organization that purchased the VASP license: - Principal investigator who is the POC for the VASP license: - VASP license number: - Version of VASP requested (VASP5, VASP6): </p>"},{"location":"polaris/applications-and-libraries/applications/vasp/#vasp-support-policy","title":"VASP support policy","text":"<p>ALCF compiles the latest release of VASP on a per request basis. We do not offer support for compiling customized versions of VASP with plugins. We are able to provide Makefiles and step-by-step build instructions to users with a verified VASP license.  Support for scientific runs that encounter performance or numerical issues should be directed to the official VASP support mailing list or the VASP user forum. Limited support is available for fatal errors encountered at run time. </p>"},{"location":"polaris/applications-and-libraries/applications/vasp/#how-to-obtain-the-code","title":"How to obtain the code","text":"<p>The VASP souce can only be obtained by an official license reseller of VASP. This is either the University of Vienna or Material Designs, Inc.</p>"},{"location":"polaris/applications-and-libraries/applications/vasp/#vasp-6xx-in-polaris-nvhpcopenaccopenmpcuda-mathcraympi","title":"VASP 6.x.x in Polaris (NVHPC+OpenACC+OpenMP+CUDA math+CrayMPI)","text":""},{"location":"polaris/applications-and-libraries/applications/vasp/#general-compilinginstalling-instructions-provided-by-vasp-support","title":"General compiling/installing instructions provided by VASP support","text":"<p>Instructions and samples of <code>makefile.include</code> could be found in the <code>vasp.at</code> wiki page.</p> <p>The follow <code>makefile.include</code> was tailored for Polaris, originally taken from here.</p> <pre><code># Precompiler options\nCPP_OPTIONS = -DHOST=\\\"LinuxNV\\\" \\\n              -DMPI -DMPI_BLOCK=8000 -Duse_collective \\\n              -DscaLAPACK \\\n              -DCACHE_SIZE=4000 \\\n              -Davoidalloc \\\n              -Dvasp6 \\\n              -Duse_bse_te \\\n              -Dtbdyn \\\n              -Dqd_emulate \\\n              -Dfock_dblbuf \\\n              -D_OPENMP \\\n              -D_OPENACC \\\n              -DUSENCCL -DUSENCCLP2P\\\n\nCPP        = nvfortran -Mpreprocess -Mfree -Mextend -E $(CPP_OPTIONS) $*$(FUFFIX)  &gt; $*$(SUFFIX)\n\nFC         = ftn -acc -gpu=cc80 -mp -target-accel=nvidia80\nFCL        = ftn -acc -gpu=cc80 -c++libs -target-accel=nvidia80\n\nFREE       = -Mfree\n\nFFLAGS     = -Mbackslash -Mlarge_arrays\n\nOFLAG      = -fast\n\nDEBUG      = -Mfree -O0 -traceback\n\n# Specify your NV HPC-SDK installation, try to set NVROOT automatically\nNVROOT     =$(shell which nvfortran | awk -F /compilers/bin/nvfortran '{ print $$1 }')\n# ...or set NVROOT manually\nNVHPC      ?= /opt/nvidia/hpc_sdk\nNVVERSION  = 23.9\nNVROOT     = $(NVHPC)/Linux_x86_64/$(NVVERSION)\n\n# Use NV HPC-SDK provided BLAS and LAPACK libraries\nLIBAOCL=/soft/libraries/aocl/3.2.0\nBLAS       = /soft/applications/vasp/aol-libs/3.2/amd-blis/lib/LP64/libblis-mt.a\nLAPACK     = /soft/applications/vasp/aol-libs/3.2/amd-libflame/lib/LP64/libflame.a\n\nBLACS      =\nSCALAPACK  =\n#SCALAPACK  = -Mscalapack\n#SCALAPACK  = ${LIBAOCL}/lib/libscalapack.a\n\nCUDA       = -cudalib=cublas,cusolver,cufft,nccl -cuda\n\nLLIBS      = $(SCALAPACK) $(LAPACK) $(BLAS) $(CUDA)\n\n# Software emulation of quadruple precsion\nQD         ?= $(NVROOT)/compilers/extras/qd\nLLIBS      += -L$(QD)/lib -lqdmod -lqd\nINCS       += -I$(QD)/include/qd\n\n#INCS       += -I/usr/include/linux\n#INCS       += -I/usr/include/c++/7/tr1\n#INCS       += -I/usr/include/c++/7\n#INCS       += -I/usr/include/x86_64-linux-gnu/c++/7\n#INCS       += -I/lus/theta-fs0/software/spack/spack-dev/opt/spack/linux-sles15-x86_64/gcc-9.3.0/gcc-10.2.0-r7v3naxd5xgzzaqxoe73jj2ytwuddamr/lib/gcc/x86_64-pc-linux-gnu/10.2.0/include/\n\n# Use the FFTs from fftw\nFFTW       = /soft/applications/vasp/aol-libs/3.2/amd-fftw\nLLIBS      += -L$(FFTW)/lib -lfftw3 -lfftw3_omp -lomp\n#INCS       += -I/soft/libraries/aocl/3.2.0/include_LP64/\nINCS       += -I$(FFTW)/include\n\nOBJECTS    = fftmpiw.o fftmpi_map.o fftw3d.o fft3dlib.o\n\n# Redefine the standard list of O1 and O2 objects\nSOURCE_O1  := pade_fit.o\nSOURCE_O2  := pead.o\n\n# For what used to be vasp.5.lib\nCPP_LIB    = $(CPP)\nFC_LIB     = nvfortran\nCC_LIB     = cc\nCFLAGS_LIB = -O $(INCS) -c++libs -cuda\nFFLAGS_LIB = -O1 -Mfixed\nFREE_LIB   = $(FREE)\n\nOBJECTS_LIB= linpack_double.o getshmem.o\n\n# For the parser library\nCXX_PARS   = nvc++ --no_warnings\n\n# Normally no need to change this\nSRCDIR     = ../../src\nBINDIR     = ../../bin\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/vasp/#setting-up-compiler-and-libraries-with-module","title":"Setting up compiler and libraries with <code>module</code>","text":"<p>The follow modules will update the include and libraries paths used by the Cray compiler wrapper <code>ftn</code> to load additional math libraries for the CPU.</p> <pre><code>module restore\nmodule load PrgEnv-nvhpc\nmodule load cray-libsci\nmodule load craype-accel-nvidia80\nexport NVROOT=${NVIDIA_PATH}\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$NVROOT/compilers/extras/qd/lib\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-blis/lib/ILP64/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-libflame/lib/ILP64/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-fftw/lib\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/vasp/#compiling-vasp","title":"Compiling VASP","text":"<p>Once the <code>modules</code> are loaded and a <code>makefile.include</code> is in the <code>vasp</code> folder, compiling all the object files and binaries is done with:</p> <pre><code>make -j1\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/vasp/#running-vasp-in-polaris","title":"Running VASP in Polaris","text":"<p>An example of a submission script could be found here <code>/soft/applications/vasp/script.sh</code> , which would looks something similar to:</p> <pre><code>#!/bin/sh\n#!/bin/sh\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:grand:eagle\n#PBS -q debug\n#PBS -A MYPROJECT\n\nmodule load PrgEnv-nvhpc\nmodule load cray-libsci\nmodule load craype-accel-nvidia80\n\nNVROOT=${NVIDIA_PATH}\n\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$NVROOT/compilers/extras/qd/lib\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-blis/lib/ILP64/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-libflame/lib/ILP64/\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/soft/applications/vasp/aol-libs/3.2/amd-fftw/lib\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS=2\nNDEPTH=4\nNTHREADS=4\nNGPUS=2\nNTOTRANKS=$(( NNODES * NRANKS ))\n# Provide full path to VASP binary\nbin=/soft/applications/vasp/vasp.6.4.3/bin/vasp_std\n\ncd $PBS_O_WORKDIR\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS} --depth ${NDEPTH} --cpu-bind depth --env OMP_NUM_THREADS=${NTHREADS} $bin\n</code></pre> <p>Submission scripts should have executable attibutes to be used with <code>qsub</code> script mode.</p> <pre><code>chmod +x script.sh\nqsub script.sh\n</code></pre>"},{"location":"polaris/applications-and-libraries/applications/vasp/#known-issues-versions-64x-in-polaris-old","title":"Known issues versions: &gt;= 6.4.x in Polaris (OLD)","text":"<ul> <li>Undefined <code>MPIX_Query_cuda_support</code> function at linking binary: This function is called in <code>src/openacc.F</code>. The  <code>MPIX_Query_cuda_support</code> is not included in<code>cray-mpich</code>. One workaround to this issue is to comment this function call. See the follow suggested changes marked by <code>!!!!!CHANGE HERE</code> in the <code>file:src/openacc.F</code></li> </ul> <pre><code>+!!!!!CHANGE HERE \n-      INTERFACE\n-        INTEGER(c_int) FUNCTION MPIX_Query_cuda_support() BIND(C, name=\"MPIX_Query_cuda_support\")\n-        END FUNCTION\n-      END INTERFACE\n\n       CHARACTER(LEN=1) :: ENVVAR_VALUE\n       INTEGER :: ENVVAR_STAT\n\n       ! This should tell us if MPI is CUDA-aware\n+!!!!!CHANGE HERE \n-       CUDA_AWARE_SUPPORT = MPIX_Query_cuda_support() == 1\n+       CUDA_AWARE_SUPPORT = .TRUE.\n       ! However, for OpenMPI some env variables can still deactivate it even though the previous\n       ! check was positive\n       CALL GET_ENVIRONMENT_VARIABLE(\"OMPI_MCA_mpi_cuda_support\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\n       CALL GET_ENVIRONMENT_VARIABLE(\"OMPI_MCA_opal_cuda_support\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\n       ! Just in case we might be non-OpenMPI, and their MPIX_Query_cuda_support behaves similarly\n       CALL GET_ENVIRONMENT_VARIABLE(\"MV2_USE_CUDA\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\n       CALL GET_ENVIRONMENT_VARIABLE(\"MPICH_RDMA_ENABLED_CUDA\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0 .AND. ENVVAR_VALUE=='0') CUDA_AWARE_SUPPORT = .FALSE.\n       CALL GET_ENVIRONMENT_VARIABLE(\"PMPI_GPU_AWARE\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n       IF (ENVVAR_STAT==0) CUDA_AWARE_SUPPORT =(ENVVAR_VALUE == '1')\n+!!!!!CHANGE HERE \n+       CALL GET_ENVIRONMENT_VARIABLE(\"MPICH_GPU_SUPPORT_ENABLED\", ENVVAR_VALUE, STATUS=ENVVAR_STAT)\n+       IF (ENVVAR_STAT==0) CUDA_AWARE_SUPPORT =(ENVVAR_VALUE == '1')\n</code></pre>"},{"location":"polaris/applications-and-libraries/libraries/cabana-polaris/","title":"Cabana","text":""},{"location":"polaris/applications-and-libraries/libraries/cabana-polaris/#cabana_1","title":"Cabana","text":"<p>Cabana is built atop Kokkos. It provides class templates useful for implementing particle codes</p>"},{"location":"polaris/applications-and-libraries/libraries/cabana-polaris/#cabana-documentation","title":"Cabana Documentation","text":"<ul> <li>Cabana Wiki</li> <li>Cabana github</li> </ul>"},{"location":"polaris/applications-and-libraries/libraries/cabana-polaris/#cabana-on-polaris","title":"Cabana on Polaris","text":"<p>Following the Polaris upgrade to HPCM 1.10, the module setup to use the prebuilt Kokkos changed.</p> <p>Built against the prebuilt Kokkos on polaris, the prebuilt Cabana includes 3 backends: Serial and OpenMP for CPU execution and CUDA for GPU execution. To use it, run</p> <pre><code>module load craype-x86-milan\nmodule load craype-accel-nvidia80\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\nmodule use /soft/modulefiles\nmodule load cuda-PrgEnv-nvidia/12.2.91\nmodule load kokkos cabana\n</code></pre> <p>Cabana is a headers-only package; there are no actual libraries installed.</p>"},{"location":"polaris/applications-and-libraries/libraries/math-libraries/","title":"Math Libraries","text":""},{"location":"polaris/applications-and-libraries/libraries/math-libraries/#blas-lapack-and-scalapack-for-cpus","title":"BLAS, LAPACK, and ScaLAPACK for CPUs","text":"<p>Some math libraries targeting CPUs are made available as part of the <code>nvhpc</code> modules and are based on the OpenBLAS project. Additional documentation is available from NVIDIA.</p> <ul> <li>BLAS &amp; LAPACK can be found in the <code>$NVIDIA_PATH/compilers/lib</code> directory.</li> <li>ScaLAPACK can be found in the <code>$NVIDIA_PATH/comm_libs</code> directory.</li> <li>GNU Scientific Library, GSL-2.7 available as <code>module help math_libs/gsl</code></li> <li>AMD Optiming CPU Libraries, AOCL v4.2 available as <code>module help math_libs/aocl</code></li> <li>Other Cray-based math libs such as Libsci, FFTW were made available by <code>module load cray-libsci</code> &amp; <code>module load cray-fftw</code></li> </ul>"},{"location":"polaris/applications-and-libraries/libraries/math-libraries/#nvidia-math-libraries-for-gpus","title":"NVIDIA Math Libraries for GPUs","text":"<p>Math libraries from NVIDIA are made available via the <code>nvhpc</code> modules. Many of the libraries users typically use can be found in the <code>$NVIDIA_PATH/math_libs</code> directory. Some examples follow and additional documentation is available from NVIDIA.</p> <ul> <li>libcublas</li> <li>libcufft</li> <li>libcurand</li> <li>libcusolver</li> <li>libcusparse</li> </ul>"},{"location":"polaris/applications-and-libraries/libraries/spack-pe/","title":"Spack PE","text":"<p>Spack is an HPC-oriented package manager which ALCF uses to install software for the user environment.</p> <p>ALCF's Spack PE is a Spack-managed software stack which provides various build tools, utilities, and libraries. It consists of a base stack (<code>spack-pe-base</code>) and PrgEnv-dependent stacks (currently <code>spack-pe-gnu</code>).</p> <p><code>spack-pe-base</code> contains commonplace software compiled for CPU with the system GCC compilers. Accordingly, the software in <code>spack-pe-base</code> can be used regardless of programming environment.</p> <p><code>spack-pe-gnu</code> is based on the E4S Project and provides performant HPC libraries built with <code>PrgEnv-gnu</code> and the <code>nvcc</code> CUDA compiler driver for GPU code. <code>spack-pe-gnu</code> is dependent on both <code>spack-pe-base</code> and <code>PrgEnv-gnu</code>. </p>"},{"location":"polaris/applications-and-libraries/libraries/spack-pe/#using-software-from-the-spack-pe","title":"Using software from the Spack PE","text":"<p>The base suite of software tools and libraries can be accessed by loading the <code>spack-pe-base</code> module. This adds a path to <code>$MODULEPATH</code> which contains numerous modules. </p> <p>For example, to load <code>cmake</code> starting from the default environment, a user should run the following commands: <pre><code>module use /soft/modulefiles\nmodule load spack-pe-base\nmodule load cmake\n</code></pre></p> <p>The <code>spack-pe-base</code> module adds paths to the user's <code>MODULEPATH</code>; individual packages are subsequently loaded through the newly available modules. The full list of available packages can be viewed by running <code>module avail</code> or <code>module --show-hidden avail</code> for a complete listing. Packages are loaded in the same way from <code>spack-pe-gnu</code>.</p>"},{"location":"polaris/applications-and-libraries/libraries/spack-pe/#inspecting-packages","title":"Inspecting packages","text":"<p>When a module in the Spack PE is loaded, several environment variables are updated to integrate the package into the user's environment. Additionally, the <code>PACKAGE_ROOT</code> variable is set to the path to the installation prefix of the package. For example, after loading <code>cmake</code> as above:</p> <pre><code>$ echo $CMAKE_ROOT\n/soft/spack/gcc/0.6.1/install/linux-sles15-x86_64/gcc-12.3.0/cmake-3.27.7-a435jtzvweeos2es6enirbxdjdqhqgdp/\n$ ls -a $CMAKE_ROOT\n.  ..  bin  doc  share  .spack\n</code></pre> <p>This variable can be used to inspect software installations and find header or library paths. Additionally, Spack packages have a <code>.spack</code> directory in the installation prefix which contains build information and logs.</p>"},{"location":"polaris/applications-and-libraries/libraries/spack-pe/#building-software-with-spack","title":"Building software with Spack","text":"<p>Spack is a powerful package manager designed for HPC. The Spack PE is installed and managed with Spack; users can also install Spack in their own home or project directory to manage their software builds. Spack has a steep learning curve, but it may benefit workflows involving frequent builds with complex dependencies.</p> <p>For users who wish to use Spack to install their own software, we provide configuration files corresponding to the Spack PE deployments. These configuration files can be found in <code>config</code> directories in <code>/soft/spack</code> within the respective Spack PE installation directories. For example, the <code>spack-pe-base/0.6.1</code> configurations are in <code>/soft/spack/gcc/0.6.1/config</code>. Not all of these settings will be useful for all builds and it is not recommended to adopt these wholesale as global settings. The recommended method is to include these settings ad hoc in a spack environment to control what information spack uses for its builds.</p> <p>Support requests and feedback should be directed to support@alcf.anl.gov. For general Spack questions, users are encouraged to consult the following resources:</p> <ul> <li>Spack development website</li> <li>Spack documentation</li> <li>Spack tutorial</li> <li>Spack Slack channel</li> </ul>"},{"location":"polaris/build-tools/cmake-polaris/","title":"CMake","text":""},{"location":"polaris/build-tools/cmake-polaris/#cmake_1","title":"CMake","text":"<p>CMake is a build configuration system that uses higher-level description files to automatically generate Makefiles.</p>"},{"location":"polaris/build-tools/cmake-polaris/#cmake-documentation","title":"CMake Documentation","text":"<ul> <li>CMake website</li> </ul>"},{"location":"polaris/build-tools/cmake-polaris/#cmake-on-polaris","title":"CMake on Polaris","text":"<p>To use CMake on Polaris, run</p> <pre><code>module use /soft/modulefiles\nmodule load spack-pe-base cmake\n</code></pre>"},{"location":"polaris/compiling-and-linking/cce-compilers-polaris/","title":"CCE Compilers on Polaris","text":"<p>The Cray Compiling Environment (CCE) compilers are available on Polaris via the <code>PrgEnv-cray</code> module. </p> <p>The CCE compilers currently on Polaris only support AMD GPU targets for HIP and are thus not usable with the A100 GPUs. </p> <p>The <code>nvhpc</code> and <code>llvm</code> compilers can be used for compiling GPU-enabled applications.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/","title":"Compiling and Linking Overview on Polaris","text":""},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#compiling-on-polaris-login-and-compute-nodes","title":"Compiling on Polaris Login and Compute Nodes","text":"<p>If your build system does not require GPUs for the build process, as is usually the case, compilation of GPU-accelerated codes is generally expected to work well on the Polaris login nodes. If your build system does require GPUs, you cannot yet compile on the Polaris login nodes, as they do not currently have GPUs installed. You may in this case compile your applications on the Polaris compute nodes. Do this by submitting an interactive single-node job, or running your build system in a batch job.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#home-file-system","title":"Home File System","text":"<p>Is it helpful to realize that there is a single <code>HOME</code> filesystem for users that can be accessed from the login and computes of each production resource at ALCF. Thus, users should be mindful of modifications to their environments (e.g. <code>.bashrc</code>) that may cause issues to arise due to differences between the systems. </p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#cray-programming-environment","title":"Cray Programming Environment","text":"<p>The Cray Programming Environment (PE) uses three compiler wrappers for building software. These compiler wrappers should be used when building MPI-enabled applications.</p> <ul> <li><code>cc</code> - C compiler</li> <li><code>CC</code> - C++ compiler</li> <li><code>ftn</code> - Fortran compiler</li> </ul> <p>Each of these wrappers can select a specific vendor compiler based on the PrgEnv module loaded in the environment. The following are some helpful options to understand what the compiler wrapper is invoking.</p> <ul> <li><code>--craype-verbose</code> : Print the command which is forwarded to the compiler invocation</li> <li><code>--cray-print-opts=libs</code> : Print library information</li> <li><code>--cray-print-opts=cflags</code> : Print include information</li> </ul> <p>The output from these commands may be useful in build scripts where a compiler other than that invoked by a compiler wrapper is desired. Defining some variables as such may prove useful in those situations. <pre><code>CRAY_CFLAGS=$(cc --cray-print-opts=cflags)\nCRAY_LIB=$(cc --cray-print-opts=libs)\n</code></pre> Further documentation and options are available via <code>man cc</code> and similar. </p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#compilers-provided-by-cray-programming-environments","title":"Compilers provided by Cray Programming Environments","text":"<p>The default programming environment on Polaris is currently <code>NVHPC</code>. The <code>GNU</code> compilers are available via another programming environment. The following sequence of <code>module</code> commands can be used to switch to the <code>GNU</code> programming environment (gcc, g++, gfortran) and also have <code>NVIDIA</code> compilers available in your path.</p> <pre><code>module swap PrgEnv-nvhpc PrgEnv-gnu\nmodule load nvhpc-mixed\n</code></pre> <p>The compilers invoked by the Cray MPI wrappers are listed for each programming environment in the following table.</p> module C C++ Fortran MPI Compiler Wrapper cc CC ftn PrgEnv-nvhpc nvc nvc++ nvfortran PrgEnv-gnu gcc g++ gfortran <p>Note, while gcc and g++ may be available in the default environment, the <code>PrgEnv-gnu</code> module is needed to provide gfortran.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#additional-compilers-provided-by-alcf","title":"Additional Compilers Provided by ALCF","text":"<p>The ALCF additionally provides compilers to enable the OpenMP and SYCL programming models for GPUs via<code>LLVM</code> as documented here</p> <p>Additional documentation for using compilers is available on the respective programming model pages: OpenMP and SYCL.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#linking","title":"Linking","text":"<p>Dynamic linking of libraries is currently the default on Polaris. The Cray MPI wrappers will handle this automatically.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#notes-on-default-modules","title":"Notes on Default Modules","text":"<ul> <li> <p><code>craype-x86-rome</code>: While the Polaris compute nodes currently have Milan CPUs, this module is loaded by default to avoid the <code>craype-x86-milan</code> module from adding a <code>zen3</code> target not supported in the default <code>nvhpc/21.9</code> compilers. The <code>craype-x86-milan</code> module is expected to be made default once a newer <code>nvhpc</code> version (e.g. 22.5) is made the default.</p> </li> <li> <p><code>craype-accel-nvidia80</code>: This module adds compiler flags to enable GPU acceleration for <code>NVHPC</code> compilers along with gpu-enabled MPI libraries as it is assumed that the majority of applications to be compiled on Polaris will target the GPUs for acceleration. Users building cpu-only applications may find it useful to unload this module to silence \"gpu code generation\" warnings.</p> </li> </ul>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#mixed-cc-fortran-applications","title":"Mixed C/C++ &amp; Fortran Applications","text":"<p>For applications consisting of a mix of C/C++ and Fortran that also uses MPI, it is suggested that the programming environment chosen for Fortran be used to build the full application because of mpi.mod (and similar) incompatibilities. </p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#compiling-for-gpus","title":"Compiling for GPUs","text":"<p>It is assumed the majority of applications to be built on Polaris will make use of the GPUs. As such, the <code>craype-accel-nvidia80</code> module is in the default environment. This has the effect of the Cray compiler wrappers adding <code>-gpu</code> to the compiler invocation along with additional include paths and libraries. Additional compilers flags may be needed depending on the compiler and GPU programming model used (e.g. <code>-cuda</code>, <code>-acc</code>, or <code>-mp=gpu</code>).</p> <p>This module also adds GPU Transport Layer (GTL) libraries to the link-line to support GPU-aware MPI applications.</p>"},{"location":"polaris/compiling-and-linking/compiling-and-linking-overview/#man-pages","title":"Man Pages","text":"<p>For additional information on the Cray wrappers, please refer to the man pages. <pre><code>man cc\nman CC\nman ftn\n</code></pre></p>"},{"location":"polaris/compiling-and-linking/gnu-compilers-polaris/","title":"GNU Compilers on Polaris","text":"<p>The GNU compilers are available on Polaris via the <code>PrgEnv-gnu</code> and <code>gcc-mixed</code> modules. The <code>gcc-mixed</code> module can be useful when, for example, the <code>PrgEnv-nvhpc</code> compilers are used to compile C/C++ MPI-enabled code and gfortran is needed.</p> <p>The GNU compilers currently on Polaris do not support GPU code generation and thus can only be used for compiling CPU codes.</p> <p>The <code>nvhpc</code> and <code>llvm</code> compilers can be used for compiling GPU-enabled applications.</p>"},{"location":"polaris/compiling-and-linking/llvm-compilers-polaris/","title":"LLVM Compilers on Polaris","text":"<p>This page is not about LLVM-based Cray Compiling Environment (CCE) compilers from <code>PrgEnv-cray</code> but about open source LLVM compilers.</p> <p>If LLVM compilers are needed without MPI support, simply load the <code>llvm</code> module.</p> <p>Cray Programming Environment does not offer LLVM compiler support. Thus cc/CC/ftn compiler wrappers using LLVM compilers currently are not available. To use Clang with MPI, one can load the <code>mpiwrappers/cray-mpich-llvm</code> module which loads the following modules.</p> <ul> <li><code>llvm</code>, upstream llvm compilers</li> <li><code>cray-mpich</code>, MPI compiler wrappers mpicc/mpicxx/mpif90. mpif90 uses gfortran because flang is not ready for production use.</li> <li><code>cray-pals</code>, MPI launchers mpiexec/aprun/mpirun</li> </ul> <p>Limitation There is no GPU-aware MPI library linking support by default. If needed, users should manually add the GTL (GPU Transport Layer) library to the application link line.</p> <p>Update 04/25/2024 To access LLVM modules, <code>module use /soft/modulefiles</code> is required.</p>"},{"location":"polaris/compiling-and-linking/llvm-compilers-polaris/#openmp-offload","title":"OpenMP offload","text":"<p>When targeting the OpenMP or CUDA programming models for GPUs, the <code>cudatoolkit-standalone</code> module should also be loaded.</p>"},{"location":"polaris/compiling-and-linking/nvidia-compiler-polaris/","title":"NVIDIA Compilers on Polaris","text":"<p>The NVIDIA compilers (<code>nvc</code>, <code>nvc++</code>, <code>nvcc</code>, and <code>nvfortran</code>) are available on Polaris via the <code>PrgEnv-nvhpc</code> and <code>nvhpc</code> modules. There is currently a <code>PrgEnv-nvidia</code> module available, but that will soon be deprecated in Cray's PE, thus it is not recommend for use.</p> <p>The Cray compiler wrappers map to NVIDIA compilers as follows.</p> <pre><code>cc -&gt; nvc\nCC -&gt; nvc++\nftn -&gt; nvfortran\n</code></pre> <p>Users are encouraged to look through NVIDIA's documentation for the NVHPC SDK and specific information on the compilers, tools, and libraries.</p>"},{"location":"polaris/compiling-and-linking/nvidia-compiler-polaris/#notes-on-nvidia-compilers","title":"Notes on NVIDIA Compilers","text":""},{"location":"polaris/compiling-and-linking/nvidia-compiler-polaris/#pgi-compilers","title":"PGI compilers","text":"<p>The NVIDIA programming environments makes available compilers from the NVIDIA HPC SDK. While the PGI compilers are available in this programming environment, it should be noted they are actually symlinks to the corresponding <code>NVIDIA</code> compilers. <pre><code>pgcc -&gt; nvc\npgc++ -&gt; nvc++\npgf90 -&gt; nvfortran\npgfortran -&gt; nvfortran\n</code></pre> While <code>nvcc</code> is the traditional CUDA C and CUDA C++ compiler for NVIDIA GPUs, the <code>nvc</code>, <code>nvc++</code>, and <code>nvfortran</code> compilers additionally target CPUs.</p>"},{"location":"polaris/compiling-and-linking/nvidia-compiler-polaris/#nvhpc-sdk-directory-structure","title":"NVHPC SDK Directory Structure","text":"<p>Users migrating from CUDA toolkits to the NVHPC SDK may find it beneficial to review the directory structure of the <code>hpc-sdk</code> directory to find the location of commonly used libraries (including math libraries for the CPU). With the <code>PrgEnv-nvhpc</code> module loaded, the <code>NVIDIA_PATH</code> environment variable can be used to locate the path to various NVIDIA tools, libraries, and examples.</p> <ul> <li><code>compiler/bin</code> - cuda-gdb, ncu, nsys, ...</li> <li><code>examples</code> - CUDA-Fortran, OpenMP, ...</li> <li><code>comm_libs</code> - nccl, nvshmem, ...</li> <li><code>compiler/libs</code> - blas, lapack, ...</li> <li><code>cuda/lib64</code> - cudart, OpenCL, ...</li> <li><code>math_libs/lib64</code> - cublas, cufft, ...</li> </ul>"},{"location":"polaris/compiling-and-linking/nvidia-compiler-polaris/#differences-between-nvcc-and-nvcnvc","title":"Differences between nvcc and nvc/nvc++","text":"<p>For users that want to continue using <code>nvcc</code> it is important to be mindful of differences with the newer <code>nvc</code> and <code>nvc++</code> compilers. For example, the <code>-cuda</code> flag instructs <code>nvcc</code> to compile <code>.cu</code> input files to <code>.cu.cpp.ii</code> output files which are to be separately compiled, whereas the same <code>-cuda</code> flag instructs <code>nvc</code>, <code>nvc++</code>, and <code>nvfortran</code> to enable CUDA C/C++ or CUDA Fortran code generation. The resulting output file in each case is different (text vs. object) and one may see <code>unrecognized format error</code> when <code>-cuda</code> is incorrectly passed to <code>nvcc</code>.</p>"},{"location":"polaris/compiling-and-linking/nvidia-compiler-polaris/#known-issues-and-workarounds","title":"Known Issues and Workarounds","text":"<p>If you are using <code>nvcc</code> to invoke <code>nvc++</code> and compiling C++17 code, and are seeing the following warning and unable to compile C++17 constructs:</p> <pre><code>polaris-login-01(~)&gt; nvcc --std=c++17 -ccbin nvc++ ~/smalltests/bool_constant.cpp\nnvcc warning : The -std=c++17 flag is not supported with the configured host compiler. Flag will be ignored.\n\"/home/zippy/smalltests/bool_constant.cpp\", line 10: error: namespace \"std\" has no member class \"bool_constant\"\n      : std::bool_constant&lt;(UnaryPred&lt;Ts&gt;::value || ...)&gt; {};\n             ^\n\n\"/home/zippy/smalltests/bool_constant.cpp\", line 10: error: class or struct definition is missing\n      : std::bool_constant&lt;(UnaryPred&lt;Ts&gt;::value || ...)&gt; {};\n                          ^\n\n2 errors detected in the compilation of \"/home/zippy/smalltests/bool_constant.cpp\".\npolaris-login-01(~)&gt;\n</code></pre> <p>you will need to work around it by loading the latest cudatoolkit module atop PrgEnv-nvhpc:</p> <pre><code>module load cudatoolkit-standalone/11.6.2\n</code></pre>"},{"location":"polaris/compiling-and-linking/oneapi-compiler/","title":"oneAPI Compilers and Support","text":"<p>The Intel oneAPI compiler and Codeplay plugins for Nvidia GPUs are available on Polaris. The oneAPI compilers are not enabled under the Cray Programming Environment system but can be used separately. Two oneAPI variants are provided, the first being a \"release\" version based on Intel's officially released oneAPI toolkit. Intel Release Notes</p> <p>Note</p> <p>The 2023.2.1 release of oneAPI Toolkit does not yet support oneDPL on Nvidia devices. Though oneMKL is now added to 2023.2.1 release onwards</p>"},{"location":"polaris/compiling-and-linking/oneapi-compiler/#components","title":"Components","text":"<ul> <li>These are the list of components associated with this module</li> </ul> User Application Component Compilers DPC++ oneMKL Interfaces oneMKL <p>The other variant being a build from the open-source. This variant will be more up-to-date at the risk of bugs and breakages based on code that has not undergone a full release cycle. The documentation is located on the SYCL page. Most notable differences being, <code>icx/icpx</code> are the names of C/C++ compilers respectively when using the release version of the module where as <code>clang/clang++</code> are for open-source variant.</p>"},{"location":"polaris/compiling-and-linking/oneapi-compiler/#compile-and-link","title":"Compile and Link","text":"<p>oneAPI uses the clang (or icx/icpx wrapper) for compiling and linking for the Nvidia A100 SM80 architecture.</p> <pre><code>module load oneapi/release\nicpx -std=c++17 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_80 test.cpp\n</code></pre> <pre><code>harms@polaris-login-04:~/working/polaris/oneapi&gt; icpx -v\nIntel(R) oneAPI DPC++/C++ Compiler 2023.2.0 (2023.2.0.20230721)\nTarget: x86_64-unknown-linux-gnu\nThread model: posix\nInstalledDir: /soft/compilers/oneapi/release/2023.2/compiler/2023.2.1/linux/bin-llvm\nConfiguration file: /soft/compilers/oneapi/release/2023.2/compiler/2023.2.1/linux/bin-llvm/../bin/icpx.cfg\nFound candidate GCC installation: /usr/lib64/gcc/x86_64-suse-linux/7\nSelected GCC installation: /usr/lib64/gcc/x86_64-suse-linux/7\nCandidate multilib: .;@m64\nSelected multilib: .;@m64\nFound CUDA installation: /opt/nvidia/hpc_sdk/Linux_x86_64/21.9/cuda/11.4, version 11.4\n</code></pre>"},{"location":"polaris/compiling-and-linking/oneapi-compiler/#running","title":"Running","text":"<p>The library should select the GPU by default, but selection of the GPUs can be forced via the ONEAPI_DEVICE_SELECTOR <pre><code>$ ONEAPI_DEVICE_SELECTOR=cuda:gpu ./a.out\n</code></pre> or a specific GPU. <pre><code>$ ONEAPI_DEVICE_SELECTOR=cuda:gpu:3 ./a.out\n</code></pre></p>"},{"location":"polaris/compiling-and-linking/oneapi-compiler/#sycl-ls","title":"sycl-ls","text":"<p>Expected output of sycl-ls and which platforms are available.</p> <pre><code>harms@x3004c0s7b0n0:~&gt; which sycl-ls\n/soft/compilers/oneapi/release/2023.2/compiler/2023.2.1/linux/bin/sycl-ls\n\nharms@x3004c0s7b0n0:~&gt; sycl-ls\n[opencl:acc:0] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device 1.2 [2023.16.7.0.21_160000]\n[opencl:cpu:1] Intel(R) OpenCL, AMD EPYC 7543P 32-Core Processor                3.0 [2023.16.7.0.21_160000]\n[ext_oneapi_cuda:gpu:0] NVIDIA CUDA BACKEND, NVIDIA A100-SXM4-40GB 8.8 [CUDA 11.4]\n[ext_oneapi_cuda:gpu:1] NVIDIA CUDA BACKEND, NVIDIA A100-SXM4-40GB 8.8 [CUDA 11.4]\n[ext_oneapi_cuda:gpu:2] NVIDIA CUDA BACKEND, NVIDIA A100-SXM4-40GB 8.8 [CUDA 11.4]\n[ext_oneapi_cuda:gpu:3] NVIDIA CUDA BACKEND, NVIDIA A100-SXM4-40GB 8.8 [CUDA 11.4]\n</code></pre>"},{"location":"polaris/compiling-and-linking/polaris-example-program-makefile/","title":"Example Programs and Makefiles for Polaris","text":"<p>Several simple examples of building CPU and GPU-enabled codes on Polaris are available in the ALCF GettingStart repo for several programming models. If build your application is problematic for some reason (e.g. absence of a GPU), then users are encouraged to build and test applications directly on one of the Polaris compute nodes via an interactive job. The discussion below makes use of the <code>NVHPC</code> compilers in the default environment as illustrative examples. Similar examples for other compilers on Polaris are available in the ALCF GettingStarted repo.</p>"},{"location":"polaris/compiling-and-linking/polaris-example-program-makefile/#cpu-mpiopenmp-example","title":"CPU MPI+OpenMP Example","text":"<p>One of the first useful tasks with any new machine, scheduler, and job launcher is to ensure one is binding MPI ranks and OpenMP threads to the host cpu as intended. A simple HelloWorld MPI+OpenMP example is available here to get started with.</p> <p>The application can be straightforwardly compiled using the Cray compiler wrappers. <pre><code>CC -fopenmp main.cpp -o hello_affinity\n</code></pre></p> <p>The executable <code>hello_affinity</code> can then be launched in a job script (or directly in shell of interactive job) using <code>mpiexec</code> as discussed here.</p> <pre><code>#!/bin/sh\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home\n\n# MPI example w/ 16 MPI ranks per node spread evenly across cores\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=16\nNDEPTH=4\nNTHREADS=1\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./hello_affinity\n</code></pre>"},{"location":"polaris/compiling-and-linking/polaris-example-program-makefile/#cuda","title":"CUDA","text":"<p>Several variants of C/C++ and Fortran CUDA examples are available here that include MPI and multi-gpu examples.</p> <p>One can use the Cray compiler wrappers to compile GPU-enabled applications as well. This example of simple vector addition uses the NVIDIA compilers.</p> <pre><code>CC -g -O3 -std=c++0x -cuda main.cpp -o vecadd\n</code></pre> <p>The <code>craype-accel-nvidia80</code> module in the default environment will add the <code>-gpu</code> compiler flag for <code>nvhpc</code> compilers along with appropriate include directories and libraries. It is left to the user to provide an additional flag to the <code>nvhpc</code> compilers to select the target GPU programming model. In this case, <code>-cuda</code> is used to indicate compilation of CUDA code. The application can then be launched within a batch job submission script or as follows on one of the compute nodes.</p> <pre><code>$ ./vecadd \n# of devices= 4\n  [0] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ]\n  [1] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ]\n  [2] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ]\n  [3] Platform[ Nvidia ] Type[ GPU ] Device[ NVIDIA A100-SXM4-40GB ]\nRunning on GPU 0!\nUsing single-precision\n\n  Name= NVIDIA A100-SXM4-40GB\n  Locally unique identifier= \n  Clock Frequency(KHz)= 1410000\n  Compute Mode= 0\n  Major compute capability= 8\n  Minor compute capability= 0\n  Number of multiprocessors on device= 108\n  Warp size in threads= 32\n  Single precision performance ratio= 2\n\nResult is CORRECT!! :)\n</code></pre>"},{"location":"polaris/compiling-and-linking/polaris-example-program-makefile/#gpu-openacc","title":"GPU OpenACC","text":"<p>A simple MPI-parallel OpenACC example is available here. Compilation proceeds similar to the above CUDA example except for the use of the <code>-acc=gpu</code> compiler flag to indicate compilation of OpenACC code for GPUs. <pre><code>CC -g -O3 -std=c++0x -acc=gpu -gpu=cc80,cuda11.0 main.cpp -o vecadd\n</code></pre> In this example, each MPI rank sees all four GPUs on a Polaris node and GPUs are bound to MPI ranks round-robin within the application.</p> <p><pre><code>$ mpiexec -n 4 ./vecadd\n# of devices= 4\nUsing single-precision\n\nRank 0 running on GPU 0!\nRank 1 running on GPU 1!\nRank 2 running on GPU 2!\nRank 3 running on GPU 3!\n\nResult is CORRECT!! :)\n</code></pre> If the application instead relies on the job launcher to bind MPI ranks to available GPUs, then a small helper script can be used to explicitly set <code>CUDA_VISIBLE_DEVICES</code> appropriately for each MPI rank. One example is available here where each MPI rank is similarly bound to a single GPU with round-robin assignment. The binding of MPI ranks to GPUs is discussed in more detail here.</p>"},{"location":"polaris/compiling-and-linking/polaris-example-program-makefile/#gpu-opencl","title":"GPU OpenCL","text":"<p>A simple OpenCL example is available here. The OpenCL headers and library are available in the NVHPC SDK and cuda toolkits. The environment variable <code>NVIDIA_PATH</code> is defined for the <code>PrgEnv-nvhpc</code> programming environment.  <pre><code>CC -o vecadd -g -O3 -std=c++0x  -I${NVIDIA_PATH}/cuda/include main.o -L${NVIDIA_PATH}/cuda/lib64 -lOpenCL\n</code></pre></p> <p>This simple example can be run on a Polaris compute node as follows. <pre><code>$ ./vecadd\nRunning on GPU!\nUsing single-precision\n\n    CL_DEVICE_NAME: NVIDIA A100-SXM4-40GB\n    CL_DEVICE_VERSION: OpenCL 3.0 CUDA\n    CL_DEVICE_OPENCL_C_VERSION: OpenCL C 1.2 \n    CL_DEVICE_MAX_COMPUTE_UNITS: 108\n    CL_DEVICE_MAX_CLOCK_FREQUENCY: 1410\n    CL_DEVICE_MAX_WORK_GROUP_SIZE: 1024\n\nResult is CORRECT!! :)\n</code></pre></p>"},{"location":"polaris/compiling-and-linking/polaris-example-program-makefile/#gpu-openmp","title":"GPU OpenMP","text":"<p>A simple MPI-parallel OpenMP example is available here. Compilation proceeds similar to the above examples except for use of the <code>-mp=gpu</code> compiler flag to indicated compilation of OpenMP code for GPUs.</p> <pre><code>CC -g -O3 -std=c++0x -mp=gpu -gpu=cc80,cuda11.0 -c main.cpp -o vecadd\n</code></pre> <p>Similar to the OpenACC example above, this code binds MPI ranks to GPUs in a round-robin fashion.  <pre><code>$ mpiexec -n 4 ./vecadd\n# of devices= 4\nRank 0 running on GPU 0!\nRank 1 running on GPU 1!\nRank 2 running on GPU 2!\nRank 3 running on GPU 3!\n\nResult is CORRECT!! :)\n</code></pre></p>"},{"location":"polaris/compiling-and-linking/polaris-programming-models/","title":"Programming Models on Polaris","text":"<p>The software environment on Polaris supports several parallel programming models targeting the CPUs and GPUs.</p>"},{"location":"polaris/compiling-and-linking/polaris-programming-models/#cpu-parallel-programming-models","title":"CPU Parallel Programming Models","text":"<p>The Cray compiler wrappers <code>cc</code>, <code>CC</code>, and <code>ftn</code> are recommended for MPI applications as they provide the needed include paths and libraries for each programming environment. A summary of available CPU parallel programming models and relevant compiler flags is shown below. Users are encouraged to review the corresponding man pages and documentation.</p> Programming Model GNU NVHPC LLVM OpenMP -fopenmp -mp -fopenmp OpenACC -- -acc=multicore -- <p>Higher-level programming models such as Kokkos and Raja may also be used for CPU programming.</p>"},{"location":"polaris/compiling-and-linking/polaris-programming-models/#gpu-programming-models","title":"GPU Programming Models","text":"<p>A summary of available GPU programming models and relevant compiler flags is shown below for compilers that generate offloadable code. Users are encouraged to review the corresponding man pages and documentation.</p> Programming Model GNU NVHPC LLVM ONEAPI CUDA -- -cuda [-gpu=cuda8.0,cc11.0] -- -- HIP* -- -- -- -- OpenACC -- -acc -- -- OpenCL* -- -- -- -- OpenMP -- -mp=gpu -fopenmp-targets=nvptx64 -- SYCL -- -- -- -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_80 <p>Note, the <code>llvm</code> and <code>oneapi</code> modules are provided by ALCF to complement the compilers provided by the Cray PE on Polaris.</p> <p>Higher-level programming models such as Kokkos and Raja may also be used for GPU programming.</p> <p>OpenCL is supported, but does not require specific compiler flags per-se as the offloaded kernels are just-in-time compiled. Abstraction programming models, such as Kokkos, can be built on top of some of these programming models (see below).</p> <p>A HIP compiler supporting the A100 GPUs is still to be installed on Polaris.</p>"},{"location":"polaris/compiling-and-linking/polaris-programming-models/#mapping-programming-models-to-polaris-modules","title":"Mapping Programming Models to Polaris Modules","text":"<p>The table below offers some suggestions for how to get started setting up your environment on Polaris depending on the programming language and model. Note, mixed C/C++ and Fortran applications should choose the programming environment for the Fortran compiler because of mpi.mod and similar incompatibilities between Fortran-generated files from different compilers. Several simple examples for testing the software environment on Polaris for different programming models are available in the ALCF GettingStart repo.</p> <p>Note, users are encouraged to use <code>PrgEnv-nvhpc</code> instead of <code>PrgEnv-nvidia</code> as the latter will soon be deprecated in Cray's PE. They are otherwise identical pointing to compilers from the same NVIDIA SDK version.</p> Programming Language GPU Programming Model Likely used Modules/Compilers Notes C/C++ CUDA PrgEnv-nvhpc, PrgEnv-gnu, llvm NVIDIA (nvcc, nvc, nvc++) and clang compilers do GPU code generation C/C++ HIP N/A need to install with support for A100 C/C++ Kokkos See CUDA HIP, OpenMP, and SYCL/DPC++ also candidates C/C++ OpenACC PrgEnv-nvhpc C/C++ OpenCL PrgEnv-nvhpc, PrgEnv-gnu, llvm JIT GPU code generation C/C++ OpenMP PrgEnv-nvhpc, llvm C/C++ RAJA See CUDA HIP, OpenMP, and SYCL/DPC++ also candidates C/C++ SYCL/DPC++ llvm-sycl Fortran CUDA PrgEnv-nvhpc NVIDIA compiler (nvfortran) does GPU code generation; <code>gfortran</code> can be loaded via <code>gcc-mixed</code> Fortran HIP N/A need to install with support for A100 Fortran OpenACC PrgEnv-nvhpc Fortran OpenCL PrgEnv-nvhpc, PrgEnv-gnu JIT GPU code generation Fortran OpenMP PrgEnv-nvhpc"},{"location":"polaris/data-science-workflows/julia/","title":"Julia","text":"<p>Julia is a high-level, high-performance dynamic programming language for technical computing. It has a syntax familiar to users of many other technical computing environments. Designed at MIT to tackle large-scale partial-differential equation simulation and distributed linear algebra, Julia features a robust ecosystem of tools for optimization, statistics, parallel programming, and data visualization. Julia is actively developed by the Julia Labs team at MIT and in industry, along with hundreds of domain-expert scientists and programmers worldwide.</p>"},{"location":"polaris/data-science-workflows/julia/#contributing","title":"Contributing","text":"<p>This guide is a first draft of the Julia documentation for Polaris. If you have any suggestions or contributions, please open a pull request or contact us by opening a ticket at the ALCF Helpdesk.</p>"},{"location":"polaris/data-science-workflows/julia/#julia-installation","title":"Julia Installation","text":"<p>We encourage users interested in using Julia on Polaris to install in their home or project directories at this time. Using the official Julia 1.9 binaries from the Julia webpage is recommended. Juliaup provides a convenient way to install Julia and manage the various Julia versions. The default installation will install <code>julia</code>, <code>juliaup</code>, and other commands in a <code>${HOME}/.julia</code> directory and update profile files like <code>.bashrc</code> to update <code>PATH</code> to include that directory. One can customize the installation to change these defaults.</p> <pre><code>module load craype-accel-nvidia80\ncurl -fsSL https://install.julialang.org | sh\n</code></pre> <p>If you chose a custom installation, then be sure to update the <code>PATH</code> environment variable appropriately.</p> <pre><code>export PATH=${HOME}/.juliaup/bin:${PATH}\n</code></pre> <p>You may then list the available Julia versions with <code>juliaup list</code> and install a specific version with <code>juliaup install &lt;version&gt;</code>. You can then activate a specific version with <code>juliaup use &lt;version&gt;</code> and set the default version with <code>juliaup default &lt;version&gt;</code>. <code>juliaup update</code> will update the installed Julia versions. In general, the latest stable release of Julia should be used.</p> <pre><code>juliaup add release\n</code></pre>"},{"location":"polaris/data-science-workflows/julia/#julia-project-environment","title":"Julia Project Environment","text":"<p>The Julia built-in package manager allows you to create a project and enable project-specific dependencies. Julia manages packages in the Julia depot located by default in <code>~/.julia</code>. However, that NFS filesystem is not meant for high-speed access. Therefore, this Julia depot folder should be located on a fast filesystem of your choice (grand, eagle). The Julia depot directory is set via the environment variable <code>JULIA_DEPOT_PATH</code>. For example, you can set the Julia depot to a directory on Polaris grand filesystem by adding the following line to your <code>~/.bashrc</code> file:</p> <pre><code>export JULIA_DEPOT_PATH=/grand/$PROJECT/$USER/julia_depot\n</code></pre>"},{"location":"polaris/data-science-workflows/julia/#programming-julia-on-polaris","title":"Programming Julia on Polaris","text":"<p>There are three key components to using Julia for large-scale computations:</p> <ol> <li>MPI support through MPI.jl</li> <li>GPU support through CUDA.jl</li> <li>HDF5 support through HDF5.jl</li> </ol> <p>In addition, we recommend VSCode with the Julia extension for a modern IDE experience, together with the ssh-remote extension for remote interactive development.</p>"},{"location":"polaris/data-science-workflows/julia/#mpi-support","title":"MPI Support","text":"<p>MPI support is provided through the MPI.jl. <pre><code>$ julia --project -e 'using Pkg; Pkg.add(\"MPI\")'\n</code></pre> This will install the MPI.jl package and default MPI prebuilt binaries provided by an artifact. For on-node debugging purposes the default artifact is sufficient. However, for large-scale computations, it is important to use the Cray MPICH installed on Polaris. As of MPI.jl v0.20 this is handled through MPIPrefences.jl. <pre><code>$ julia --project -e 'using Pkg; Pkg.add(\"MPIPreferences\")'\n$ julia --project -e 'using MPIPreferences; MPIPreferences.use_system_binary(vendor=\"cray\")'\n</code></pre></p> <p>The <code>vendor=\"cray\"</code> option is important if you intend to use gpu-aware MPI in your applications. </p> <p>Check that the correct MPI library is targeted with Julia. <pre><code>julia --project -e 'using MPI; MPI.versioninfo()'\nMPIPreferences:\n  binary:  system\n  abi:     MPICH\n  libmpi:  libmpi_nvidia.so\n  mpiexec: mpiexec\n\nPackage versions\n  MPI.jl:             0.20.19\n  MPIPreferences.jl:  0.1.11\n\nLibrary information:\n  libmpi:  libmpi_nvidia.so\n  libmpi dlpath:  /opt/cray/pe/lib64/libmpi_nvidia.so\n  MPI version:  3.1.0\n  Library version:  \n    MPI VERSION    : CRAY MPICH version 8.1.28.2 (ANL base 3.4a2)\n    MPI BUILD INFO : Wed Nov 15 21:59 2023 (git hash 1cde46f)\n</code></pre> When running on the login node, switch back to the default provided MPI binaries in <code>MPI_jll.jl</code> by removing the <code>LocalPreferences.toml</code> file.</p>"},{"location":"polaris/data-science-workflows/julia/#gpu-support","title":"GPU Support","text":"<p>NVIDIA GPU support is provided through the CUDA.jl package. The default in Julia is to download artifacts (e.g. CUDA toolkit) based on the runtime detected. While that should generally work, it is recommended to use the local CUDA installation provided on Polaris especially if using gpu-aware MPI in your workloads (important to use supported versions of CUDA with Cray MPICH provided). </p> <p>To use the local CUDA installation provided by the modules on Polaris, the <code>LocalPreferences.toml</code> file can be modified as follows.</p> <pre><code>$ head $JULIA_DEPOT_PATH/environments/v1.10/LocalPreferences.toml\n[CUDA_Runtime_jll]\nlocal = true\n</code></pre> <p>If using the default <code>PrgEnv-nvhpc</code> module on Polaris, then it will be necessary to correct a path to the CUPTI library to successfully install <code>CUDA.jl</code>.</p> <pre><code>$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CRAY_NVIDIA_PREFIX/cuda/12.2/extras/CUPTI/lib64/\n$ julia --project -e 'using Pkg; Pkg.add(\"CUDA\")'\n</code></pre> <p>The GPUs are not currently usable on the Polaris login nodes, so one can confirm the version of CUDA being used by testing in a batch or interactive job on a compute node.</p> <pre><code>$ qsub -I -l select=1,walltime=1:00:00,filesystems=home:grand:eagle -A [PROJECT] -q debug\n\n$ julia --project -e \"using CUDA; CUDA.versioninfo()\"\nCUDA runtime 12.4, artifact installation\nCUDA driver 12.4\nNVIDIA driver 535.154.5, originally for CUDA 12.2\n\nCUDA libraries: \n- CUBLAS: 12.2.1\n- CURAND: 10.3.5\n- CUFFT: 11.2.1\n- CUSOLVER: 11.6.1\n- CUSPARSE: 12.3.1\n- CUPTI: 22.0.0\n- NVML: 12.0.0+535.154.5\n\nJulia packages: \n- CUDA: 5.3.3\n- CUDA_Driver_jll: 0.8.1+0\n- CUDA_Runtime_jll: 0.12.1+0\n\nToolchain:\n- Julia: 1.10.3\n- LLVM: 15.0.7\n\n4 devices:\n  0: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  1: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  2: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  3: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n</code></pre> <p>One can then switch between versions of CUDA as needed. Note, the following commands were executed in an interactive job on a compute node.</p> <pre><code>$ julia --project -e \"using CUDA; CUDA.set_runtime_version!(local_toolkit=true)\"\n[ Info: Configure the active project to use the default CUDA from the local system; please re-start Julia for this to take effect.\n</code></pre> <pre><code>$ julia --project -e \"using CUDA; CUDA.versioninfo()\"\nCUDA runtime 12.2, local installation\nCUDA driver 12.4\nNVIDIA driver 535.154.5, originally for CUDA 12.2\n\nCUDA libraries: \n- CUBLAS: 12.2.1\n- CURAND: 10.3.3\n- CUFFT: 11.0.8\n- CUSOLVER: 11.5.0\n- CUSPARSE: 12.1.1\n- CUPTI: 20.0.0\n- NVML: 12.0.0+535.154.5\n\nJulia packages: \n- CUDA: 5.3.3\n- CUDA_Driver_jll: 0.8.1+0\n- CUDA_Runtime_jll: 0.12.1+0\n- CUDA_Runtime_Discovery: 0.2.4\n\nToolchain:\n- Julia: 1.10.3\n- LLVM: 15.0.7\n\nPreferences:\n- CUDA_Runtime_jll.local: true\n\n4 devices:\n  0: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  1: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  2: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)\n  3: NVIDIA A100-SXM4-40GB (sm_80, 39.390 GiB / 40.000 GiB available)```\n</code></pre> <p>Warning messages from the presence of CUDA in <code>LD_LIBRARY_PATH</code> were ommitted in output of the first two commands. In this case, the artifact and local installation are similar. If there was a difference, then the local installation should be preferred.</p> <p>In case you want write portable GPU kernels we highly recommend the KernelAbstractions.jl package. It provides a high-level abstraction for writing GPU kernels that can be compiled for different GPU backends.</p> <pre><code>julia --project -e 'using Pkg; Pkg.add(\"KernelAbstractions\")'\n</code></pre> <p>By loading either oneAPI.jl, AMDGPU.jl, or CUDA.jl (see quickstart guide below).</p>"},{"location":"polaris/data-science-workflows/julia/#cuda-aware-mpi","title":"CUDA-aware MPI","text":"<p>MPI.jl supports CUDA-aware MPI. This is enabled by setting the following environment variables.</p> <pre><code>export JULIA_CUDA_MEMORY_POOL=none\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport JULIA_MPI_PATH=${CRAY_MPICH_DIR}\nexport JULIA_MPI_HAS_CUDA=1\n</code></pre> <p>Note that <code>MPI.jl</code> needs to be rebuilt for the changes to take effect.</p> <pre><code>julia --project -e 'using Pkg; Pkg.build(\"MPI\")'\n</code></pre>"},{"location":"polaris/data-science-workflows/julia/#hdf5-support","title":"HDF5 Support","text":"<p>Parallel HDF5 support is provided by <pre><code>module load cray-hdf5-parallel\n</code></pre> After setting <code>export JULIA_HDF5_PATH=$HDF5_DIR</code> we can install the HDF5.jl package.</p> <pre><code>julia --project -e 'using Pkg; Pkg.add(\"HDF5\")'\n</code></pre> <p>To remove warning messages indicating that use of <code>JULIA_HDF5_PATH</code> has been deprecated, one can use the following command to set the HDF5 libraries.</p> <pre><code>$ echo $CRAY_HDF5_PARALLEL_PREFIX/\n/opt/cray/pe/hdf5-parallel/1.12.2.9/nvidia/23.3\n\n$ julia --project -e 'using HDF5; HDF5.API.set_libraries!(\"/opt/cray/pe/hdf5-parallel/1.12.2.9/nvidia/23.3/lib/libhdf5.so\", \"/opt/cray/pe/hdf5-parallel/1.12.2.9/nvidia/23.3/lib/libhdf5_hl.so\")'\n</code></pre>"},{"location":"polaris/data-science-workflows/julia/#quickstart-guide","title":"Quickstart Guide","text":"<p>The following example shows how to use MPI.jl, CUDA.jl, and HDF5.jl to write a parallel program that computes the sum of two vectors on the GPU and writes the result to an HDF5 file. A repository with an example code computing an approximation of pi can be found at Polaris.jl. In this repository, you will also find a <code>setup_polaris.sh</code> script that will build the HDF5.jl and MPI.jl package for the system libraries. The dependencies are installed with the following commands: <pre><code>julia --project\n</code></pre></p> <pre><code>julia&gt; ] up\n</code></pre> <pre><code>using CUDA\nusing HDF5\nusing MPI\nusing Printf\nusing Random\n\nfunction pi_kernel(x, y, d, n)\n    idx = (blockIdx().x-1) * blockDim().x + threadIdx().x\n    if idx &lt;= n\n        d[idx] = (x[idx] - 0.5)^2 + (y[idx] - 0.5)^2 &lt;= 0.25 ? 1 : 0\n    end\n    return nothing\nend\n\nfunction approximate_pi_gpu(n::Integer)\n    x = CUDA.rand(Float64, n)\n    y = CUDA.rand(Float64, n)\n    d = CUDA.zeros(Float64, n)\n\n    nblocks = ceil(Int64, n/32)\n\n    @cuda threads=32 blocks=nblocks pi_kernel(x,y,d,n)\n\n    return sum(d)\nend\n\nfunction main()\n    n = 100000  # Number of points to generate per rank\n    Random.seed!(1234)  # Set a fixed random seed for reproducibility\n\n    dsum = MPI.Allreduce(approximate_pi_gpu(n), MPI.SUM, MPI.COMM_WORLD)\n\n    pi_approx = (4 * dsum) / (n * MPI.Comm_size(MPI.COMM_WORLD))\n\n    if MPI.Comm_rank(MPI.COMM_WORLD) == 0\n        @printf \"Approximation of \u03c0 using Monte Carlo method: %.10f\\n\" pi_approx\n        @printf \"Error: %.10f\\n\" abs(pi_approx - \u03c0)\n    end\n    return pi_approx\nend\n\nMPI.Init()\nif !isinteractive()\n    pi_approx = main()\n    h5open(\"pi.h5\", \"w\") do file\n        write(file, \"pi\", pi_approx)\n    end\nend\n</code></pre>"},{"location":"polaris/data-science-workflows/julia/#job-submission-script","title":"Job submission script","text":"<p>This example can be run on Polaris with the following job submission script:</p> <pre><code>#!/bin/bash -l\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:grand\n#PBS -q debug\n#PBS -A PROJECT\n\ncd ${PBS_O_WORKDIR}\n\n# MPI example w/ 4 MPI ranks per node spread evenly across cores\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=4\nNDEPTH=8\nNTHREADS=1\n\n# Setup Julia environment\n. ./setup_env.sh\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE=${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\nEXE=/home/knight/.julia/juliaup/julia-1.10.3+0.x64.linux.gnu/bin/julia\n\nMPI_ARGS=\"-n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth\"\n\nmpiexec ${MPI_ARGS} ${EXE} --check-bounds=no --project pi.jl\n</code></pre> <p>The <code>setup_env.sh</code> script updates the environment as indicated above.</p> <pre><code>$ cat ./setup_env.sh\nmodule restore\nmodule load craype-accel-nvidia80\nmodule load cray-hdf5-parallel\n\nexport PATH=/home/knight/.juliaup/bin:${PATH}\nexport JULIA_DEPOT_PATH=/grand/catalyst/proj-shared/knight/polaris/julia/depot\n\nexport JULIA_HDF5_PATH=$HDF5_DIR\n\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CRAY_NVIDIA_PREFIX/cuda/12.2/extras/CUPTI/lib64/\n\nexport JULIA_CUDA_MEMORY_POOL=none\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport JULIA_MPI_PATH=${CRAY_MPICH_DIR}\nexport JULIA_MPI_HAS_CUDA=1\n\nexport TMPDIR=/local/scratch\n\n# Temporary workaround\nexport LD_PRELOAD=libmpi_gtl_cuda.so\n</code></pre> <p>Verify that <code>JULIA_DEPOT_PATH</code> is set to the correct path and <code>JULIA_PATH</code> points to the Julia executable. When using <code>juliaup</code>, the Julia executable is located in the <code>juliaup</code> folder of your <code>JULIA_DEPOT_PATH</code>.</p>"},{"location":"polaris/data-science-workflows/julia/#large-scale-parallelism","title":"Large-scale parallelism","text":"<p><code>CUDA.jl</code> uses the <code>nvcc</code> compiler to compile GPU kernels. This will create object files in the <code>TEMP</code> filesystem. The default <code>TMPDIR</code> in a job on Polaris is set to a temp directory that only exists on the head node of a job. We recommend setting <code>TEMPDIR</code> to a local directory on each compute node. <pre><code>export TMPDIR=/local/scratch\n</code></pre></p> <p>A simple example to test gpu-aware MPI on multiple nodes is available here.</p>"},{"location":"polaris/data-science-workflows/python/","title":"Python","text":"<p>We provide prebuilt <code>conda</code> environments containing GPU-supported builds of <code>torch</code>, <code>tensorflow</code> (both with <code>horovod</code> support for multi-node calculations), <code>jax</code>, and many other commonly-used Python modules.</p> <p>Users can activate this environment by first loading the <code>conda</code> module, and then activating the base environment.</p> <p>Explicitly (either from an interactive job, or inside a job script):</p> <pre><code>module use /soft/modulefiles; module load conda ; conda activate base\n</code></pre> <p>This will load and activate the base environment.</p>"},{"location":"polaris/data-science-workflows/python/#virtual-environments-via-venv","title":"Virtual environments via <code>venv</code>","text":"<p>To install additional packages that are missing from the <code>base</code> environment, we can build a <code>venv</code> on top of it.</p> <p>Conda <code>base</code> environment + <code>venv</code></p> <p>If you need a package that is not already installed in the <code>base</code> environment, this is generally the recommended approach.</p> <p>We can create a <code>venv</code> on top of the base Anaconda environment (with <code>--system-site-packaes</code> to inherit the <code>base</code> packaes):</p> <pre><code>module use /soft/modulefiles ; module load conda; conda activate base\nCONDA_NAME=$(echo ${CONDA_PREFIX} | tr '\\/' '\\t' | sed -E 's/mconda3|\\/base//g' | awk '{print $NF}')\nVENV_DIR=\"$(pwd)/venvs/${CONDA_NAME}\"\nmkdir -p \"${VENV_DIR}\"\npython -m venv \"${VENV_DIR}\" --system-site-packages\nsource \"${VENV_DIR}/bin/activate\"\n</code></pre> <p>You can always retroactively change the <code>--system-site-packages</code> flag state for this virtual environment by editing <code>${VENV_DIR}/pyvenv.cfg</code> and changing the value of the line <code>include-system-site-packages=false</code>.</p> <p>To install a different version of a package that is already installed in the base environment, you can use:</p> <pre><code>python3 pip install --ignore-installed &lt;package&gt; # or -I\n</code></pre> <p>The shared base environment is not writable, so it is impossible to remove or uninstall packages from it. The packages installed with the above <code>pip</code> command should shadow those installed in the base environment.</p>"},{"location":"polaris/data-science-workflows/python/#cloning-the-base-anaconda-environment","title":"Cloning the base Anaconda environment","text":"<p>Warning</p> <p>This approach is generally not recommended as it can be quite slow and can use significant storage space.</p> <p>If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via <code>conda install&lt;module&gt;</code> or <code>pip install &lt;module&gt;</code>.</p> <p>Unlike the <code>venv</code> approach, using a cloned Anaconda environment requires you to copy the entirety of the base environment, which can use significant storage space.</p> <p>To clone the <code>base</code> environment:</p> <pre><code>module load conda ; conda activate base\nconda create --clone base --prefix /path/to/envs/base-clone\nconda activate /path/to/envs/base-clone\n</code></pre> <p>where, <code>path/to/envs/base-clone</code> should be replaced by a suitably chosen path.</p> <p>Note: The cloning process can be quite slow.</p>"},{"location":"polaris/data-science-workflows/python/#using-pip-install-user-not-recommended","title":"Using <code>pip install --user</code> (not recommended)","text":"<p>Danger</p> <p>This is typically not recommended.</p> <p>With the conda environment setup, one can install common Python modules using <code>python3 pip install --users '&lt;module-name&gt;'</code> which will install packages in <code>$PYTHONUSERBASE/lib/pythonX.Y/site-packages</code>.</p> <p>The <code>$PYTHONUSERBASE</code> environment variable is automatically set when you load the base conda module, and is equal to  <code>/home/$USER/.local/polaris/conda/YYYY-MM-DD</code>.</p> <p>Note, Python modules installed this way that contain command line binaries will not have those binaries automatically added to the shell's <code>$PATH</code>. To manually add the path:</p> <pre><code>export PATH=\"$PYTHONUSERBASE/bin:$PATH\"\n</code></pre> <p>Be sure to remove this location from <code>$PATH</code> if you deactivate the base Anaconda environment or unload the module.</p> <p>Cloning the Anaconda environment, or using <code>venv</code> are both more flexible and transparent when compared to <code>--user</code> installs.</p>"},{"location":"polaris/data-science-workflows/python/#existing-issue-and-solution","title":"Existing issue and solution","text":"<p>There is an issue with the current conda environment. One may encounter the following error message: </p> <pre><code>aborting job:\nMPIDI_CRAY_init: GPU_SUPPORT_ENABLED is requested, but GTL library is not linked\n</code></pre> <p>To addresss this, please add the following line in the very beginning of your python script. </p> <pre><code>from mpi4py import MPI\n</code></pre>"},{"location":"polaris/data-science-workflows/applications/gpt-neox/","title":"Instructions for <code>gpt-neox</code>:","text":"<p>We include below a set of instructions to get <code>EleutherAI/gpt-neox</code> running on Polaris.</p> <p>A batch submission script for the following example is available here.</p> <p>Warning</p> <p>The instructions below should be ran directly from a compute node.</p> <p>Explicitly, to request an interactive job (from <code>polaris-login</code>): <pre><code>$ qsub -A &lt;project&gt; -q debug-scaling -l select=2 -l walltime=01:00:00\n</code></pre></p> <p>Refer to job scheduling and execution for additional information.</p> <ol> <li> <p>Load and activate the base <code>conda</code> environment:   <pre><code>module load conda\nconda activate base\n</code></pre></p> </li> <li> <p>We've installed the requirements for running <code>gpt-neox</code> into a virtual    environment. To activate this environment,   <pre><code>source /soft/datascience/venvs/polaris/2022-09-08/bin/activate\n</code></pre></p> </li> <li> <p>Clone the <code>EleutherAI/gpt-neox</code> repository if it doesn't already exist:   <pre><code>git clone https://github.com/EleutherAI/gpt-neox\n</code></pre></p> </li> <li> <p>Navigate into the <code>gpt-neox</code> directory:   <pre><code>cd gpt-neox\n</code></pre> <p>Note</p> <p>The remaining instructions assume you're inside the <code>gpt-neox</code> directory   </p> </p> </li> <li> <p>Create a DeepSpeed compliant <code>hostfile</code> (each line is formatted as <code>hostname, slots=N</code>):   <pre><code>cat $PBS_NODEFILE &gt; hostfile\nsed -e 's/$/ slots=4/' -i hostfile\nexport DLTS_HOSTFILE=hostfile \n</code></pre></p> </li> <li> <p>Create a <code>.deepspeed_env</code> file to ensure a consistent environment across all    workers    <pre><code>echo \"PATH=${PATH} &gt; .deepspeed_env\"\necho \"LD_LIBRARY_PATH=${LD_LIBRARY_PATH} &gt;&gt; .deepspeed_env\"\necho \"http_proxy=${http_proxy} &gt;&gt; .deepspeed_env\"\necho \"https_proxy=${https_proxy} &gt;&gt; .deepspeed_env\"\n</code></pre></p> </li> <li> <p>Prepare data:   <pre><code>python3 prepare_data.py -d ./data\n</code></pre></p> </li> <li> <p>Train:   <pre><code>python3 ./deepy.py train.py -d configs small.yml local_setup.yml\n</code></pre></p> </li> </ol> <p>Danger</p> <p>If your training seems to be getting stuck at</p> <pre><code>Using /home/user/.cache/torch_extensions as PyTorch extensions root...\n</code></pre> <p>there may be a leftover <code>.lock</code> file from an aborted build. Cleaning either the whole <code>.cache</code> or the extensions' sub-directory should force a clean build on the next attempt.</p>"},{"location":"polaris/data-science-workflows/applications/megatron-deepspeed/","title":"Megatron-DeepSpeed","text":"<p>We describe below the instructions for launching distributed training with Microsoft's Megatron-DeepSpeed and briefly describe some parallelism strategies and various optimizations that are supported.</p> <p>Note</p> <p>We maintain a forked version at <code>argonne-lcf/Megatron-DeepSpeed</code> that has some helper scripts for launching and setting various training options.</p>"},{"location":"polaris/data-science-workflows/applications/megatron-deepspeed/#setup","title":"Setup","text":"<ol> <li> <p>Load <code>conda</code> and activate base environment:</p> <pre><code># load conda + activate base env\nmodule load conda/2023-10-04 ; conda activate base\n</code></pre> </li> <li> <p>Clone    <code>argonne-lcf/Megatron-DeepSpeed</code>    and navigate into it:</p> <pre><code># clone + navigate into Megatron-DeepSpeed repo\ngit clone https://github.com/argonne-lcf/Megatron-DeepSpeed\ncd Megatron-DeepSpeed\n</code></pre> </li> <li> <p>Make virtual environment (on top of base conda):</p> <pre><code># make virtual environment (on top of base conda)\nmkdir -p venvs/polaris/2023-10-04\npython3 -m venv venvs/polaris/2023-10-04 --system-site-packages\nsource venvs/polaris/2023-10-04/bin/activate\n</code></pre> </li> <li> <p>Install missing dependency:</p> <pre><code># install *missing dependency\npython3 -m pip install \"git+https://github.com/saforem2/ezpz\"\n</code></pre> </li> <li> <p>Launch training:</p> <pre><code># ---- launch training -----------------------\n# - MODEL_SIZE_KEY: defined in ALCF/model.sh\n# - other args: defined in ALCF/args.sh\n# ---------------------------------------------\nMODEL_SIZE_KEY=\"GPT25B\" \\\n    SEQ_LEN=4096 \\ \n    USE_FLASH_ATTN_V2=1 \\\n    MICRO_BATCH=1 \\\n    GAS=1 \\\n    SP_TYPE=\"megatron\" \\\n    ZERO_STAGE=1 \\\n    ./ALCF/train-gpt3.sh\n</code></pre> </li> </ol>"},{"location":"polaris/data-science-workflows/applications/megatron-deepspeed/#helper-scripts","title":"Helper Scripts","text":"<code>ALCF/train-gpt3.sh</code> <p>Main entry point for training. This script will automatically source the rest of the required ALCF/*.sh scripts below</p> <code>ALCF/model.sh</code> <p>Contains some example model architectures for GPT3-style models</p> <code>ALCF/args.sh</code> <p>Logic for parsing / setting up runtime options for Megatron and DeepSpeed.</p> <code>ALCF/setup.sh</code> <p>Locate and activate virtual environment to be used, ensure MPI variables are set properly</p> <code>ALCF/launch.sh</code> <p>Identify available resources and build the command to be ran i.e. figure out how many: <code>{nodes, GPUs per node, GPUs total}</code>, to pass to <code>mpi{run,exec}</code> then, use this to build  <code>mpiexec &lt;mpiexec-args&gt; python3 pretrain_gpt.py</code></p>"},{"location":"polaris/data-science-workflows/containers/containers/","title":"Containers on Polaris","text":"<p>Polaris, powered by NVIDIA A100 GPUs, benefits from container-based workloads for seamless compatibility across NVIDIA systems. This guide details the use of containers on Polaris, including custom container creation, large-scale execution, and common pitfalls.</p>"},{"location":"polaris/data-science-workflows/containers/containers/#apptainer-setup","title":"Apptainer Setup","text":"<p>Polaris employs Apptainer (formerly known as Singularity) for container management. To set up Apptainer, run:</p> <pre><code>module use /soft/spack/gcc/0.6.1/install/modulefiles/Core\nmodule load apptainer\napptainer version #1.2.2\n</code></pre> <p>The Apptainer version on Polaris is 1.2.2. Detailed user documentation is available here</p>"},{"location":"polaris/data-science-workflows/containers/containers/#building-from-docker-or-argonne-github-container-registry","title":"Building from Docker or Argonne GitHub Container Registry","text":"<p>Containers on Polaris can be built by writing Dockerfiles on a local machine and then publish the container to DockerHub, or by directly building them on ALCF compute node by writing an Apptainer recipe file. If you prefer to use existing containers, you can pull them from various registries like DockerHub and run them on Polaris.</p> <p>Since Docker requires root privileges, which users do not have on Polaris, existing Docker containers must be converted to Apptainer. To build a Docker-based container on Polaris, use the following as an example:</p> <p><pre><code>qsub -I -A &lt;Project&gt; -q debug -l select=1 -l walltime=01:00:00 -l filesystems=home:eagle -l singularity_fakeroot=true\nexport HTTP_PROXY=http://proxy.alcf.anl.gov:3128\nexport HTTPS_PROXY=http://proxy.alcf.anl.gov:3128\nexport http_proxy=http://proxy.alcf.anl.gov:3128\nexport https_proxy=http://proxy.alcf.anl.gov:3128\nmodule use /soft/spack/gcc/0.6.1/install/modulefiles/Core\nmodule load apptainer\napptainer build --fakeroot pytorch:22.06-py3.sing docker://nvcr.io/nvidia/pytorch:22.06-py3\n</code></pre> You can find the latest prebuilt Nvidia PyTorch containers here.  The Tensorflow containers are here (though note that LCF doesn't prebuild the TF-1 containers typically).  You can search the full container registry here. For custom containers tailored for Polaris, visit ALCF's GitHub container registry</p> <p>Note: Currently container build and executions are only supported on the Polaris compute nodes</p>"},{"location":"polaris/data-science-workflows/containers/containers/#recipe-based-container-building","title":"Recipe-Based Container Building","text":"<p>As mentioned earlier, you can build Apptainer containers from recipe files. Instructions are available here. See available containers for more recipes. </p> <p>Note: You can also build custom recipes by bootstrapping from prebuilt images. For e.g the first two lines in a recipe to use our custom Tensorflow implementation would be <code>Bootstrap: oras</code> followed by <code>From: ghcr.io/argonne-lcf/tf2-mpich-nvidia-gpu:latest</code></p>"},{"location":"polaris/data-science-workflows/containers/containers/#running-containers-on-polaris","title":"Running Containers on Polaris","text":"<p>To run a container on Polaris you can use the submission script described here. Below is an explanation of the job submission script.</p> <pre><code>#!/bin/sh\n#PBS -l select=2:system=polaris\n#PBS -q debug\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:grand\n#PBS -A &lt;project_name&gt;\ncd ${PBS_O_WORKDIR}\necho $CONTAINER\n</code></pre> <p>We move to current working directory and enable network access at run time by setting the proxy. We also load apptainer.</p> <pre><code># SET proxy for internet access\nmodule use /soft/spack/gcc/0.6.1/install/modulefiles/Core\nmodule load apptainer\nexport HTTP_PROXY=http://proxy.alcf.anl.gov:3128\nexport HTTPS_PROXY=http://proxy.alcf.anl.gov:3128\nexport http_proxy=http://proxy.alcf.anl.gov:3128\nexport https_proxy=http://proxy.alcf.anl.gov:3128\n</code></pre> <p>This is important for system (Polaris - Cray) mpich to bind to containers mpich. Set the following environment variables</p> <pre><code>ADDITIONAL_PATH=/opt/cray/pe/pals/1.2.12/lib\nmodule load cray-mpich-abi\nexport APPTAINERENV_LD_LIBRARY_PATH=\"$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH:$ADDITIONAL_PATH\"\n</code></pre> <p>Set the number of ranks per node spread as per your scaling requirements</p> <pre><code># MPI example w/ 16 MPI ranks per node spread evenly across cores\nNODES=`wc -l &lt; $PBS_NODEFILE`\nPPN=16\nPROCS=$((NODES * PPN))\necho \"NUM_OF_NODES= ${NODES} TOTAL_NUM_RANKS= ${PROCS} RANKS_PER_NODE= ${PPN}\"\n</code></pre> <p>Finally launch your script</p> <pre><code>echo C++ MPI\nmpiexec -hostfile $PBS_NODEFILE -n $PROCS -ppn $PPN apptainer exec -B /opt -B /var/run/palsd/ $CONTAINER /usr/source/mpi_hello_world\n\necho Python MPI\nmpiexec -hostfile $PBS_NODEFILE -n $PROCS -ppn $PPN apptainer exec -B /opt -B /var/run/palsd/ $CONTAINER python3 /usr/source/mpi_hello_world.py\n</code></pre> <p>The job can be submitted using:</p> <pre><code>qsub -v CONTAINER=mpich-4_latest.sif job_submission.sh\n</code></pre>"},{"location":"polaris/data-science-workflows/containers/containers/#available-containers","title":"Available containers","text":"<p>If you just want to know what containers are available, here you go. </p> <ul> <li>For running mpich/MPI containers on Polaris, it can be found here</li> <li>For running databases on Polaris. It can be found here</li> <li>For using shpc - that allows for running containers as modules. It can be found here</li> </ul> <p>The latest containers are updated periodically. If you have trouble using containers, or request a newer or a different container please contact ALCF support at <code>support@alcf.anl.gov</code>.</p>"},{"location":"polaris/data-science-workflows/containers/containers/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":"<ol> <li> <p>Permission Denied Error: If you encounter permission errors during the build</p> <ul> <li>Check your quota and delete any unnecessary files. </li> <li>Clean-up apptainer cache, <code>~/.apptainer/cache</code>, and set the apptainer tmp and cache directories as below:     <pre><code>export APPTAINER_TMPDIR=/tmp/apptainer-tmpdir\nmkdir $APPTAINER_TMPDIR\nexport APPTAINER_CACHEDIR=/tmp/apptainer-cachedir/\nmkdir $APPTAINER_CACHEDIR\n</code></pre></li> <li>Make sure you are not on a directory accessed with a symlink, i.e. check if <code>pwd</code> and <code>pwd -P</code> returns the same path.</li> <li>If any of the above doesn't work, try running the build in your home directory.</li> </ul> </li> <li> <p>Mapping to rank 0 on all nodes: Ensure that the container's MPI aligns with the system MPI. Follow the additional steps outlined in the container registry documentation for MPI on Polaris</p> </li> <li> <p>libmpi.so.40 not found: This can happen if the container's application has an OpenMPI dependency which is not currently supported on Polaris. It can also spring up if the containers base environment is not debian architecture like Ubuntu. Ensure the application has an MPICH implementation as well. Also try removing .conda, .cache, and .local folders from your home directory and rebuild the container.</p> </li> </ol>"},{"location":"polaris/data-science-workflows/frameworks/deepspeed/","title":"DeepSpeed","text":"<p>The base <code>conda</code> environment on Polaris comes with Microsoft's DeepSpeed pre-installed. Instructions for using / cloning the base environment can be found here.</p> <p>A batch submission script for the following example is available here.</p> <p>We describe below the steps needed to get started with DeepSpeed on Polaris.</p> <p>We focus on the <code>cifar</code> example provided in the DeepSpeedExamples repository, though this approach should be generally applicable for running any model with DeepSpeed support.</p>"},{"location":"polaris/data-science-workflows/frameworks/deepspeed/#running-deepspeed-on-polaris","title":"Running DeepSpeed on Polaris","text":"<p>Note</p> <p>The instructions below should be ran directly from a compute node.</p> <p>Explicitly, to request an interactive job (from <code>polaris-login</code>): <pre><code>qsub -A &lt;project&gt; -q debug-scaling -l select=2 -l walltime=01:00:00 -I\n</code></pre></p> <p>Refer to job scheduling and execution for additional information.</p> <ol> <li> <p>Load <code>conda</code> module and activate base environment:</p> <pre><code>module load conda ; conda activate base\n</code></pre> </li> <li> <p>Clone    microsoft/DeepSpeedExamples    and navigate into the directory:</p> <pre><code>git clone https://github.com/microsoft/DeepSpeedExamples.git\ncd DeepSpeedExamples/cifar\n</code></pre> </li> </ol> <p>Launching DeepSpeed</p> Launching with MPICHLaunching with DeepSpeed <ol> <li> <p>Get total number of available GPUs:</p> <ol> <li>Count number of lines in <code>$PBS_NODEFILE</code> (1 host per line)</li> <li>Count number of GPUs available on current host</li> <li><code>NGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"</code> <pre><code>NHOSTS=$(wc -l &lt; \"${PBS_NODEFILE}\")\nNGPU_PER_HOST=$(nvidia-smi -L | wc -l)\nNGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"\n</code></pre></li> </ol> </li> <li> <p>Launch with <code>mpiexec</code>: <pre><code>mpiexec \\\n  --verbose \\\n  --envall \\\n  -n \"${NGPUS}\" \\\n  --ppn \"${NGPU_PER_HOST}\" \\\n  --hostfile=\"${PBS_NODEFILE}\" \\\n  python3 \\\n    cifar10_deepspeed.py \\\n    --deepspeed_config ds_config.json\n</code></pre></p> </li> </ol> <ol> <li> <p>Create a DeepSpeed compliant <code>hostfile</code>, specifying the <code>hostname</code> and    number of GPUs (<code>slots</code>) for each of our available workers: <pre><code>cat $PBS_NODEFILE &gt; hostfile\nsed -e 's/$/ slots=4/' -i hostfile\n</code></pre></p> </li> <li> <p>Create a <code>.deepspeed_env</code> containing the environment    variables our workers will need access to: <pre><code>echo \"PATH=${PATH}\" &gt;&gt; .deepspeed_env\necho \"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\" &gt;&gt; .deepspeed_env\necho \"http_proxy=${http_proxy}\" &gt;&gt; .deepspeed_env\necho \"https_proxy=${https_proxy}\" &gt;&gt; .deepspeed_env\n</code></pre></p> </li> </ol> <p>Warning</p> <p>The <code>.deepspeed_env</code> file expects each line to be of the form <code>KEY=VALUE</code>. Each of these will then be set as environment variables on each available worker specified in our <code>hostfile</code>.</p> <p>We can then run the <code>cifar10_deepspeed.py</code> module using DeepSpeed: <pre><code>deepspeed --hostfile=hostfile cifar10_deepspeed.py \\\n    --deepspeed \\\n    --deepspeed_config ds_config.json\n</code></pre></p> <code>AssertionError: Micro batch sizer per gpu: 0 has to be greater than 0</code> <p>Depending on the details of your specific job, it may be necessary to modify the provided <code>ds_config.json</code>.</p> <p>If you encounter an error: <pre><code>x3202c0s31b0n0: AssertionError: Micro batch size per gpu: 0 has to be greater than 0\n</code></pre> you can modify the <code>\"train_batch_size\": 16</code> variable in the provided <code>ds_config.json</code> to the (total) number of available GPUs, and explicitly set <code>\"gradient_accumulation_steps\": 1</code>, as shown below. <pre><code>$ export NHOSTS=$(wc -l &lt; \"${PBS_NODEFILE}\")\n$ export NGPU_PER_HOST=$(nvidia-smi -L | wc -l)\n$ export NGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"\n$ echo $NHOSTS $NGPU_PER_HOST $NGPUS\n24 4 96\n$ # replace \"train_batch_size\" with $NGPUS in ds_config.json\n$ # and write to `ds_config-polaris.json`\n$ sed \\\n    \"s/$(cat ds_config.json| grep batch | cut -d ':' -f 2)/ ${NGPUS},/\" \\\n    ds_config.json \\\n    &gt; ds_config-polaris.json\n$ cat ds_config-polaris.json\n{\n    \"train_batch_size\": 96,\n    \"gradient_accumulation_steps\": 1,\n    ...\n}\n</code></pre></p>"},{"location":"polaris/data-science-workflows/frameworks/jax/","title":"JAX","text":"<p>JAX is another popular python package for accelerated computing.  JAX is built on XLA (the same XLA TensorFlow uses) as well as AutoGrad, and additionally has acceleration tools that operate on functions such as <code>vmap</code>, <code>jit</code>, etc.  JAX is not as widespread in machine learning as TensorFlow and PyTorch for traditional models (Computer Vision, Language Models) though it is quickly gaining promienence.  JAX is very powerful when a program needs non-traditional autodifferentiation or vectorizatoin, such as: forward-mode AD, higher order derivatives, Jacobians, Hessians, or any combination of the above.  Users of JAX on Polaris are encouraged to read the user documentation in detail, particularly the details about pure-functional programming, no in-place operations, and the common mistakes in writing functions for the <code>@jit</code> decorator.</p>"},{"location":"polaris/data-science-workflows/frameworks/jax/#jax-on-polaris","title":"JAX on Polaris","text":"<p>JAX is installed on Polaris via the <code>conda</code> module, available with: <pre><code>module load conda; conda activate\n</code></pre></p> <p>Then, you can load JAX in <code>python</code> as usual (below showing results from the <code>conda/2022-07-19</code> module):</p> <pre><code>&gt;&gt;&gt; import jax\n&gt;&gt;&gt; jax.__version__\n'0.3.15'\n&gt;&gt;&gt;\n</code></pre>"},{"location":"polaris/data-science-workflows/frameworks/jax/#notes-on-jax-0315","title":"Notes on JAX 0.3.15","text":"<p>On Polaris, due to a bug, an environment variable must be set to use JAX on GPUs.  The following code will crash: <pre><code>import jax.numpy as numpy\na = numpy.zeros(1000)\n</code></pre> outputting an error that looks like: <pre><code>jaxlib.xla_extension.XlaRuntimeError: UNKNOWN: no kernel image is available for execution on the device\n</code></pre></p> <p>You can fix this by setting an environment variable: <pre><code>export XLA_FLAGS=\"--xla_gpu_force_compilation_parallelism=1\"\n</code></pre></p>"},{"location":"polaris/data-science-workflows/frameworks/jax/#scaling-jax-to-multiple-gpus-and-multiple-nodes","title":"Scaling JAX to multiple GPUs and multiple Nodes","text":"<p>Jax has intrinsic scaling tools to use multiple GPUs on a single node, via the <code>pmap</code> function.  If this is sufficient for your needs, excellent.  If not, another alternative is to use the newer package mpi4jax.</p> <p>mpi4Jax is a relatively new project and requires setting some environment variables for good performance and usability: - Set <code>MPI4JAX_USE_CUDA_MPI=1</code> to use CUDA-Aware MPI, supported in the <code>conda</code> module, to do operations directly from the GPU. - Set <code>MPICH_GPU_SUPPORT_ENABLED=1</code> to use CUDA-Aware MPI.</p> <p>The following code, based off of a test script from the mpi4jax repository, can help you verify you are using mpi4jax properly:</p> <pre><code>import os\nfrom mpi4py import MPI\nimport jax\nimport jax.numpy as jnp\nimport mpi4jax\n\ncomm = MPI.COMM_WORLD\nrank = comm.Get_rank()\nlocal_rank = int(os.environ[\"PMI_LOCAL_RANK\"])\n\navailable_devices = jax.devices(\"gpu\")\nif len(available_devices) &lt;= local_rank:\n    raise Exception(\"Could not find enough GPUs\")\n\ntarget_device = available_devices[local_rank]\n\n\n@jax.jit\ndef foo(arr):\n   arr = arr + rank\n   arr_sum, _ = mpi4jax.allreduce(arr, op=MPI.SUM, comm=comm)\n   return arr_sum\n\nwith jax.default_device(target_device):\n    a = jnp.zeros((3, 3))\n    print(f\"Rank {rank}, local rank {local_rank}, a.device is {a.device()}\")\n    result = foo(a)\n    print(f\"Rank {rank}, local rank {local_rank}, result.device is {result.device()}\")\n\n    import time\n    print(\"Sleeping for 5 seconds if you want to look at nvidia-smi ... \")\n    import time\n    time.sleep(5)\n    print(\"Done sleeping\")\n\nif rank == 0:\n   print(result)\n</code></pre> <p>JAX and mpi4jax are both still somewhat early in their software lifecycles.  Updates are frequent, and if you require assistance please contact support@alcf.anl.gov.</p>"},{"location":"polaris/data-science-workflows/frameworks/pytorch/","title":"PyTorch on Polaris","text":"<p>PyTorch is a popular, open source deep learning framework developed and released by Facebook.  The PyTorch home page has more information about PyTorch, which you can refer to.  For trouble shooting on Polaris, please contact support@alcf.anl.gov.</p>"},{"location":"polaris/data-science-workflows/frameworks/pytorch/#installation-on-polaris","title":"Installation on Polaris","text":"<p>PyTorch is installed on Polaris already, available in the <code>conda</code> module.  To use it from a compute node, please do:</p> <pre><code>module use /soft/modulefiles\nmodule load conda\nconda activate\n</code></pre> <p>Then, you can load PyTorch in <code>python</code> as usual (below showing results from the <code>conda/2022-07-19</code> module):</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.__version__\n'1.12.0a0+git67ece03'\n&gt;&gt;&gt;\n</code></pre> <p>This installation of PyTorch was built from source and the cuda libraries it uses are found via the <code>CUDA_HOME</code> environment variable (below showing results from the <code>conda/2022-07-19</code> module):</p> <pre><code>$ echo $CUDA_HOME\n/soft/datascience/cuda/cuda_11.5.2_495.29.05_linux\n</code></pre> <p>If you need to build applications that use this version of PyTorch and CUDA, we recommend using these cuda libraries to ensure compatibility.  We periodically update the PyTorch release, though updates will come in the form of new versions of the <code>conda</code> module.</p> <p>PyTorch is also available through nvidia containers that have been translated to Singularity containers.  For more information about containers, please see the containers documentation page.</p>"},{"location":"polaris/data-science-workflows/frameworks/pytorch/#pytorch-best-practices-on-polaris","title":"PyTorch Best Practices on Polaris","text":""},{"location":"polaris/data-science-workflows/frameworks/pytorch/#single-node-performance","title":"Single Node Performance","text":"<p>When running PyTorch applications, we have found the following practices to be generally, if not universally, useful and encourage you to try some of these techniques to boost performance of your own applications.</p> <ol> <li> <p>Use Reduced Precision. Reduced Precision is available on A100 via tensorcores and is supported with PyTorch operations.  In general, the way to do this is via the PyTorch Automatic Mixed Precision package (AMP), as descibed in the mixed precision documentation.  In PyTorch, users generally need to manage casting and loss scaling manually,  though context managers and function decorators can provide easy tools to do this.</p> </li> <li> <p>PyTorch has a <code>JIT</code> module as well as backends to support op fusion, similar to TensorFlow's <code>tf.function</code> tools.  However, PyTorch JIT capabilities are newer and may not yield performance improvements.  Please see TorchScript for more information.</p> </li> </ol>"},{"location":"polaris/data-science-workflows/frameworks/pytorch/#multi-gpu-multi-node-scale-up","title":"Multi-GPU / Multi-Node Scale up","text":"<p>PyTorch is compatible with scaling up to multiple GPUs per node, and across multiple nodes.  Good scaling performance has been seen up to the entire Polaris system, &gt; 2048 GPUs.  Good performance with PyTorch has been seen with both DDP and Horovod.  For details, please see the Horovod documentation or the Distributed Data Parallel documentation.  Some Polaris-specific details that may be helpful to you:</p> <ol> <li>CPU affinity and NCCL settings can improve scaling performance, particularly at the largest scales.  In particular, we encourage users to try their scaling measurements with the following settings:</li> <li>Set the environment variable <code>NCCL_COLLNET_ENABLE=1</code></li> <li>Set the environment varialbe <code>NCCL_NET_GDR_LEVEL=PHB</code></li> <li> <p>Manually set the CPU affinity via mpiexec, such as with <code>--cpu-bind verbose,list:0,8,16,24</code></p> </li> <li> <p>Horovod and DDP work best when you limit the visible devices to only one GPU.  Note that if you import <code>mpi4py</code> or <code>horovod</code>, and then do something like <code>os.environ[\"CUDA_VISIBLE_DEVICES\"] = hvd.local_rank()</code>, it may not actually work!  You must set the <code>CUDA_VISIBLE_DEVICES</code> environment variable prior to doing <code>MPI.COMM_WORLD.init()</code>, which is done in <code>horovod.init()</code> as well as implicitly in <code>from mpi4py import MPI</code>.   On Polaris specifically, you can use the environment variable <code>PMI_LOCAL_RANK</code> (as well as <code>PMI_LOCAL_SIZE</code>) to learn information about the node-local MPI ranks.  </p> </li> </ol>"},{"location":"polaris/data-science-workflows/frameworks/pytorch/#deepspeed","title":"DeepSpeed","text":"<p>DeepSpeed is also available and usable on Polaris.  For more information, please see the DeepSpeed documentation directly.</p>"},{"location":"polaris/data-science-workflows/frameworks/pytorch/#pytorch-dataloader-and-multi-node-horovod","title":"PyTorch <code>DataLoader</code> and multi-node Horovod","text":"<p>Please note there is a bug that causes a hang when using PyTorch's multithreaded data loaders with distributed training across multiple nodes. To workaround this, NVIDIA recommends setting <code>num_workers=0</code> in the dataloader configuration, which serializes data loading. </p> <p>For more details, see Polaris Known Issues.</p>"},{"location":"polaris/data-science-workflows/frameworks/tensorflow/","title":"TensorFlow on Polaris","text":"<p>TensorFlow is a popular, open-source deep learning framework developed and released by Google.  The TensorFlow home page has more information about TensorFlow, which you can refer to.  For trouble shooting on Polaris, please contact support@alcf.anl.gov.</p>"},{"location":"polaris/data-science-workflows/frameworks/tensorflow/#installation-on-polaris","title":"Installation on Polaris","text":"<p>TensorFlow is already pre-installed on Polaris, available in the <code>conda</code> module.  To use it from a compute node, please do:</p> <pre><code>module load conda\nconda activate\n</code></pre> <p>Then, you can load TensorFlow in <code>python</code> as usual (below showing results from the <code>conda/2022-07-19</code> module):</p> <pre><code>&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; tf.__version__\n'2.9.1'\n&gt;&gt;&gt;\n</code></pre> <p>This installation of TensorFlow was built from source and the CUDA libraries it uses are found via the <code>CUDA_HOME</code> environment variable (below showing results from the <code>conda/2022-07-19</code> module):</p> <pre><code>$ echo $CUDA_HOME\n/soft/datascience/cuda/cuda_11.5.2_495.29.05_linux\n</code></pre> <p>If you need to build applications that use this version of TensorFlow and CUDA, we recommend using these cuda libraries to ensure compatibility.  We periodically update the TensorFlow release, though updates will come in the form of new versions of the <code>conda</code> module.</p> <p>TensorFlow is also available through NVIDIA containers that have been translated to Singularity containers.  For more information about containers, please see the Containers documentation page.</p>"},{"location":"polaris/data-science-workflows/frameworks/tensorflow/#tensorflow-best-practices-on-polaris","title":"TensorFlow Best Practices on Polaris","text":""},{"location":"polaris/data-science-workflows/frameworks/tensorflow/#single-node-performance","title":"Single Node Performance","text":"<p>When running TensorFlow applications, we have found the following practices to be generally, if not universally, useful and encourage you to try some of these techniques to boost performance of your own applications.</p> <ol> <li> <p>Use Reduced Precision. Reduced Precision is available on A100 via tensorcores and is supported with TensorFlow operations.  In general, the way to do this is via the <code>tf.keras.mixed_precision</code> Policy, as descibed in the mixed precision documentation.  If you use a custom training loop (and not <code>keras.Model.fit</code>), you will also need to apply loss scaling.</p> </li> <li> <p>Use TensorFlow's graph API to improve efficiency of operations.  TensorFlow is, in general, an imperative language but with function decorators like <code>@tf.function</code> you can trace functions in your code.  Tracing replaces your python function with a lower-level, semi-compiled TensorFlow Graph. More information about the <code>tf.function</code> interface is available here.  When possible, use jit_compile, but be aware of sharp bits when using <code>tf.function</code>: python expressions that aren't tensors are often replaced as constants in the graph, which may or may not be your intention.</p> </li> <li> <p>Use XLA compilation on your code.  XLA is the Accelerated Linear Algebra library that is available in tensorFlow and critical in software like JAX.  XLA will compile a <code>tf.Graph</code> object, generated with <code>tf.function</code> or similar, and perform optimizations like operation-fusion.  XLA can give impressive performance boosts with almost no user changes except to set an environment variable <code>TF_XLA_FLAGS=--tf_xla_auto_jit=2</code>.  If your code is complex, or has dynamically sized tensors (tensors where the shape changes every iteration), XLA can be detrimental: the overhead for compiling functions can be large enough to mitigate performance improvements.  XLA is particularly powerful when combined with reduced precision, yielding speedups &gt; 100% in some models.</p> </li> </ol>"},{"location":"polaris/data-science-workflows/frameworks/tensorflow/#multi-gpu-multi-node-scale-up","title":"Multi-GPU / Multi-Node Scale up","text":"<p>TensorFlow is compatible with scaling up to multiple GPUs per node, and across multiple nodes.  Good scaling performance has been seen up to the entire Polaris system, &gt; 2048 GPUs.  Good performance with tensorFlow has been seen with horovod in particular.  For details, please see the Horovod documentation.  Some polaris specific details that may be helpful to you:</p> <ol> <li>CPU affinity and NCCL settings can improve scaling performance, particularly at the largest scales.  In particular, we encourage users to try their scaling measurements with the following settings:</li> <li>Set the environment variable <code>NCCL_COLLNET_ENABLE=1</code></li> <li>Set the environment varialbe <code>NCCL_NET_GDR_LEVEL=PHB</code></li> <li> <p>Manually set the CPU affinity via mpiexec, such as with <code>--cpu-bind verbose,list:0,8,16,24</code></p> </li> <li> <p>Horovod works best when you limit the visible devices to only one GPU.  Note that if you import <code>mpi4py</code> or <code>horovod</code>, and then do something like <code>os.environ[\"CUDA_VISIBLE_DEVICES\"] = hvd.local_rank()</code>, it may not actually work!  You must set the <code>CUDA_VISIBLE_DEVICES</code> environment variable prior to doing <code>MPI.COMM_WORLD.init()</code>, which is done in <code>horovod.init()</code> as well as implicitly in <code>from mpi4py import MPI</code>.   On Polaris specifically, you can use the environment variable <code>PMI_LOCAL_RANK</code> (as well as <code>PMI_LOCAL_SIZE</code>) to learn information about the node-local MPI ranks.  </p> </li> </ol>"},{"location":"polaris/data-science-workflows/frameworks/tensorflow/#tensorflow-dataloaders","title":"TensorFlow Dataloaders","text":"<p>Additional information to be provided.</p>"},{"location":"polaris/debugging-tools/CUDA-GDB/","title":"CUDA-GDB","text":""},{"location":"polaris/debugging-tools/CUDA-GDB/#references","title":"References","text":"<p>NVIDIA CUDA-GDB Documentation </p>"},{"location":"polaris/debugging-tools/CUDA-GDB/#introduction","title":"Introduction","text":"<p>CUDA-GDB is the NVIDIA tool for debugging CUDA applications running on Polaris. CUDA-GDB is an extension to GDB, the GNU Project debugger. The tool provides developers with a mechanism for debugging CUDA applications running on actual hardware. This enables developers to debug applications without the potential variations introduced by simulation and emulation environments.</p>"},{"location":"polaris/debugging-tools/CUDA-GDB/#step-by-step-guide","title":"Step-by-step guide","text":""},{"location":"polaris/debugging-tools/CUDA-GDB/#debug-compilation","title":"Debug Compilation","text":"<p>NVCC, the NVIDIA CUDA compiler driver, provides a mechanism for generating the debugging information necessary for CUDA-GDB to work properly. The -g -G option pair must be passed to NVCC when an application is compiled for ease of debugging with CUDA-GDB; for example, <pre><code>nvcc -g -G foo.cu -o foo\n</code></pre> Using this line to compile the CUDA application <code>foo.cu</code> * forces <code>-O0</code> compilation, with the exception of very limited dead-code eliminations and register-spilling optimizations. * makes the compiler include debug information in the executable</p>"},{"location":"polaris/debugging-tools/CUDA-GDB/#running-cuda-gdb-on-polaris-compute-nodes","title":"Running CUDA-gdb on Polaris compute nodes","text":"<p>Start an interactive job mode on Polaris as follows: <pre><code>$ qsub -I -l select=1 -l walltime=1:00:00\n\n$ cuda-gdb --version\nNVIDIA (R) CUDA Debugger\n11.4 release\nPortions Copyright (C) 2007-2021 NVIDIA Corporation\nGNU gdb (GDB) 10.1\nCopyright (C) 2020 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\n$ cuda-gdb foo\n</code></pre></p>"},{"location":"polaris/debugging-tools/CUDA-GDB/#a-quick-example-with-a-stream-benchmark-on-a-polaris-compute-node","title":"A quick example with a stream benchmark on a Polaris compute node","text":"<pre><code>jkwack@polaris-login-02:~&gt; qsub -I -l select=1 -l walltime=1:00:00\nqsub: waiting for job 308834.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov to start\nqsub: job 308834.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov ready\n\n\nCurrently Loaded Modules:\n  1) craype-x86-rome          4) perftools-base/22.05.0   7) cray-dsmml/0.2.2   10) cray-pmi-lib/6.0.17  13) PrgEnv-nvhpc/8.3.3\n  2) libfabric/1.11.0.4.125   5) nvhpc/21.9               8) cray-mpich/8.1.16  11) cray-pals/1.1.7      14) craype-accel-nvidia80\n  3) craype-network-ofi       6) craype/2.7.15            9) cray-pmi/6.1.2     12) cray-libpals/1.1.7\n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug&gt; nvcc -g -G -c ../src/cuda/CUDAStream.cu  -I ../src/\n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug&gt; nvcc -g -G -c ../src/main.cpp -DCUDA -I ../src/cuda/ -I ../src/\n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug&gt; nvcc -g -G main.o CUDAStream.o -o cuda-stream-debug\n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug&gt; ./cuda-stream-debug \nBabelStream\nVersion: 4.0\nImplementation: CUDA\nRunning kernels 100 times\nPrecision: double\nArray size: 268.4 MB (=0.3 GB)\nTotal size: 805.3 MB (=0.8 GB)\nUsing CUDA device NVIDIA A100-SXM4-40GB\nDriver: 11040\nFunction    MBytes/sec  Min (sec)   Max         Average     \nCopy        1313940.694 0.00041     0.00047     0.00047     \nMul         1302000.791 0.00041     0.00048     0.00047     \nAdd         1296217.720 0.00062     0.00070     0.00069     \nTriad       1296027.887 0.00062     0.00070     0.00069     \nDot         823405.227  0.00065     0.00076     0.00075     \n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug&gt; cuda-gdb ./cuda-stream-debug \nNVIDIA (R) CUDA Debugger\n11.4 release\nPortions Copyright (C) 2007-2021 NVIDIA Corporation\nGNU gdb (GDB) 10.1\nCopyright (C) 2020 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\nType \"show copying\" and \"show warranty\" for details.\nThis GDB was configured as \"x86_64-pc-linux-gnu\".\nType \"show configuration\" for configuration details.\nFor bug reporting instructions, please see:\n&lt;https://www.gnu.org/software/gdb/bugs/&gt;.\nFind the GDB manual and other documentation resources online at:\n    &lt;http://www.gnu.org/software/gdb/documentation/&gt;.\n\nFor help, type \"help\".\nType \"apropos word\" to search for commands related to \"word\"...\nReading symbols from ./cuda-stream-debug...\n(cuda-gdb) b CUDAStream.cu:203\nBreakpoint 1 at 0x412598: CUDAStream.cu:203. (2 locations)\n(cuda-gdb) r      \nStarting program: /home/jkwack/BabelStream/build_polaris_debug/cuda-stream-debug \n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib64/libthread_db.so.1\".\nBabelStream\nVersion: 4.0\nImplementation: CUDA\nRunning kernels 100 times\nPrecision: double\nArray size: 268.4 MB (=0.3 GB)\nTotal size: 805.3 MB (=0.8 GB)\n[Detaching after fork from child process 58459]\n[New Thread 0x15554c6bb000 (LWP 58475)]\nUsing CUDA device NVIDIA A100-SXM4-40GB\nDriver: 11040\n[New Thread 0x15554c4ba000 (LWP 58476)]\n[Switching focus to CUDA kernel 0, grid 5, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 3, lane 0]\n\nThread 1 \"cuda-stream-deb\" hit Breakpoint 1, triad_kernel&lt;double&gt;&lt;&lt;&lt;(32768,1,1),(1024,1,1)&gt;&gt;&gt; (a=0x155506000000, b=0x1554f6000000, c=0x1554e6000000)\n    at ../src/cuda/CUDAStream.cu:203\n203   a[i] = b[i] + scalar * c[i];\n(cuda-gdb) c\nContinuing.\n[Switching focus to CUDA kernel 0, grid 5, block (1,0,0), thread (0,0,0), device 0, sm 0, warp 32, lane 0]\n\nThread 1 \"cuda-stream-deb\" hit Breakpoint 1, triad_kernel&lt;double&gt;&lt;&lt;&lt;(32768,1,1),(1024,1,1)&gt;&gt;&gt; (a=0x155506000000, b=0x1554f6000000, c=0x1554e6000000)\n    at ../src/cuda/CUDAStream.cu:203\n203   a[i] = b[i] + scalar * c[i];\n(cuda-gdb) info locals\ni = 1024\n(cuda-gdb) p b[i]\n$1 = 0.040000000000000008\n(cuda-gdb) p scalar\n$2 = 0.40000000000000002\n(cuda-gdb) p c[i]\n$3 = 0.14000000000000001\n(cuda-gdb) d 1\n(cuda-gdb) c\nContinuing.\nFunction    MBytes/sec  Min (sec)   Max         Average     \nCopy        1314941.553 0.00041     0.00041     0.00041     \nMul         1301022.680 0.00041     0.00042     0.00041     \nAdd         1293858.147 0.00062     0.00063     0.00063     \nTriad       1297681.929 0.00062     0.00063     0.00062     \nDot         828446.963  0.00065     0.00066     0.00065     \n[Thread 0x15554c4ba000 (LWP 58476) exited]\n[Thread 0x15554c6bb000 (LWP 58475) exited]\n[Inferior 1 (process 58454) exited normally]\n(cuda-gdb) q\n\njkwack@x3008c0s13b1n0:~/BabelStream/build_polaris_debug&gt; \n</code></pre>"},{"location":"polaris/hardware-overview/machine-overview/","title":"Polaris","text":"<p>Polaris is a 560 node HPE Apollo 6500 Gen 10+ based system.  Each node has a single 2.8 GHz AMD EPYC Milan 7543P 32 core CPU with 512 GB of DDR4 RAM and four NVIDIA A100 GPUs connected via NVLink, a pair of local 1.6TB of SSDs in RAID0 for the users use, and a pair of Slingshot 11 network adapters.  There are two nodes per chassis, seven chassis per rack, and 40 racks for a total of 560 nodes.  More detailed specifications are as follows:</p>"},{"location":"polaris/hardware-overview/machine-overview/#polaris-compute-nodes","title":"Polaris Compute Nodes","text":"POLARIS COMPUTE DESCRIPTION PER NODE AGGREGATE Processor (Note 1) 2.8 GHz 7543P 1 560 Cores/Threads AMD Zen 3 (Milan) 32/64 17,920/35,840 RAM (Note 2) DDR4 512 GiB 280 TiB GPUS NVIDIA A100 4 2240 Local SSD 1.6 TB 2/3.2 TB 1120/1.8PB <p>Note 1: 256MB shared L3 cache, 512KB L2 cache per core, 32 KB L1 cache per core Note 2: 8 memory channels rated at 204.8 GiB/s</p>"},{"location":"polaris/hardware-overview/machine-overview/#polaris-a100-gpu-information","title":"Polaris A100 GPU Information","text":"DESCRIPTION A100 PCIe A100 HGX (Polaris) GPU Memory 40 GiB HBM2 160 GiB HBM2 GPU Memory BW 1.6 TB/s 6.4 TB/s Interconnect PCIe Gen4 64 GB/s NVLink 600 GB/s FP 64 9.7 TF 38.8 TF FP64 Tensor Core 19.5 TF 78 TF FP 32 19.5 TF 78 TF BF16 Tensor Core 312 TF 1.3 PF FP16 Tensor Core 312 TF 1.3 PF INT8 Tensor Core 624 TOPS 2496 TOPS Max TDP Power 250 W 400 W"},{"location":"polaris/hardware-overview/machine-overview/#polaris-device-affinity-information","title":"Polaris Device Affinity Information","text":"CPU Affinity NUMA Affinity GPU0 GPU1 GPU2 GPU3 mlx5_0 mlx5_1 24-31,56-63 3 GPU0 X NV4 NV4 NV4 SYS SYS 16-23,48-55 2 GPU1 NV4 X NV4 NV4 SYS PHB 8-15,40-47 1 GPU2 NV4 NV4 X NV4 SYS SYS 0-7,32-39 0 GPU3 NV4 NV4 NV4 X PHB SYS mlx5_0 SYS SYS SYS PHB X SYS mlx5_1 SYS PHB SYS SYS SYS X"},{"location":"polaris/hardware-overview/machine-overview/#legend","title":"Legend:","text":"<p>X    = Self SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI) NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU) PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge) PIX  = Connection traversing at most a single PCIe bridge NV#  = Connection traversing a bonded set of # NVLinks</p> <p>Links to detailed NVIDIA A100 documentation:     - NVIDIA A100 Tensor Core GPU Architecture     - NVIDIA Ampere Architecture In-Depth</p>"},{"location":"polaris/hardware-overview/machine-overview/#login-nodes","title":"Login nodes","text":"<p>There are four login nodes available to users for editing code, building code, submitting / monitoring jobs, checking usage (<code>sbank</code>), etc..  Their full hostnames are <code>polaris-login-N.hsn.cm.polaris.alcf.anl.gov</code>  for <code>N</code> equal to <code>01</code> through <code>04</code>; there are an additional two login nodes that are not user-accessible which are used for running services such as JupyterHub. The various compilers and libraries are present on the logins, so most users should be able to build their code.  However, if your build requires the physical presence of the GPU, you will need to build on a compute node.</p> <p>All users share the same login nodes so please be courteous and respectful of your fellow users.  For example, please do not run computationally or IO intensive pre- or post-processing on the logins and keep the parallelism of your builds to a reasonable level.</p> POLARIS LOGIN DESCRIPTION PER NODE AGGREGATE Processor (Note 1) 2.0 GHz 7713 2 12 Cores/Threads AMD Zen 3 (Milan) 128/256 768/1536 RAM (Note 2) DDR4 512 GiB 3 TiB GPUs (Note 3) No GPUs 0 0 Local SSD None 0 0 <p>Note 1: 256MB shared L3 cache, 512KB L2 cache per core, 32 KB L1 cache per core Note 2: 8 memory channels rated at 204.8 GiB/s per socket Note 3: If your build requires the physical presence of a GPU you will need to build on a compute node.</p>"},{"location":"polaris/hardware-overview/machine-overview/#gateway-nodes","title":"Gateway nodes","text":"<p>There are 50 gateway nodes.  These nodes are not user accessible, but are used transparently for access to the storage systems.  Each node has a single 200 Gbps HDR IB card for access to the storage area network.  This gives a theoretical peak bandwidth of 1250 GB/s which is approximately the aggregate bandwidth of the global file systems (1300 GB/s).</p>"},{"location":"polaris/hardware-overview/machine-overview/#storage","title":"Storage","text":"<p>Polaris has access to the ALCF global file systems.  Details on storage can be found here.</p>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/","title":"NVIDIA Nsight tools","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#references","title":"References","text":"<p>NVIDIA Nsight Systems Documentation NVIDIA Nsight Compute Documentation</p>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#introduction","title":"Introduction","text":"<p>NVIDIA\u00ae Nsight\u2122 Systems provides developers a system-wide visualization of an applications performance. Developers can optimize bottlenecks to scale efficiently across any number or size of CPUs and GPUs on Polaris. For further optimizations to compute kernels developers should use Nsight Compute.</p> <p>The NVIDIA Nsight Compute is an interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command line tool. </p> <p>In addition, the baseline feature of this tool allows users to compare results within the tool. NVIDIA Nsight Compute provides a customizable and data-driven user interface,  metric collection, and can be extended with analysis scripts for post-processing results.</p>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#step-by-step-guide","title":"Step-by-step guide","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#common-part-on-polaris","title":"Common part on Polaris","text":"<p>Build your application for Polaris, and then submit your job script to Polaris or start an interactive job mode on Polaris as follows: <pre><code>$ qsub -I -l select=1 -l walltime=1:00:00 -l filesystems=home:eagle -q debug -A &lt;project-name&gt;\n\n$ module li\n\nCurrently Loaded Modules:\n  1) nvhpc/23.9          5) cray-pmi/6.1.13      9) PrgEnv-nvhpc/8.5.0      13) darshan/3.4.4\n  2) craype/2.7.30       6) cray-pals/1.3.4     10) libfabric/1.15.2.0\n  3) cray-dsmml/0.2.2    7) cray-libpals/1.3.4  11) craype-network-ofi\n  4) cray-mpich/8.1.28   8) craype-x86-milan    12) perftools-base/23.12.0\n\n$ nsys --version\nNVIDIA Nsight Systems version 2023.3.1.92-233133147223v0\n\n$ ncu --version\nNVIDIA (R) Nsight Compute Command Line Profiler\nCopyright (c) 2018-2023 NVIDIA Corporation\nVersion 2023.2.1.0 (build 33050884) (public-release)\nNVIDIA Nsight Systems version 2022.4.2.1-df9881f\n</code></pre></p>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#nsight-systems","title":"Nsight Systems","text":"<p>Run your application with Nsight Systems as follows: <pre><code>$ nsys profile -o {output_filename} --stats=true ./{your_application}\n</code></pre></p> <p>Run your application on multiple nodes (e.g., 2 nodes) with Nsight Systems as follows: <pre><code>$ mpirun -n 8 --ppn 4 --env TMPDIR=/home/{user ID}/ nsys profile -o {output_filename}_%q{PMI_RANK} --stats=true ./{your_application}\n</code></pre></p>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#nsight-compute","title":"Nsight Compute","text":"<p>Run your application with Nsight Compute. <pre><code>$ ncu --set detailed -k {kernel_name} -o {output_filename} ./{your_application}\n</code></pre></p> <p>Remark: Without -o option, Nsight Compute provides performance data as a standard output</p>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#post-processing-the-profiled-data","title":"Post-processing the profiled data","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#post-processing-via-cli","title":"Post-processing via CLI","text":"<pre><code>$ nsys stats {output_filename}.qdrep\n$ ncu -i {output_filename}.ncu-rep  \n</code></pre>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#post-processing-on-your-local-system-via-gui","title":"Post-processing on your local system via GUI","text":"<ul> <li>Install NVIDIA Nsight Systems and NVIDIA Nsight Compute after downloading both of them from the  NVIDIA Developer Zone.  Remark: Local client version should be the same as or newer than NVIDIA Nsight tools on Polaris. </li> <li>Download nsys output files (i.e., ending with .qdrep and . sqlite) to your local system, and then open them with NVIDIA Nsight Systems on your local system.  </li> <li>Download ncu output files (i.e., ending with .ncu-rep) to your local system, and then open them with NVIDIA Nsight Compute on your local system.  </li> </ul>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#more-options-for-performance-analysis-with-nsight-systems-and-nsight-compute","title":"More options for performance analysis with Nsight Systems and Nsight Compute","text":"<pre><code>$ nsys --help\n$ ncu --help\n</code></pre>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#a-quick-example","title":"A quick example","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#nsight-systems_1","title":"Nsight Systems","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#running-a-stream-benchmark-with-nsight-systems","title":"Running a stream benchmark with Nsight Systems","text":"<pre><code>jkwack@x3008c0s13b1n0:~/BabelStream/build_polaris&gt; nsys profile -o JKreport-nsys-BableStream --stats=true ./cuda-stream\nWarning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\nCollecting data...\nBabelStream\nVersion: 4.0\nImplementation: CUDA\nRunning kernels 100 times\nPrecision: double\nArray size: 268.4 MB (=0.3 GB)\nTotal size: 805.3 MB (=0.8 GB)\nUsing CUDA device NVIDIA A100-SXM4-40GB\nDriver: 11040\nFunction    MBytes/sec  Min (sec)   Max         Average     \nCopy        1368294.603 0.00039     0.00044     0.00039     \nMul         1334324.779 0.00040     0.00051     0.00041     \nAdd         1358476.737 0.00059     0.00060     0.00059     \nTriad       1366095.332 0.00059     0.00059     0.00059     \nDot         1190200.569 0.00045     0.00047     0.00046     \nProcessing events...\nSaving temporary \"/var/tmp/pbs.308834.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov/nsys-report-f594-c524-6b4c-300a.qdstrm\" file to disk...\n\nCreating final output files...\nProcessing [===============================================================100%]\nSaved report file to \"/var/tmp/pbs.308834.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov/nsys-report-f594-c524-6b4c-300a.qdrep\"\nExporting 7675 events: [===================================================100%]\n\nExported successfully to\n/var/tmp/pbs.308834.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov/nsys-report-f594-c524-6b4c-300a.sqlite\n\n\nCUDA API Statistics:\n\n Time(%)  Total Time (ns)  Num Calls  Average (ns)  Minimum (ns)  Maximum (ns)  StdDev (ns)           Name         \n -------  ---------------  ---------  ------------  ------------  ------------  ------------  ---------------------\n    41.5      197,225,738        401     491,834.8       386,695       592,751      96,647.5  cudaDeviceSynchronize\n    35.4      168,294,004          4  42,073,501.0       144,211   167,547,885  83,649,622.0  cudaMalloc           \n    22.5      106,822,589        103   1,037,112.5       446,617    20,588,840   3,380,727.4  cudaMemcpy           \n     0.4        1,823,597        501       3,639.9         3,166        24,125       1,228.9  cudaLaunchKernel     \n     0.2        1,166,186          4     291,546.5       130,595       431,599     123,479.8  cudaFree             \n\n\n\nCUDA Kernel Statistics:\n\n Time(%)  Total Time (ns)  Instances  Average (ns)  Minimum (ns)  Maximum (ns)  StdDev (ns)                             Name                           \n -------  ---------------  ---------  ------------  ------------  ------------  -----------  ----------------------------------------------------------\n    24.5       58,415,138        100     584,151.4       582,522       585,817        543.0  void add_kernel&lt;double&gt;(const T1 *, const T1 *, T1 *)     \n    24.4       58,080,329        100     580,803.3       579,802       582,586        520.5  void triad_kernel&lt;double&gt;(T1 *, const T1 *, const T1 *)   \n    18.3       43,602,345        100     436,023.5       430,555       445,979      2,619.5  void dot_kernel&lt;double&gt;(const T1 *, const T1 *, T1 *, int)\n    16.5       39,402,677        100     394,026.8       392,444       395,708        611.5  void mul_kernel&lt;double&gt;(T1 *, const T1 *)                 \n    16.1       38,393,119        100     383,931.2       382,556       396,892      1,434.1  void copy_kernel&lt;double&gt;(const T1 *, T1 *)                \n     0.2          523,355          1     523,355.0       523,355       523,355          0.0  void init_kernel&lt;double&gt;(T1 *, T1 *, T1 *, T1, T1, T1)    \n\n\n\nCUDA Memory Operation Statistics (by time):\n\n Time(%)  Total Time (ns)  Count  Average (ns)  Minimum (ns)  Maximum (ns)  StdDev (ns)      Operation     \n -------  ---------------  -----  ------------  ------------  ------------  -----------  ------------------\n   100.0       61,323,171    103     595,370.6         2,399    20,470,146  3,439,982.0  [CUDA memcpy DtoH]\n\n\n\nCUDA Memory Operation Statistics (by size):\n\n Total (MB)  Count  Average (MB)  Minimum (MB)  Maximum (MB)  StdDev (MB)      Operation     \n ----------  -----  ------------  ------------  ------------  -----------  ------------------\n    805.511    103         7.820         0.002       268.435       45.361  [CUDA memcpy DtoH]\n\n\n\nOperating System Runtime API Statistics:\n\n Time(%)  Total Time (ns)  Num Calls  Average (ns)  Minimum (ns)  Maximum (ns)  StdDev (ns)        Name     \n -------  ---------------  ---------  ------------  ------------  ------------  ------------  --------------\n    85.9      600,896,697         20  30,044,834.9         3,477   100,141,768  42,475,064.1  poll          \n    13.5       94,610,402      1,201      78,776.4         1,002    11,348,375     402,562.6  ioctl         \n     0.2        1,374,312         79      17,396.4         3,486       434,715      48,015.2  mmap64        \n     0.1          877,705         51      17,209.9         1,031       748,723     104,491.6  fopen         \n     0.1          741,969         12      61,830.8        17,272       256,852      64,706.5  sem_timedwait \n     0.1          529,563        120       4,413.0         1,292        20,579       2,134.3  open64        \n     0.0          251,602          4      62,900.5        57,337        72,126       6,412.6  pthread_create\n     0.0           93,461         18       5,192.3         1,011        19,386       4,401.0  mmap          \n     0.0           37,621         11       3,420.1         1,302        11,672       2,867.6  munmap        \n     0.0           35,735          9       3,970.6         1,723         6,251       1,477.2  fgetc         \n     0.0           33,533          1      33,533.0        33,533        33,533           0.0  fgets         \n     0.0           26,832         13       2,064.0         1,452         3,366         542.6  write         \n     0.0           21,341          5       4,268.2         1,213         9,738       3,378.3  putc          \n     0.0           20,838          6       3,473.0         1,763         6,853       1,801.1  open          \n     0.0           17,016         10       1,701.6         1,523         1,834          96.9  read          \n     0.0           11,430          8       1,428.8         1,082         1,583         151.9  fclose        \n     0.0            6,202          1       6,202.0         6,202         6,202           0.0  pipe2         \n     0.0            5,961          2       2,980.5         2,254         3,707       1,027.4  socket        \n     0.0            5,670          2       2,835.0         2,795         2,875          56.6  fwrite        \n     0.0            5,481          1       5,481.0         5,481         5,481           0.0  connect       \n     0.0            5,279          2       2,639.5         1,743         3,536       1,267.8  fread         \n     0.0            1,082          1       1,082.0         1,082         1,082           0.0  bind          \n\nReport file moved to \"/home/jkwack/BabelStream/build_polaris/JKreport-nsys-BableStream.qdrep\"\nReport file moved to \"/home/jkwack/BabelStream/build_polaris/JKreport-nsys-BableStream.sqlite\"\n</code></pre>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#reviewing-the-nsight-systems-data-via-gui","title":"Reviewing the Nsight Systems data via GUI","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#nsight-compute_1","title":"Nsight Compute","text":""},{"location":"polaris/performance-tools/NVIDIA-Nsight/#running-a-stream-benchmark-with-nsight-compute-for-triad_kernel","title":"Running a stream benchmark with Nsight Compute for triad_kernel","text":"<pre><code>jkwack@x3008c0s13b1n0:~/BabelStream/build_polaris&gt; ncu --set detailed -k triad_kernel -o JKreport-ncu_detailed-triad_kernel-BableStream ./cuda-stream\nBabelStream\nVersion: 4.0\nImplementation: CUDA\nRunning kernels 100 times\nPrecision: double\nArray size: 268.4 MB (=0.3 GB)\nTotal size: 805.3 MB (=0.8 GB)\n==PROF== Connected to process 56600 (/home/jkwack/BabelStream/build_polaris/cuda-stream)\nUsing CUDA device NVIDIA A100-SXM4-40GB\nDriver: 11040\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\nFunction    MBytes/sec  Min (sec)   Max         Average     \nCopy        1331076.105 0.00040     0.00042     0.00041     \nMul         1304696.608 0.00041     0.00043     0.00042     \nAdd         1322600.587 0.00061     0.00062     0.00061     \nTriad       1327.700    0.60654     0.62352     0.61106     \nDot         850376.762  0.00063     0.00070     0.00065     \n==PROF== Disconnected from process 56600\n==PROF== Report: /home/jkwack/BabelStream/build_polaris/JKreport-ncu_detailed-triad_kernel-BableStream.ncu-rep\n</code></pre>"},{"location":"polaris/performance-tools/NVIDIA-Nsight/#reviewing-the-nsight-compute-data-via-gui","title":"Reviewing the Nsight Compute data via GUI","text":""},{"location":"polaris/programming-models/kokkos-polaris/","title":"Kokkos","text":""},{"location":"polaris/programming-models/kokkos-polaris/#kokkos_1","title":"Kokkos","text":"<p>Kokkos Core implements a programming model in C++ for writing performance portable applications targeting all major HPC platforms. For that purpose it provides abstractions for both parallel execution of code and data management. Kokkos is designed to target complex node architectures with N-level memory hierarchies and multiple types of execution resources. It currently can use Serial and OpenMP (threads) for CPU execution spaces (\"backends\") and CUDA, HIP, SYCL, and OpenMPTarget for GPU execution spaces. By convention, Kokkos only allows one GPU backend at a time.</p>"},{"location":"polaris/programming-models/kokkos-polaris/#kokkos-documentation","title":"Kokkos Documentation","text":"<ul> <li>Kokkos-core Wiki</li> <li>Kokkos github</li> </ul>"},{"location":"polaris/programming-models/kokkos-polaris/#kokkos-on-polaris","title":"Kokkos on Polaris","text":"<p>Following the Polaris upgrade to HPCM 1.10, the module setup to use the prebuilt Kokkos changed.</p> <p>The prebuilt Kokkos on polaris includes 3 backends: Serial and OpenMP for CPU execution and CUDA for GPU execution. To use it, run</p> <pre><code>module load craype-x86-milan\nmodule load craype-accel-nvidia80\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\nmodule use /soft/modulefiles\nmodule load cuda-PrgEnv-nvidia/12.2.91\nmodule load kokkos\n</code></pre> <p>This sets the following environment variables, some of which are used by <code>cmake</code>:</p> <ul> <li><code>KOKKOS_HOME</code> - path to the <code>lib64/</code>, <code>include/</code> files installed</li> <li><code>LIBRARY_PATH</code> - prepends <code>$KOKKOS_HOME/lib64</code> to this variable used by <code>cmake</code></li> <li><code>CPATH</code> - prepends <code>$KOKKOS_HOME/include</code> to this variable used by <code>cmake</code></li> <li><code>LD_LIBRARY_PATH</code> - prepends <code>$KOKKOS_HOME/lib64</code> to this variable</li> </ul>"},{"location":"polaris/programming-models/kokkos-polaris/#building-a-kokkos-application-using-cmake","title":"Building a Kokkos Application Using <code>cmake</code>","text":"<p>Add these lines to <code>CMakeLists.txt</code>:</p> <pre><code>find_package(Kokkos REQUIRED)\ntarget_link_libraries(myTarget Kokkos::kokkoscore)\n</code></pre> <p>Here is a simple example <code>CMakeLists.txt</code> to compile an example program:</p> <pre><code>cmake_minimum_required(VERSION 3.22)\nproject(buildExample)\nfind_package(Kokkos REQUIRED)\n\nset(buildExample_SOURCE_DIR \".\")\n\nset(top_SRCS\n  ${buildExample_SOURCE_DIR}/example1.cpp)\n\nset(SOURCE_FILES ${top_SRCS})\n\nadd_executable(example1_sycl_aot ${SOURCE_FILES})\ntarget_link_libraries(example1_sycl_aot Kokkos::kokkoscore)\ntarget_include_directories(example1_sycl_aot PUBLIC ${buildExample_SOURCE_DIR})\n</code></pre> <p>Configure and build it like this:</p> <pre><code>mkdir build\ncd build\ncmake -DCMAKE_CXX_COMPILER=CC -DCMAKE_C_COMPILER=cc ..\nmake\n</code></pre>"},{"location":"polaris/programming-models/kokkos-polaris/#building-a-kokkos-application-using-make","title":"Building a Kokkos Application Using <code>make</code>","text":"<p>Here's an example <code>Makefile</code>:</p> <pre><code># KOKKOS_HOME set via:\n#   module load kokkos\n\n# You can look at the first lines of $KOKKOS_HOME/KokkosConfigCommon.cmake to\n# see the flags used in cmake configuration of the kokkos library build. The\n# default Kokkos module on Polaris was built with PrgEnv-nvhpc and includes\n# Serial, OpenMP (threads) and CUDA backends. So you should have that\n# environment module loaded and include compiler flags for cuda and openmp:\n\n# Cray MPI wrapper for C++ and C compilers:\nCXX=CC\nCC=cc\n\nCPPFLAGS=-cuda -fopenmp\nLDFLAGS=\n\nLDFLAGS=$(CPPFLAGS) $(LDFLAGS)\nLDLIBS=-L$(KOKKOS_HOME)/lib64 -lkokkoscore -lkokkossimd -lpthread\n\nSRCS=example1.cpp\nOBJS=$(subst .cpp,.o,$(SRCS))\n\nall: example1_polaris\n\nexample1_polaris: $(OBJS)\n        $(CXX) $(LDFLAGS) -o example1_polaris $(OBJS) $(LDLIBS)\n\nexample1.o: example1.cpp\n\nclean:\n        rm -f $(OBJS)\n\ndistclean: clean\n        rm -f example1_polaris\n</code></pre>"},{"location":"polaris/programming-models/kokkos-polaris/#configuring-your-own-kokkos-build-on-polaris","title":"Configuring Your Own Kokkos Build on Polaris","text":"<p>Here are recommended environment settings and configuration to build your own kokkos libraries on Polaris:</p>"},{"location":"polaris/programming-models/kokkos-polaris/#environment","title":"Environment","text":"<p>To match what was done in the centrally-built kokkos associated with the modules discussed above, use the programming environment <code>PrgEnv-gnu</code>, and use the Cray wrapper <code>CC</code> as the C++ compiler. You'll also need to explicitly load the Cuda toolkit version 12.2.91 as shown:</p> <pre><code>module restore\nmodule load craype-x86-milan\nmodule load craype-accel-nvidia80\nmodule swap PrgEnv-nvhpc PrgEnv-gnu\nmodule use /soft/modulefiles\nmodule load cuda-PrgEnv-nvidia/12.2.91\nmodule load spack-pe-base cmake\n</code></pre>"},{"location":"polaris/programming-models/kokkos-polaris/#cmake-configuration","title":"CMake Configuration","text":"<p>This example builds three backends: OpenMP, Serial, and Cuda.</p> <pre><code>git clone git@github.com:kokkos/kokkos.git\ncd kokkos\nmkdir build\ncd build\n\ncmake\\\n -DCMAKE_BUILD_TYPE=RelWithDebInfo\\\n -DCMAKE_INSTALL_PREFIX=\"./install\"\\\n -DCMAKE_CXX_COMPILER=CC\\\n -DKokkos_ENABLE_OPENMP=ON\\\n -DKokkos_ENABLE_SERIAL=ON\\\n -DKokkos_ARCH_ZEN3=ON\\\n -DKokkos_ARCH_AMPERE80=ON\\\n -DKokkos_ENABLE_CUDA=ON\\\n -DKokkos_ENABLE_AGGRESSIVE_VECTORIZATION=ON\\\n -DKokkos_ENABLE_TESTS=OFF\\\n -DBUILD_TESTING=OFF\\\n -DKokkos_ENABLE_CUDA_LAMBDA=ON\\\n -DKokkos_ENABLE_IMPL_DESUL_ATOMICS=OFF\\\n -DCMAKE_CXX_STANDARD=17\\\n -DCMAKE_EXE_LINKER_FLAGS=-no-gcc-rpath\\\n ..\n\nmake -j8 install\n</code></pre>"},{"location":"polaris/programming-models/openmp-polaris/","title":"OpenMP","text":""},{"location":"polaris/programming-models/openmp-polaris/#overview","title":"Overview","text":"<p>The OpenMP API is an open standard for parallel programming. The specification document can be found here: https://www.openmp.org. The specification describes directives, runtime routines, and environment variables that allow an application developer to express parallelism (e.g. shared memory multiprocessing and device offloading). Many compiler vendors provide implementations of the OpenMP specification (https://www.openmp.org/specifications).</p>"},{"location":"polaris/programming-models/openmp-polaris/#setting-the-environment-to-use-openmp-on-polaris","title":"Setting the environment to use OpenMP on Polaris","text":"<p>Many of the programming environments available on Polaris have OpenMP support.</p> module OpenMP CPU support? OpenMP GPU support? PrgEnv-nvhpc yes yes llvm yes yes PrgEnv-gnu yes no PrgEnv-cray yes yes* <p>*Currently PrgEnv-cray is not recommended for OpenMP offload.</p> <p>By default, the PrgEnv-nvhpc module is loaded. To switch to other modules, you can use <code>module switch</code>.</p>"},{"location":"polaris/programming-models/openmp-polaris/#using-prgenv-nvhpc","title":"Using PrgEnv-nvhpc","text":"<p>This is loaded by default, so there's no need to load additional modules. You can confirm that it is loaded by running <code>module list</code> to check that PrgEnv-nvhpc is in the list.</p>"},{"location":"polaris/programming-models/openmp-polaris/#using-llvm","title":"Using LLVM","text":"<p>To use the LLVM module, load the following. <pre><code>module load mpiwrappers/cray-mpich-llvm\nmodule load cudatoolkit-standalone\n</code></pre></p> <p>See the the LLVM compiling page here for more information.</p>"},{"location":"polaris/programming-models/openmp-polaris/#using-prgenv-gnu","title":"Using PrgEnv-gnu","text":"<p>To switch from PrgEnv-nvhpc to PrgEnv-gnu you can run:</p> <pre><code>module switch PrgEnv-nvhpc PrgEnv-gnu\n</code></pre> <p>The gcc/gfortran on Polaris was not built with GPU support. To use OpenMP on the CPU, you need to unload craype-accel-nvidia80:</p> <pre><code>module unload craype-accel-nvidia80\n</code></pre>"},{"location":"polaris/programming-models/openmp-polaris/#using-prgenv-cray","title":"Using PrgEnv-cray","text":"<p>To switch from PrgEnv-nvhpc to PrgEnv-cray you can run:</p> <pre><code>module switch PrgEnv-nvhpc PrgEnv-cray\n</code></pre> <p>To use OpenMP on the CPU only, also unload craype-accel-nvidia80:</p> <pre><code>module unload craype-accel-nvidia80\n</code></pre> <p>To use OpenMP on the GPU, load cudatoolkit-standalone, although this is not recommended at the moment. <pre><code>module load cudatoolkit-standalone\n</code></pre></p>"},{"location":"polaris/programming-models/openmp-polaris/#building-on-polaris","title":"Building on Polaris","text":"<p>The following table shows what compiler and flags to use with which PrgEnv:</p> module compiler flags PrgEnv-nvhpc cc/CC/ftn (nvc/nvc++/nvfortran) -mp=gpu -gpu=cc80 llvm mpicc/mpicxx (clang/clang++) -fopenmp -fopenmp-targets=nvptx64-nvidia-cuda PrgEnv-gnu cc/CC/ftn (gcc/g++/gfortran) -fopenmp PrgEnv-cray cc/CC/ftn -fopenmp <p>For example to compile a simple code hello.cpp:</p>"},{"location":"polaris/programming-models/openmp-polaris/#for-prgenv-nvhpc-after-loading-the-modules-as-discussed-above-we-would-use","title":"For PrgEnv-nvhpc, after loading the modules as discussed above we would use:","text":"<pre><code>CC -mp=gpu -gpu=cc80 hello.cpp\nftn -mp=gpu -gpu=cc80 hello.F90\n</code></pre>"},{"location":"polaris/programming-models/openmp-polaris/#for-llvm-after-loading-the-modules-as-discussed-above","title":"For LLVM, after loading the modules as discussed above:","text":"<pre><code>mpicxx -fopenmp -fopenmp-targets=nvptx64-nvidia-cuda hello.cpp \n</code></pre>"},{"location":"polaris/programming-models/openmp-polaris/#for-prgenv-gnu-after-loading-the-modules-as-discussed-above-we-would-use","title":"For PrgEnv-gnu, after loading the modules as discussed above we would use:","text":"<pre><code>CC -fopenmp hello.cpp\nftn -fopenmp hello.F90\n</code></pre>"},{"location":"polaris/programming-models/openmp-polaris/#for-prgenv-cray-after-loading-the-modules-as-discussed-above-we-would-use","title":"For PrgEnv-cray, after loading the modules as discussed above we would use:","text":"<pre><code>CC -fopenmp hello.cpp\nftn -fopenmp hello.F90\n</code></pre>"},{"location":"polaris/programming-models/openmp-polaris/#running-on-polaris","title":"Running on Polaris","text":"<p>To run, you can run the produced executable or with mpiexec in a job script, and then submit the script to the Polaris queue, like:</p> <pre><code>$ cat submit.sh\n#!/bin/sh\n#PBS -l select=1:system=polaris\n#PBS -l walltime=0:30:00\n#PBS -q debug \n#PBS -A Catalyst\n#PBS -l filesystems=home:eagle\n\ncd ${PBS_O_WORKDIR}\n mpiexec -n 1 ./executable\n$ # submit to the queue:\n$ qsub -l select=1:system=polaris -l walltime=0:30:00 -l filesystems=home:eagle -q debug -A Catalyst ./submit.sh\n</code></pre> <p>In the above, having the PBS options in the script and on the command line is redundant, but we put it there to show both ways of launching. This submits the script to one node in the debug queue on Polaris, requesting 30 min and the eagle and home filesystems. It will charge project Catalyst for the time.</p> <p>More details for setting up the job script are in Job Scheduling and Execution section.</p>"},{"location":"polaris/programming-models/openmp-polaris/#example","title":"Example","text":"<pre><code>$ cat hello.cpp\n#include &lt;stdio.h&gt;\n#include &lt;omp.h&gt;\n\nint main( int argv, char** argc ) {\n\n  printf( \"Number of devices: %d\\n\", omp_get_num_devices() );\n\n  #pragma omp target\n  {\n    if( !omp_is_initial_device() )\n      printf( \"Hello world from accelerator.\\n\" );\n    else\n      printf( \"Hello world from host.\\n\" );\n  }\n  return 0;\n}\n\n$ cat hello.F90\nprogram  main\n  use omp_lib\n  implicit none\n  integer flag\n\n  write(*,*) \"Number of devices:\", omp_get_num_devices()\n\n  !$omp target map(from:flag)\n    if( .not. omp_is_initial_device() ) then\n      flag = 1\n    else\n      flag = 0\n   endif\n  !$omp end target\n\n   if( flag == 1 ) then\n      print *, \"Hello world from accelerator\"\n   else\n      print *, \"Hello world from host\"\n   endif\n\n end program main\n\n$ # To compile\n$ CC -mp=gpu -gpu=cc80 hello.cpp -o c_test\n$ ftn -mp=gpu -gpu=cc80 hello.F90 -o f_test\n\n$ # To run \n$ mpiexec -n 1 ./c_test\nNumber of devices: 4\nHello world from accelerator.\n$ mpiexec -n 1 ./f_test\n Number of devices:            4\n Hello world from accelerator\n</code></pre>"},{"location":"polaris/programming-models/sycl-polaris/","title":"SYCL","text":"<p>SYCL (pronounced \u2018sickle\u2019) is a royalty-free, cross-platform abstraction layer that enables code for heterogeneous processors to be written using standard ISO C++ with the host and kernel code for an application contained in the same source file.</p> <ul> <li>Specification: https://www.khronos.org/sycl/</li> <li>Source code of the compiler: https://github.com/intel/llvm</li> <li>ALCF Tutorial: https://github.com/argonne-lcf/sycltrain</li> </ul> <pre><code>module load oneapi/upstream\n</code></pre> <p>Note</p> <p>This module (compilers, libraries) gets built periodically from the latest open-source rather than releases. For more details on the release version of compiler, please find the details here. As such, these compilers will get new features and updates quickly that may break on occasion. Please submit any issues at the respective github repositories for the compilers and libraries.</p>"},{"location":"polaris/programming-models/sycl-polaris/#components","title":"Components","text":"<ul> <li>These are the list of components associated with this module</li> </ul> User Application Component Compilers DPC++ oneMKL Interfaces oneMKL oneDPL oneDPL SYCLomatic/DPCT dpct"},{"location":"polaris/programming-models/sycl-polaris/#dependencies","title":"Dependencies","text":"<ul> <li>SYCL programming model is supported through <code>oneapi</code> compilers that were built from source-code</li> <li>Loading this module switches the default programming environment to GNU and with the following dependencies</li> <li>PrgEnv-gnu</li> <li>cuda-PrgEnv-nvidia</li> <li>Environment variable is set when loading the module: <code>ONEAPI_DEVICE_SELECTOR=cuda:gpu</code></li> </ul>"},{"location":"polaris/programming-models/sycl-polaris/#example-how-to-use-sycl-with-mpi-and-openmp","title":"Example: How to use SYCL with MPI and OpenMP","text":"Toggle for SYCL example with OpenMP &amp; MPI for CPU-side <pre><code>#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;iostream&gt;\n#include &lt;iomanip&gt;\n#include &lt;string.h&gt;\n#include &lt;mpi.h&gt;\n#include &lt;sched.h&gt;\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;omp.h&gt;\n\n// SYCL port of https://code.ornl.gov/olcf/hello_jobstep\n// To compile: mpicxx -fsycl -fopenmp -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_80 hello_jobstep.cpp -o hello_jobstep.out\n\nint main(int argc, char *argv[]){\n\n  MPI_Init(&amp;argc, &amp;argv);\n\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &amp;size);\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);\n\n  char name[MPI_MAX_PROCESSOR_NAME];\n  int resultlength;\n  MPI_Get_processor_name(name, &amp;resultlength);\n\n  // If CUDA_VISIBLE_DEVICES is set, capture visible GPUs\n  const char* gpu_id_list;\n  const char* cuda_visible_devices = getenv(\"CUDA_VISIBLE_DEVICES\");\n  if(cuda_visible_devices == NULL){\n    gpu_id_list = \"N/A\";\n  }\n  else{\n    gpu_id_list = cuda_visible_devices;\n  }\n\n  // Find how many GPUs L0 runtime says are available\n  int num_devices = 0;\n  std::vector&lt;sycl::device&gt; sycl_all_devs = sycl::device::get_devices(sycl::info::device_type::gpu);\n  num_devices = sycl_all_devs.size();\n\n  int hwthread;\n  int thread_id = 0;\n\n  if(num_devices == 0){\n#pragma omp parallel default(shared) private(hwthread, thread_id)\n    {\n      thread_id = omp_get_thread_num();\n      hwthread = sched_getcpu();\n\n      printf(\"MPI %03d - OMP %03d - HWT %03d - Node %s\\n\",\n             rank, thread_id, hwthread, name);\n\n    }\n  }\n  else{\n\n    std::string busid = \"\";\n\n    std::string busid_list = \"\";\n    std::string rt_gpu_id_list = \"\";\n\n    // Loop over the GPUs available to each MPI rank\n    for(int i=0; i&lt;num_devices; i++){\n\n      // // Get the PCIBusId for each GPU and use it to query for UUID\n      busid = sycl_all_devs[i].get_info&lt;sycl::ext::intel::info::device::pci_address&gt;();\n      busid_list.append(busid);\n\n      // Concatenate per-MPIrank GPU info into strings for print\n      if(i &gt; 0) rt_gpu_id_list.append(\",\");\n      rt_gpu_id_list.append(std::to_string(i));\n    }\n\n#pragma omp parallel default(shared) private(hwthread, thread_id)\n    {\n#pragma omp critical\n      {\n        thread_id = omp_get_thread_num();\n        hwthread = sched_getcpu();\n\n        printf(\"MPI %03d - OMP %03d - HWT %03d - Node %s - RT_GPU_ID %s - GPU_ID %s - Bus_ID %s\\n\",\n               rank, thread_id, hwthread, name, rt_gpu_id_list.c_str(), gpu_id_list, busid_list.c_str());\n      }\n    }\n  }\n\n  MPI_Finalize();\n\n  return 0;\n}\n</code></pre> <p>Compile and Run <pre><code>$ mpiexec -n 4 --ppn 4 --env OMP_NUM_THREADS=1 ./set_affinity_gpu_polaris.sh ./hello_jobstep.out\n\nMPI 000 - OMP 000 - HWT 000 - Node x3200c0s37b0n0 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 0000:C7:00.0\nMPI 001 - OMP 000 - HWT 001 - Node x3200c0s37b0n0 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 0000:85:00.0\nMPI 003 - OMP 000 - HWT 003 - Node x3200c0s37b0n0 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID 0000:07:00.0\nMPI 002 - OMP 000 - HWT 002 - Node x3200c0s37b0n0 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 0000:46:00.0\n$ ./a.out\n</code></pre></p>"},{"location":"polaris/programming-models/sycl-polaris/#example-using-gpu-aware-mpi","title":"Example (using GPU-aware MPI)","text":"<pre><code>#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;mpi.h&gt;\n\n#include &lt;sycl/sycl.hpp&gt;\n\n// Modified from NERSC website:\n// https://docs.nersc.gov/development/programming-models/mpi\nint main(int argc, char *argv[]) {\n\n    int myrank, num_ranks;\n    double *val_device;\n    double *val_host;\n    char machine_name[MPI_MAX_PROCESSOR_NAME];\n    int name_len=0;\n\n    MPI_Init(&amp;argc, &amp;argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;myrank);\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;num_ranks);\n    MPI_Get_processor_name(machine_name, &amp;name_len);\n\n    sycl::queue q{sycl::gpu_selector_v};\n\n    std::cout &lt;&lt; \"Rank #\" &lt;&lt; myrank &lt;&lt; \" runs on: \" &lt;&lt; machine_name\n              &lt;&lt; \", uses device: \"\n              &lt;&lt; q.get_device().get_info&lt;sycl::info::device::name&gt;() &lt;&lt; \"\\n\";\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    int one=1;\n    val_host = (double *)malloc(one*sizeof(double));\n    val_device = sycl::malloc_device&lt;double&gt;(one,q);\n\n    const size_t size_of_double = sizeof(double);\n    *val_host = -1.0;\n    if (myrank != 0) {\n        std::cout &lt;&lt; \"I am rank \" &lt;&lt; myrank\n                  &lt;&lt; \" and my initial value is: \" &lt;&lt; *val_host &lt;&lt; \"\\n\";\n    }\n\n    if (myrank == 0) {\n        *val_host = 42.0;\n        q.memcpy(val_device,val_host,size_of_double).wait();\n        std::cout &lt;&lt; \"I am rank \" &lt;&lt; myrank\n                  &lt;&lt; \" and will broadcast value: \" &lt;&lt; *val_host &lt;&lt; \"\\n\";\n    }\n\n    MPI_Bcast(val_device, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    double check = 42.0;\n    if (myrank != 0) {\n        //Device to Host\n        q.memcpy(val_host,val_device,size_of_double).wait();\n        assert(*val_host == check);\n        std::cout &lt;&lt; \"I am rank \" &lt;&lt; myrank\n                  &lt;&lt; \" and received broadcast value: \" &lt;&lt; *val_host &lt;&lt; \"\\n\";\n    }\n\n    sycl::free(val_device,q);\n    free(val_host);\n\n    MPI_Finalize();\n\n    return 0;\n}\n</code></pre> <p>Load Modules</p> <pre><code>module load oneapi/upstream\nmodule load mpiwrappers/cray-mpich-oneapi-upstream\nmodule load craype-accel-nvidia80\nexport MPICH_GPU_SUPPORT_ENABLED=1\n</code></pre> <p>Compile and Run</p> <p><pre><code>$ mpicxx -L/opt/cray/pe/mpich/8.1.28/gtl/lib -lmpi_gtl_cuda -std=c++17 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_80 main.cpp\n$ mpiexec -n 2 --ppn 2 --depth=1 --cpu-bind depth ./set_affinity_gpu_polaris.sh ./a.out\n</code></pre> For further details regarding the arguments passed to <code>mpiexec</code> command shown above, please visit the Job Scheduling and Execution section. A simple example describing the details and execution of the <code>set_affinity_gpu_polaris.sh</code> file can be found here.</p> <p>Note: By default, there is no GPU-aware MPI library linking support.  The example above shows how the user can enable the linking by specifying the path to the GTL (GPU Transport Layer) library (<code>libmpi_gtl_cuda</code>) to the link line.</p>"},{"location":"polaris/programming-models/sycl-polaris/#oneapi-math-kernel-library-onemkl-interfaces","title":"oneAPI Math Kernel Library (oneMKL) Interfaces","text":"<p>oneMKL Interfaces is an open-source implementation of the oneMKL Data Parallel C++ (DPC++) interface according to the oneMKL specification. It works with multiple devices (backends) using device-specific libraries underneath.</p> <p>oneMKL is part of oneAPI. Various backend supported are shown below. More Information here.</p> User Application Third-Party Library cuBLAS oneMKL interface cuSOLVER cuRAND"},{"location":"polaris/programming-models/sycl-polaris/#example-using-onemklgemm","title":"Example (using onemkl::gemm)","text":"<p>The following snippet shows how to compile and run a SYCL code with oneMKL library. For instance, a GPU-based GEMM is performed using <code>mkl::gemm</code> API and the results are compared to a CPU-based GEMM performed using the traditional blas (e.g., AOCL-BLIS) library. <pre><code>#include &lt;limits&gt;\n#include &lt;random&gt;\n\n#include &lt;sycl/sycl.hpp&gt;\n\n#include &lt;oneapi/mkl.hpp&gt;  // ONEMKL GPU header\n#include &lt;cblas.h&gt;         // BLIS   CPU header\n\n// Matrix size constants\n#define SIZE 4800 // Must be a multiple of 8.\n#define M SIZE / 8\n#define N SIZE / 4\n#define P SIZE / 2\n\n//////////////////////////////////////////////////////////////////////////////////////////\n\nbool ValueSame(double a, double b) { return std::fabs(a - b) &lt; 1.0e-08; }\nint VerifyResult(double *c_A, double *c_B) {\n  bool MismatchFound = false;\n\n  for (size_t i = 0; i &lt; M; i++) {\n    for (size_t j = 0; j &lt; P; j++) {\n      if (!ValueSame(c_A[i * P + j], c_B[i * P + j])) {\n        std::cout &lt;&lt; \"fail - The result is incorrect for element: [\" &lt;&lt; i &lt;&lt; \", \" &lt;&lt; j\n                  &lt;&lt; \"], expected: \" &lt;&lt; c_A[i * P + j] &lt;&lt; \" , but got: \" &lt;&lt; c_B[i * P + j]\n                  &lt;&lt; std::endl;\n        MismatchFound = true;\n      }\n    }\n  }\n\n  if (!MismatchFound) {\n    std::cout &lt;&lt; \"SUCCESS - The results are correct!\" &lt;&lt; std::endl;\n    return 0;\n  } else {\n    std::cout &lt;&lt; \"FAIL - The results mis-match!\" &lt;&lt; std::endl;\n    return -1;\n  }\n}\n\n//////////////////////////////////////////////////////////////////////////////////////////\n\nint main() {\n  std::random_device rd;  // Will be used to obtain a seed for the random number engine\n  std::mt19937 gen(rd()); // Standard mersenne_twister_engine seeded with rd()\n  std::uniform_real_distribution&lt;&gt; dis(1.0, 2.0);\n\n  // C = alpha * op(A) * op(B)  + beta * C\n  oneapi::mkl::transpose transA = oneapi::mkl::transpose::nontrans;\n  oneapi::mkl::transpose transB = oneapi::mkl::transpose::nontrans;\n\n  // matrix data sizes\n  int m = M;\n  int n = P;\n  int k = N;\n\n  // leading dimensions of data\n  int ldA = k;\n  int ldB = n;\n  int ldC = n;\n\n  // set scalar fp values\n  double alpha = 1.0;\n  double beta = 0.0;\n\n  // 1D arrays on host side\n  double *A;\n  double *B;\n  double *C_host_onemkl, *C_cblas;\n\n  A = new double[M * N]{};\n  B = new double[N * P]{};\n  C_cblas = new double[M * P]{};\n  C_host_onemkl = new double[M * P]{};\n\n  // prepare matrix data with ROW-major style\n  // A(M, N)\n  for (size_t i = 0; i &lt; M; i++)\n    for (size_t j = 0; j &lt; N; j++)\n      A[i * N + j] = dis(gen);\n  // B(N, P)\n  for (size_t i = 0; i &lt; N; i++)\n    for (size_t j = 0; j &lt; P; j++)\n      B[i * P + j] = dis(gen);\n\n  std::cout &lt;&lt; \"Problem size: c(\" &lt;&lt; M &lt;&lt; \",\" &lt;&lt; P &lt;&lt; \") = a(\" &lt;&lt; M &lt;&lt; \",\" &lt;&lt; N &lt;&lt; \") * b(\" &lt;&lt; N\n            &lt;&lt; \",\" &lt;&lt; P &lt;&lt; \")\" &lt;&lt; std::endl;\n\n  // Resultant matrix: C_cblas\n  cblas_dgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, m, n, k, alpha, A, ldA, B, ldB, beta,\n              C_cblas, ldC);\n\n  // Resultant matrix: C_onemkl\n  sycl::queue q(sycl::property_list{sycl::property::queue::in_order{}});\n  std::cout &lt;&lt; \"Device: \" &lt;&lt; q.get_device().get_info&lt;sycl::info::device::name&gt;() &lt;&lt; std::endl &lt;&lt; std::endl;\n\n  double* A_dev        = sycl::malloc_device&lt;double&gt;(M*N, q);\n  double* B_dev        = sycl::malloc_device&lt;double&gt;(N*P, q);\n  double* C_dev_onemkl = sycl::malloc_device&lt;double&gt;(M*P, q);\n\n  q.memcpy(A_dev, A, (M*N) * sizeof(double));\n  q.memcpy(B_dev, B, (N*P) * sizeof(double));\n\n  auto gemm_event = oneapi::mkl::blas::column_major::gemm(q, transB, transA, n, m, k, alpha, B_dev, ldB, A_dev, ldA, beta, C_dev_onemkl, ldC);\n\n  q.memcpy(C_host_onemkl, C_dev_onemkl, (M*P) * sizeof(double));\n\n  q.wait();\n  std::cout &lt;&lt; \"Verify results between OneMKL &amp; CBLAS: \";\n  int result_cblas = VerifyResult(C_cblas, C_host_onemkl);\n\n  delete[] A;\n  delete[] B;\n  delete[] C_cblas;\n  delete[] C_host_onemkl;\n  sycl::free(A_dev, q);\n  sycl::free(B_dev, q);\n  sycl::free(C_dev_onemkl, q);\n  return result_cblas;\n}\n</code></pre></p> <p>Compile and Run</p> <p>The user would need to provide paths the math-libraris as shown below. Also please provide AOCL library for CPU GEMM by <code>module load aocl</code>. Environment variables <code>MKLROOT</code> is defined with <code>oneapi</code> module &amp; <code>AOCL_ROOT</code> is defined with <code>aocl</code> module. Note: Please pay attention to the linker options for AOCL &amp; oneMKL libraries. <pre><code>$ clang++ -std=c++17 -sycl-std=2020 -O3 -fsycl -fsycl-targets=nvptx64-nvidia-cuda -Xsycl-target-backend --cuda-gpu-arch=sm_80 -L$AOCL_ROOT/lib -lblis -L$MKLROOT/lib -lonemkl sycl_onemkl_gemm.cpp -o sycl_onemkl_gemm.out\n</code></pre></p>"},{"location":"polaris/visualization/ffmpeg/","title":"FFmpeg on Polaris","text":""},{"location":"polaris/visualization/ffmpeg/#note-ffmpeg-module-is-currently-missing-on-polaris-after-a-recent-upgrade-a-spack-build-of-ffmpeg-will-be-available-soon","title":"NOTE: FFmpeg module is currently missing on Polaris after a recent upgrade. A spack build of ffmpeg will be available soon","text":"<p>To use FFmpeg on Polaris first load the corresponding module:</p> <pre><code>module load ffmpeg\n</code></pre> <p>This is a typical command line to create a movie from a series of snapshots in PNG format:</p> <pre><code>ffmpeg -r 15 -i frames.%03d.png -r 25 -pix_fmt yuv420p movie.mp4\n</code></pre> <p>where:</p> <pre><code>-r 15 is the input frame rate. Experiment with values smaller than the output frame rate for longer movies.\n-r 25 is the output frame rate (use this value for standard 25 frames per second)\n-i frames.%03d.png reads the input frames in sequence\n-pix_fmt yuv420p is needed for movies to play in browsers\nmovie.mp4 is the resulting movie\n</code></pre>"},{"location":"polaris/visualization/imagemagick/","title":"ImageMagick on Polaris","text":"<p>To use ImageMagick on Polaris first load the corresponding module:</p> <pre><code>module use /soft/modulefiles\nmodule load spack-pe-base imagemagick\n</code></pre>"},{"location":"polaris/visualization/paraview-manual-launch/","title":"Manually launching a ParaView server on Polaris","text":"<p>Sometimes it is convenient to manually launch an instance of the ParaView server. In this section we will explain an alternative method to run the ParaView server on Polaris using an interactive job, where the user can launch the ParaView server from the command line interface.</p> <p>Note: this is a method better suited for experienced users. If you are just starting with ParaView, we recommend the client/server mode as your primary method for using this tool.</p>"},{"location":"polaris/visualization/paraview-manual-launch/#setting-up-paraview","title":"Setting up ParaView","text":"<p>From your local client select Connect, either from the File menu, or by clicking on the icon circled below:</p> <p> </p> <p>A new window will open where you can configure a server. Click on Add Server:</p> <p></p> <p>Give your server a name, select Client/Server, localhost, and a TCP port (8000 in this example)</p> <p></p> <p>Click \"Configure\". In the next window there is an option to set up how ParaView server will be launched, and the default is \"Manual\". Leave it on \"Manual\" and click \"Save\".</p> <p>You will use these settings when establishing the connection.</p>"},{"location":"polaris/visualization/paraview-manual-launch/#launching-the-paraview-server-on-polaris","title":"Launching the ParaView server on Polaris","text":"<p>You can launch an interactive session on Polaris compute nodes with the following command (adjust parameters as needed to match your allocation, desired number of nodes, queue, walltime, and filesystems):</p> <pre><code>qsub -l walltime=01:00:00 -l select=2 -A yourallocation -q debug -I -l filesystems=home:grand\n</code></pre> <p>When the job starts you will receive a prompt on your head node like this:</p> <pre><code>username@x3005c0s7b0n0:~&gt;\n</code></pre> <p>Make a note of the node hostname (<code>x3005c0s7b0n0</code> in the example above). You can also get this information from <code>qstat -fx jobID</code></p> <p>Now load the ParaView module</p> <pre><code>username@x3005c0s7b0n0:~&gt; module use /soft/modulefiles \nusername@x3005c0s7b0n0:~&gt; module load visualization/paraview/paraview-5.12.0-EGL\n</code></pre> <p>and launch the ParaView server with</p> <pre><code>srizzi@x3005c0s7b0n0:~&gt; mpirun -n 8 pvserver --server-port=8000\nWaiting for client...\nConnection URL: cs://x3005c0s7b0n0:8000\nAccepting connection(s): x3005c0s7b0n0:8000\n</code></pre> <p>In this case <code>pvserver</code> will be listening on TCP port 8000 of your head node. You can change this port if you want.</p>"},{"location":"polaris/visualization/paraview-manual-launch/#creating-a-tunnel-over-ssh","title":"Creating a tunnel over ssh","text":"<p>We need to establish an ssh tunnel to connect client to server. On your local machine open a new terminal and type:</p> <pre><code>ssh -v -N -L 8000:x3005c0s7b0n0:8000 polaris.alcf.anl.gov\n</code></pre> <p>where 8000 is a TCP port and <code>x3005c0s7b0n0</code> the name of your head node. Adjust these values accordingly.</p> <p>Among multiple lines with debug information,  you should see something like:</p> <pre><code>debug1: Local connections to LOCALHOST:8000 forwarded to remote address x3005c0s7b0n0:8000\n</code></pre> <p>Keep this terminal open for the duration of your session to keep the ssh tunnel active.</p> <p>Now you are ready to launch your ParaView client locally. Keep in mind that client and servers versions must match. The ParaView version currently deployed on Polaris is 5.12.0</p>"},{"location":"polaris/visualization/paraview-manual-launch/#connecting-to-paraview-server","title":"Connecting to ParaView server","text":"<p>Connect your ParaView client to the server configuration you created above. You can select Connect, either from the File menu, or the icon circled in the figure:</p> <p> </p> <p>and selecting the configuration you created in a previous step.</p> <p>The connection should point to:</p> <pre><code>localhost:8000\n</code></pre> <p>In the terminal where you launched the server you will see now that the connection is established. Note that ParaView may take a few seconds to connect. This is normal behavior.</p> <pre><code>username@x3005c0s7b0n0:~&gt; mpirun -n 8 pvserver --server-port=8000\nWaiting for client...\nConnection URL: cs://x3005c0s7b0n0:8000\nAccepting connection(s): x3005c0s7b0n0:8000\nClient connected.\n</code></pre> <p>At this point you can use ParaView normally.</p>"},{"location":"polaris/visualization/paraview-tutorial/","title":"ParaView Tutorial","text":""},{"location":"polaris/visualization/paraview-tutorial/#overview","title":"Overview","text":"<p>This tutorial is intended to be a hands-on resource for users interested in learning the basic concepts of ParaView. The examples can easily be run on a laptop, using the example data set provided.</p> <ul> <li>Tour of ParaView</li> <li>Show range of visualization methods</li> <li>Walk through various visualization techniques, hopefully illustrate how these can apply to your own data</li> <li>Feel for ParaView \"way\"</li> <li>Terminology and step-by-step process peculiar to ParaView, which may differ from other packages, e.g. VisIt</li> </ul> <p> </p> Bloodflow Visualization by Joe Insley, ALCF"},{"location":"polaris/visualization/paraview-tutorial/#data","title":"Data","text":"<p>The data used for this tutorial is: - Blood flow simulation data - Multiple data types   - Continuum data field (unstructured mesh, tetrahedral): fluid field, plasma   - Particle data (unstructured points): individual particles moving in the flow   - Red Blood Cells (RBC, unstructured mesh, triangle): mesh of the surface of an RBC     - Healthy     - Diseased - Generated using an integrated Nektar/LAMMPS simulation code - Courtesy of George Karniadakis and Leopold Grinberg of Brown University</p> <p>The data is available for download here (~27MB compressed, ~39MB uncompressed): Data set for ParaView Red Blood Cell Tutorial</p>"},{"location":"polaris/visualization/paraview-tutorial/#1-load-multi-component-dataset","title":"1. Load Multi-component Dataset","text":"<ul> <li>From the Filemenu, (you can also click the file folder icon, shown above) open each of the following data sets (select then click \"OK\")</li> <li>The files will then appear in the Pipeline Browser</li> <li>Click Apply in the Object Inspector</li> <li>You will need to do this one at a time:</li> <li>continuum...vtu</li> <li>particles...vtu</li> <li>rbc_...vtu</li> <li>bad_rbc...vtu Note: The \"...\" in the name, and the arrow in the file browser, indicates that there are multiple time steps for each of these files</li> </ul> With all of the default settings, you should see something like this"},{"location":"polaris/visualization/paraview-tutorial/#2-select-which-data-to-view","title":"2. Select which data to view","text":"<p>Let's start by looking at the continuum.000*data. This is an unstructured mesh that has velocity and count (density) values. - Hide other data sets using the Eyeball icon next their names in the Pipeline Browser.   - Black = visible, Grey = hidden - Select continuum.000*(name is highlighted) in the Pipeline Browser   - Click on the name to highlight it - When manipulating appearance or applying filters, these always affect the selected data set - Switch to the Display tab in the Object Inspector - Under Color by, select Velocity from the dropdown   - There is also a shortcut to Color by in the menu bar near the top of the GUI   - </p> <p> </p> Select which data to view"},{"location":"polaris/visualization/paraview-tutorial/#3-manipulating-the-color-map","title":"3. Manipulating the Color Map","text":"<p>To change the colors used to represent the Velocity: - Under Color byclick the Edit Color Map... button - On the Color Scale Editor window click the Choose Preset button - On the Preset Color Scales window, select: Blue to Red Rainbow, and click OK. Then click Close on the Color Scale Editor window - You can also create and save your own color maps</p> <p> </p> Manipulating the Color Map"},{"location":"polaris/visualization/paraview-tutorial/#4-data-representation","title":"4. Data Representation","text":"<p>In order to be able to see the particles and red blood cells inside the cylinder, we need to be able to see through it. If we scroll down a bit in the Object Inspector view: - Group of controls labeled Style - In the Representation dropdown, select Wireframe</p> <p> </p> Data Representation"},{"location":"polaris/visualization/paraview-tutorial/#5-generate-streamlines","title":"5. Generate Streamlines","text":"<ul> <li>ParaView enables the generation of different types of data from existing data sets in the Pipeline</li> <li>Streamlines: Generated from vectors of the flow field. These curves show the direction a fluid element will travel in at any point in time</li> <li>Make sure that the continuum.000*data is selected in the Pipeline Browser</li> <li>From the main menu select: Filters-&gt;Alphabetical-&gt;Stream Tracer, or click on the Stream Tracer icon from the menu bar</li> <li>In the Object Inspector make sure the Properties tab is selected.</li> <li>Scroll down to seeds, and change Seed Type to Line Source</li> <li>Click the Y Axis button to set the seed line to run along the Y axis.</li> <li>The default Resolution is set to 100. This will make things a bit cluttered, especially when we start adding in the other data, so let's reduce this to 25</li> <li>Click the Apply button</li> </ul> Generate Streamlines"},{"location":"polaris/visualization/paraview-tutorial/#6-streamlines-as-tubes","title":"6. Streamlines as Tubes","text":"<p>The streamlines are just that, lines. We can use the Tubes filter to represent them as 3D objects, rather than just lines. - With StreamTracer1selected in the Pipeline Browser, from the main menu select: Filters-&gt;Alphabetical-&gt;Tube - In the Object Inspector make sure the Properties tab is selected - The default value for the Radius is a bit too large for this data, let's set that value to 0 - Click the Apply button - Notice that the StreamLine1 object has automatically been hidden - There are many different ways to color these tubes - With Tubes1 selected, switch to the Display tab in the Object Inspector - The Color bydropdown lets you choose from a handful of different variables</p> <p> </p> Streamlines as Tubes"},{"location":"polaris/visualization/paraview-tutorial/#7-cutting-planes-slices","title":"7. Cutting Planes (Slices)","text":"<p>Now let's add some cutting plans, or slices, to see what the cross-section of the continuum data looks like. - Again, be sure that the <code>continuum.000*data</code> is selected in the Pipeline Browser - Filters-&gt;Alphabetical-&gt;Slice or Click on the Slice icon from the menu bar - In the Object Inspector make sure the Propertiestab is selected - At the bottom on the Object Inspector is a section titled Slice Offset Values. Here we can generate values for multiple slices to be made - First click the Delete All button to remove initial values - Next, click the New Range button. This will bring up an Add Range dialog box. - Set the number of Steps to 7. Click OK - Click the Apply button - With Slice1 selected in the Object Inspector, switch to the Display tab - Set Color by value to Velocity</p> <p> </p> Cutting Planes (Slices)"},{"location":"polaris/visualization/paraview-tutorial/#8-data-representation-opacity","title":"8. Data Representation: Opacity","text":"<p>Even with the continuum data represented as wireframe, there is still considerable occlusion of the interior structures. In order to further reduce this occlusion by the wireframe, we can make it more transparent.</p> <ul> <li>Again, be sure that the <code>continuum.000*data</code> is selected in the Pipeline Browser</li> <li>In the Object Inspector make sure the Display tab is selected</li> <li>In the Object Inspector there is a section titled Style</li> <li>Set Opacity to 0.2</li> </ul> <p> </p> Data Representation: Opacity"},{"location":"polaris/visualization/paraview-tutorial/#9-animating-simulation-data","title":"9. Animating Simulation Data","text":"<p>Since our data has multiple time steps, we can easily animate through them to see how the data changes over time.</p> <ul> <li>Simply click the Play button on the animation bar at the top of the GUI</li> <li>Pause to make it stop</li> <li>Loop: With this button toggled on, animation will repeat until stopped</li> </ul> <p> </p> Animating Simulation Data"},{"location":"polaris/visualization/paraview-tutorial/#10-animations","title":"10. Animations","text":"<p>Animations can be saved to disk as a movie file, to be played back later.</p> <ul> <li>From the main menu: File-&gt;Save Animation</li> <li>Animation Settings Dialog: Save Animation</li> <li>Files of type: AVI files (*.avi)</li> <li>Enter a name in File name:</li> <li>Click OK</li> <li>Movie can be played back with standard media players (Windows Media Player, QuickTime, VLC, etc.)</li> </ul> <p> </p> Animations"},{"location":"polaris/visualization/paraview-tutorial/#11-particles-as-glyphs","title":"11. Particles as Glyphs","text":"<p>Glyphs are another way of visually representing data where the attributes of a graphical element are dictated by attributes of the data.</p> <p>All of the particles are displayed as red points in the graphics window. There are ~39K particles in this particular data set, which makes the display a bit cluttered. In order to both filter some of these out, and create 3D representations for them, let's apply a glyph filter to this data.</p> <p>Now let's add some of our other data back into the scene. Let's start with the particle data.</p> <p>All of the particles are displayed as red points in the graphics window. There are ~39K particles in this particular data set, which makes the display rather cluttered. In order to both filter some of these out, and create 3D representations for them, we will apply the glyph filter to this data.</p> <p>Note: that the particles.000* is still visible.</p> <ul> <li>Unhide the <code>particles.000*data</code>: click Eye icon</li> <li>Select <code>particles.000*data</code>: click on name</li> <li>Filters-&gt;Alphabetical-&gt;Glyph or click on the Glyph icon from the menu bar</li> <li>Glyph Type: Sphere</li> <li>Radius:. 0.15</li> <li>Orient: Unchecked</li> <li>Scale Mode: off</li> <li>Set Scale Factor: 1 - Edit: Checked</li> <li>Maximum Number of Points: 3000</li> <li>Mask Points: Checked</li> <li>Random Mode: Unchecked</li> <li>Click the Apply button</li> <li>Since our goal was to unclutter the display, let's hide the particles.000*by toggling them off, by clicking on the Eye icon next to it in the Pipeline Browser</li> <li>Let's also switch to the Display tab in the Object Inspector, with Glyph1 selected, and change the Color by value to GlyphVector. Since the GlyphVector value is based on the velocity. We can Edit Color Map...and choose the same Blue to Red Rainbow preset that we previously chose for velocity</li> </ul> <p> </p> Particles as Glyphs"},{"location":"polaris/visualization/paraview-tutorial/#12-enter-red-blood-cells","title":"12. Enter: Red Blood Cells","text":"<p>Now let's add in both of the other data sets, which are polygonal meshes which make up Red Blood Cells (RBCs).</p> <p>These two data sets are essentially the same kind of data, so we can apply the same filters and make the same types of representation changes to each of them. However, some of the RBCs are marked by the simulation that generated them as healthy (rbc.000) and some of them are marked as diseased (bad_rbc.000).</p> <ul> <li>Unhide the rbc.000 and bad_rbc.000 data sets by clicking the Eye icon next to each of them to make them visible</li> </ul> <p> </p> Enter: Red Blood Cells"},{"location":"polaris/visualization/paraview-tutorial/#13-using-color-to-differentiate-data","title":"13. Using Color to Differentiate Data","text":"<p>To enable us to distinguish these two types of data from one other, we can vary their representations.</p> <p>One way to do this is by setting the color of the two data sets to different colors. Repeat this process for each of rbc.000 and bad_rbc.000, picking different colors.</p> <ul> <li>Select one of the rbc data sets in the Pipeline Browser</li> <li>Go to the Displaytab in the Object Inspector</li> <li>In the Color by:dropdown select Solid Color</li> <li>Click on the Set Solid Color... button</li> <li>Select a color from the Select Colordialog that appears</li> <li>Repeat for the other RBC data set, choosing a different color</li> </ul> <p> </p> Using Color to Differentiate Data"},{"location":"polaris/visualization/paraview-tutorial/#14-further-exploration-highlight-the-mesh","title":"14. Further Exploration: Highlight the Mesh","text":"<p>Change the representation of one of the RBC data sets.</p> <p>In this example, the continuum.000* data is also hidden to reduce confusion with showing multiple overlapping meshes.</p> <ul> <li>Select on of the RBC data sets</li> <li>Go to the Displaytab in the Object Inspector</li> <li>For the Representationselect Surface With Edges</li> <li>In the Edge Style section click on the Set Edge Color...button to select a different color from the Select Color dialog</li> </ul> <p> </p> Further Exploration: Highlight the Mesh"},{"location":"polaris/visualization/paraview-tutorial/#15-further-exploration-highlight-the-vertices","title":"15. Further Exploration: Highlight the Vertices","text":"<p>Add glyphs to illustrate the position of the vertices of one of the RBC data sets.</p> <ul> <li>Select one of the RBC data sets</li> <li>Select the Glyphfilter</li> <li>Since this filter was used recently, can also be found under: Filters-&gt;Recent-&gt;Glyph</li> <li>As in the earlier example, set the various configuration options for the glyph attributes </li> <li>Note: that this time, we want to show all of the vertices of the RBC, so we should uncheckthe Mask Points option</li> </ul> <p> </p> Further Exploration: Highlight the Vertices"},{"location":"polaris/visualization/paraview-tutorial/#16-further-exploration-color-by-variable","title":"16. Further Exploration: Color by Variable","text":"<p>Try playing around with the viewing options and representations of the other data objects.</p> <p>Change the: - Color by values - Opacity - Representation - Etc.</p> <p> </p> Further Exploration: Color by Variable"},{"location":"polaris/visualization/paraview-tutorial/#17-background-color","title":"17. Background Color","text":"<ul> <li>Background color is an important part of final visualization</li> <li>From the main menu choose: Edit-&gt;View Settings...</li> <li>Under General in the View Settings dialog box, select Choose Color</li> <li>Select Color: OK</li> <li>Apply, then OK</li> </ul> Background Color <p>This tutorial was developed with support from National Science Foundation Grant OCI-0904190, and from the Argonne Leadership Computing Facility at Argonne National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under contract DE-AC02-06CH11357.</p>"},{"location":"polaris/visualization/paraview/","title":"ParaView on Polaris","text":"<p>The recommended way of running ParaView on Polaris is in client/server mode. This consists of running the ParaView client on your local resource, and the ParaView server on the Polaris compute nodes. The ParaView client needs to first be installed on your local resource, and needs to match the version that you run on Polaris.</p> <p>There are multiple versions of ParaView installed on Polaris. To find the versions of ParaView currently available on Polaris run the following command on a login node:  <pre><code>module use /soft/modulefiles\nmodule avail paraview\n</code></pre></p> <p>Binary and source packages of the ParaView client for Linux, MacOS, and Windows are available from the ParaView Download Page. </p>"},{"location":"polaris/visualization/paraview/#connecting-to-the-paraview-server-on-polaris","title":"Connecting to the ParaView server on Polaris","text":"<p>This section describes how to launch the ParaView server on Polaris from a local ParaView client.</p>"},{"location":"polaris/visualization/paraview/#start-paraview-client","title":"Start ParaView Client","text":"<p>First, launch the ParaView client on your local resource. You will need to configure some server settings in the client. This initial set up should only need to be done once, and can be reused each time you want to run ParaView on Polaris.</p>"},{"location":"polaris/visualization/paraview/#server-configuration","title":"Server Configuration","text":""},{"location":"polaris/visualization/paraview/#1-select-connect","title":"1. Select Connect","text":"<p>From the ParaView client choose to connect to a server by either clicking on the \"Connect\" icon in the menu bar</p> <p> </p> <p>or selecting File-&gt;Connect from the main menu</p> <p></p>"},{"location":"polaris/visualization/paraview/#2-set-up-servers-first-time-only","title":"2. Set Up Servers (first time only)","text":"<p>The first time you want to run a server on Polaris and have it connect to your local ParaView client, you will need to set up a Server. Once this server is set up, you can reuse it each time you run the ParaView client with the ParaView server on Polaris.</p> <p>Kitware, the developers of ParaView, maintain a database of server configurations which you can retrieve through the ParaView client. In the File-&gt;Connect menu press the button named \"Fetch Servers\" and select POLARIS@ANL. Windows users should select \"windows to POLARIS@ANL\". Press \"Import Selected\"</p> <p></p>"},{"location":"polaris/visualization/paraview/#3-use-paraview","title":"3. Use ParaView","text":"<p>After the previous step, you can now select POLARIS@ANL in the File-&gt;Connect menu and press Connect</p> <p></p> <p>At this point a new window will pop up</p> <p></p> <p>There are a number of parameters that you must enter manually here:</p> <p>Xterm executable: the path of a terminal on your system. The figure shows the case of a Mac with XQuartz. You may need to change these values for Windows or Linux.</p> <p>SSH executable: the name of your ssh command. It may be different on Windows depending on the ssh client installed (i.e putty)</p> <p>Remote machine: leave this value at polaris.alcf.anl.gov</p> <p>Username: your ALCF user name</p> <p>ParaView version: the version of Paraview that you want to use. Verify first that this version is installed on the system (as described at the top of this document). You will also need to add a <code>-EGL</code> suffix.</p> <p>Example: <pre><code>5.12.0-EGL\n</code></pre></p> <p>Client port: it is safe to use the default value</p> <p>Server port: is is safe to use the default value</p> <p>Number of nodes to reserve: enter the number of Polaris compute nodes you want to use for your job</p> <p>Number of ranks per node: enter the number of ranks per node</p> <p>Number of minutes to reserve: the duration of your job in minutes</p> <p>Account: enter here the name of your ALCF allocation</p> <p>Queue: the name of the Polaris queue you would like to use (i.e: <code>debug</code> for small, quick jobs, <code>prod</code>, <code>preemptable</code>)</p> <p>File Systems: enter here the file systems you need for your job, separated with colons, no spaces. Keep in mind that your job may not run if one of these file systems is not available at that time, so enter these values carefully</p> <p>Job name: safe to use default value. The PBS scheduler will assign this name to your job</p> <p>Now you can press OK to establish the connection with a ParaView server on Polaris.</p> <p>An ssh connection will be established with a Polaris login node and a password will be requested in a terminal, similar to the process you normally use to connect and work on the system.</p> <p>After you enter your password, a job will be queued and you will see a window like this:</p> <p></p> <p>When the job is launched on the compute nodes, the previous window will go away and ParaView will show it is connected to Polaris in its Pipeline Browser:</p> <p></p> <p>At this point you can open datasets stored on the ALCF file systems and use ParaView normally.</p>"},{"location":"polaris/visualization/paraview/#additional-information","title":"Additional Information","text":"<ul> <li>ParaView Documentation</li> <li>ParaView Community Support</li> </ul>"},{"location":"polaris/visualization/visit/","title":"VisIt on Polaris","text":""},{"location":"polaris/visualization/visit/#getting-started","title":"Getting Started","text":"<p>The latest VisIt versions installed on Polaris are 3.3.3 and 3.4.0.</p> <p>Please note that at the time of this writing VisIt version 3.4.0 does not yet have a client for Mac available. </p> <p>Follow these steps to install VisIt on your local machine:</p> <ul> <li>Download and install VisIt for your local platform (MacOS, Windows, Linux). The version you download must match the server version installed on Polaris. Use this page</li> <li>Download the Polaris host profile for VisIt (you may need to right-click and choose \"Save link as...\" or \"Save target as...\")</li> <li>Copy this file to a file called ~/.visit/hosts/host_anl_polaris.xml on Mac or Linux. [ We need to also specify this path for for Windows]</li> </ul> <p>Note: VisIt allows the user to download host profiles for ANL, but all these settings are outdated. We are working with the VisIt developers to update the ANL host list.</p> <p>Additional information for using VisIt in client/server mode here</p>"},{"location":"polaris/visualization/visit/#running-visit","title":"Running VisIt","text":"<ul> <li>Start up VisIt on your local machine </li> <li>Click File -&gt; Open File and choose \"ANL Polaris\" from the \"Host\" dropdown</li> </ul> <ul> <li>You'll be prompted for your password; enter your ALCF authenticator app response</li> <li>When you open a selected file, it will launch a job on Polaris<ul> <li>You will need to specify the \"Bank\" (Project) to use when VisIt submits jobs to the queue on Polaris. Specify a project in the Options box.</li> <li>If your environment doesn't get sourced correctly with non-interactive SSH, you can set the default project to use under Options -&gt; Host profiles</li> </ul> </li> </ul> <pre><code>- **Note:** Don't change the contents of the \"Machine file\" field (it should be $PBS_NODEFILE)\n- **Note:** The default Launch Profile is set to serial.  We recommend leaving this setting in its default value, but using the parallel method to launch jobs on Polaris.\n- **Note:** Don't change the contents of \"launchMethod\". It must be `qsub/aprun` even though Polaris does not use `aprun`.\n- If you'd like to change other job parameters (like the number of processes, nodes, and walltime), you can do so. Please enter time in the format required by the PBS scheduler (i.e 1:00:00 for one hour)\n- If you'd like these changes to be used as your default, be sure to save them using Save Settings under the Options menu.\n</code></pre>"},{"location":"polaris/visualization/visit/#additional-information","title":"Additional Information","text":"<ul> <li>VisIt user manual</li> <li>VisIt wiki</li> </ul>"},{"location":"polaris/visualization/visualization/","title":"Visualization on Polaris","text":"<p>Starting in January 2024, Polaris will serve as the primary production resource for visualization and analysis.</p> <p>Below is a list of the available visualization tools along with links to their corresponding documentation.</p> <p>ParaView: ParaView is an open-source visualization engine that seamlessly integrates with your existing tools and workflows. It allows you to construct visualization pipelines for quick data analysis. Whether interactively exploring large datasets in 3D or performing batch processing programmatically, ParaView provides versatile capabilities. For additional information, visit the Kitware website.</p> <p>VisIt: VisIt is an open-source, interactive, and scalable visualization, animation, and analysis tool. Users can rapidly generate visualizations, animate them over time, apply various operators and mathematical expressions, and save resulting images and animations for presentations. VisIt supports a diverse range of visualization features, enabling users to view data, including scalar and vector fields, on 2D and 3D structured, adaptive, and unstructured meshes. Thanks to its customizable plugin design, VisIt can visualize data from over 120 different scientific data formats. For more information, check the VisIt project GitHub page.</p> <p>FFmpeg: FFmpeg is a complete solution to record, convert, and stream audio and video. For more information, visit the FFmpeg webpage</p> <p>ImageMagick: ImageMagick is a free, open-source software suite, used for editing and manipulating digital images. It can be used to create, edit, compose, or convert bitmap images, and supports a wide range of file formats, including JPEG, PNG, GIF, TIFF, and PDF. More information in the ImageMagick webpage. </p>"},{"location":"polaris/workflows/balsam/","title":"Balsam","text":"<p>Balsam is a Python-based workflow manager that helps users execute large numbers of jobs, potentially with interjob dependencies, track job outcomes, and manage postprocessing analysis. A Balsam Site runs on a node with access to the job scheduler, where it can submit and monitor jobs. Overall job state is aggregated on the Balsam Server, making job data from all Sites accessible from any individual site (or the user's laptop), via the command-line interface or the Python API. To get information on how to use the command line tool, you can type <code>balsam --help</code> in your shell.</p> <p>Full documentation for Balsam is available online.</p> <p>Balsam requires Python 3.7+. To install Balsam on Polaris, first set up a virtual Python environment:</p> <pre><code>module use /soft/modulefiles\nmodule load conda\nconda activate base\npython -m venv env\nsource env/bin/activate\npip install --upgrade pip\npip install --pre balsam\n</code></pre> <p>To use Balsam, users need an account on the Balsam server.  Users can get an account by contacting the ALCF Help Desk.  Once a user has an account, they can login and make a new site.  A Balsam site is a project space for your workflow. You will be prompted to select what machine (Polaris) you are working on when creating a new site:</p> <pre><code>balsam login\nbalsam site init -n new-site new-site\ncd new-site\nbalsam site start\n</code></pre> <p>See the Balsam documentation for full details.</p>"},{"location":"polaris/workflows/libensemble/","title":"libEnsemble","text":"<p>libEnsemble is a Python toolkit for running dynamic ensembles of calculations.</p> <p>Users provide generator and simulator functions to express their ensembles, where the generator can steer the ensemble based on previous results. These functions can portably submit external executables at any scale.</p> <p>System details are detected, and dynamic resource management is provided. This includes automatically detecting, assigning, and reassigning GPUs for ensemble members.</p> <p>libEnsemble can be used in a consistent manner on laptops, clusters, and supercomputers with minimal required dependencies.</p>"},{"location":"polaris/workflows/libensemble/#getting-libensemble-on-polaris","title":"Getting libEnsemble on Polaris","text":"<p>libEnsemble is provided on Polaris in the conda module:</p> <pre><code>module use /soft/modulefiles\nmodule load conda\nconda activate base\n</code></pre> <p>See the docs for more details on using python on Polaris.</p> Example: creating virtual environment and updating libEnsemble      E.g., to create a virtual environment that allows installation of     further packages with pip:      <pre><code>python -m venv /path/to-venv --system-site-packages\n. /path/to-venv/bin/activate\n</code></pre>      Where ``/path/to-venv`` can be anywhere you have write access.     For future uses just load the conda module and run the activate line.      You can also ensure you are using the latest version of libEnsemble:      <pre><code>pip install libensemble\n</code></pre>"},{"location":"polaris/workflows/libensemble/#libensemble-examples","title":"libEnsemble examples","text":"<p>For a very simple example of using libEnsemble see the Simple Introduction tutorial</p> <p>For an example that runs a small ensemble using a C application (offloading work to the GPU), see the GPU app tutorial. The required files for this tutorial can be found in this directory. A video demo is also available.</p>"},{"location":"polaris/workflows/libensemble/#job-submission","title":"Job Submission","text":"<p>libEnsemble runs on the compute nodes on Polaris using either Python's <code>multiprocessing</code> or <code>mpi4py</code>. The user can set the number of workers for maximum concurrency. libEnsemble will detect the nodes available from the PBS environment and use these for running simulations. Polaris supports running multiple concurrent simulations on each node if desired.</p> <p>A simple example batch script for a libEnsemble use case that runs five workers on one node:</p> <pre><code>    #!/bin/bash -l\n    #PBS -l select=1:system=polaris\n    #PBS -l walltime=00:15:00\n    #PBS -l filesystems=home:grand\n    #PBS -q debug\n    #PBS -A &lt;myproject&gt;\n\n    export MPICH_GPU_SUPPORT_ENABLED=1\n    cd $PBS_O_WORKDIR\n    python run_libe_forces.py --comms local --nworkers 5\n</code></pre> <p>The script can be run with:</p> <pre><code>qsub submit_libe.sh\n</code></pre> <p>Or you can run an interactive session with:</p> <pre><code>qsub -A &lt;myproject&gt; -l select=1 -l walltime=15:00 -lfilesystems=home:grand -qdebug -I\n</code></pre>"},{"location":"polaris/workflows/libensemble/#further-links","title":"Further links","text":"<p>Docs: https://libensemble.readthedocs.io  GitHub: https://github.com/Libensemble/libensemble</p>"},{"location":"polaris/workflows/mig-compute/","title":"Multi-Instance GPU (MIG) mode","text":"<p>MIG mode can be enabled and configured on Polaris by passing a valid configuration file to <code>qsub</code>:</p> <p>qsub ... -l mig_config=/home/ME/path/to/mig_config.json ...</p> <p>You can find a concise explanation of MIG concepts and terms at https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#concepts</p>"},{"location":"polaris/workflows/mig-compute/#configuration","title":"Configuration","text":"<p>Please study the following example of a valid configuration file:</p> <pre><code>{\n  \"group1\": {\n    \"gpus\": [0,1],\n    \"mig_enabled\": true,\n    \"instances\": {\"7g.40gb\": [\"4c.7g.40gb\", \"3c.7g.40gb\"] }\n  },\n  \"group2\": {\n    \"gpus\": [2,3],\n    \"mig_enabled\": true,\n    \"instances\": {\"3g.20gb\": [\"2c.3g.20gb\", \"1c.3g.20gb\"], \"2g.10gb\": [\"2g.10gb\"], \"1g.5gb\": [\"1g.5gb\"], \"1g.5gb\": [\"1g.5gb\"]}\n  }\n}\n</code></pre>"},{"location":"polaris/workflows/mig-compute/#notes","title":"Notes","text":"<ul> <li>Group names are arbitrary, but must be unique</li> <li><code>\"gpus\"</code> must be an array of integers.  if only one physical gpu is being configured in a group, it must still be contained within an array(ex. <code>\"gpus\": [0],</code>)</li> <li>Only groups with <code>mig_enabled</code> set to <code>true</code> will be configured</li> <li><code>instances</code> denote the MIG gpu instances and the nested compute instances you wish to be configured</li> <li>syntax is <code>{\"gpu instance 1\": [\"cpu instance 1\", \"cpu instance 2\"], ...}</code></li> <li>valid gpu instances are <code>1g.5gb</code>, <code>1g.10gb</code>, <code>2g.10gb</code>, <code>3g.20gb</code>, <code>4g.20gb</code>, and <code>7g.40gb</code>.  the first number denotes the number of slots used out of 7 total, and the second number denotes memory in GB</li> <li>the default cpu instance for any gpu instance has the same identifier as the gpu instance(in which case it will be the only one configurable)</li> <li>other cpu instances can be configured with the identifier syntax <code>Xc.Y</code>, where <code>X</code> is the number of slots available in that gpu instance, and <code>Y</code> is the gpu instance identifier string</li> <li>some gpu instances cannot be configured adjacently, despite there being sufficient slots/memory remaining(ex. <code>3g.20gb</code> and <code>4g.20gb</code>). Please see NVIDIA MIG documentation for further details</li> <li>Currently, MIG configuration is only available in the debug, debug-scaling, and preemptable queues.  submissions to other queues will result in any MIG config files passed being silently ignored</li> <li>Files which do not match the above syntax will be silently rejected, and any invalid configurations in properly formatted files will be silently ignored. Please test any changes to your configuration in an interactive job session before use</li> <li>A basic validator script is available at <code>/soft/pbs/mig_conf_validate.sh</code>. It will check for simple errors in your config, and print the expected configuration. For example:</li> </ul> <pre><code>ascovel@polaris-login-02:~&gt; /soft/pbs/mig_conf_validate.sh -h\nusage: mig_conf_validate.sh -c CONFIG_FILE\nascovel@polaris-login-02:~&gt; /soft/pbs/mig_conf_validate.sh -c ./polaris-mig/mig_config.json\nexpected MIG configuration:\nGPU     GPU_INST   COMPUTE_INST\n-------------------------------\n0       7g.40gb    4c.7g.40gb\n0       7g.40gb    3c.7g.40gb\n1       7g.40gb    4c.7g.40gb\n1       7g.40gb    3c.7g.40gb\n2       2g.10gb    2g.10gb\n2       4g.20gb    2c.4g.20gb\n2       4g.20gb    2c.4g.20gb\n3       2g.10gb    2g.10gb\n3       4g.20gb    2c.4g.20gb\n3       4g.20gb    2c.4g.20gb\nascovel@polaris-login-02:~&gt;\n</code></pre>"},{"location":"polaris/workflows/mig-compute/#example-use-of-mig-compute-instances","title":"Example use of MIG compute instances","text":"<p>The following example demonstrates the use of MIG compute instances via the <code>CUDA_VISIBLE_DEVICES</code> environment variable:</p> <pre><code>ascovel@polaris-login-02:~/polaris-mig&gt; qsub -l mig_config=/home/ascovel/polaris-mig/mig_config.json -l select=1 -l walltime=60:00 -l filesystems=home:grand:swift -A Operations -q R639752 -k doe -I\nqsub: waiting for job 640002.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov to start\nqsub: job 640002.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov ready\n\nascovel@x3209c0s19b0n0:~&gt; cat ./polaris-mig/mig_config.json\n{\n  \"group1\": {\n    \"gpus\": [0,1],\n    \"mig_enabled\": true,\n    \"instances\": {\"7g.40gb\": [\"4c.7g.40gb\", \"3c.7g.40gb\"] }\n  },\n  \"group2\": {\n    \"gpus\": [2,3],\n    \"mig_enabled\": true,\n    \"instances\": {\"4g.20gb\": [\"2c.4g.20gb\", \"2c.4g.20gb\"], \"2g.10gb\": [\"2g.10gb\"] }\n  }\n}\nascovel@x3209c0s19b0n0:~&gt; nvidia-smi -L | grep -Po -e \"MIG[0-9a-f\\-]+\"\nMIG-63aa1884-acb8-5880-a586-173f6506966c\nMIG-b86283ae-9953-514f-81df-99be7e0553a5\nMIG-79065f64-bdbb-53ff-89e3-9d35f270b208\nMIG-6dd56a9d-e362-567e-95b1-108afbcfc674\nMIG-76459138-79df-5d00-a11f-b0a2a747bd9e\nMIG-4d5c9fb3-b0e3-50e8-a60c-233104222611\nMIG-bdfeeb2d-7a50-5e39-b3c5-767838a0b7a3\nMIG-87a2c2f3-d008-51be-b64b-6adb56deb679\nMIG-3d4cdd8c-fc36-5ce9-9676-a6e46d4a6c86\nMIG-773e8e18-f62a-5250-af1e-9343c9286ce1\nascovel@x3209c0s19b0n0:~&gt; for mig in $( nvidia-smi -L | grep -Po -e \"MIG[0-9a-f\\-]+\" ) ; do CUDA_VISIBLE_DEVICES=${mig} ./saxpy &amp; done 2&gt;/dev/null\nascovel@x3209c0s19b0n0:~&gt; nvidia-smi | tail -n 16\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0    0    0      17480      C   ./saxpy                          8413MiB |\n|    0    0    1      17481      C   ./saxpy                          8363MiB |\n|    1    0    0      17482      C   ./saxpy                          8413MiB |\n|    1    0    1      17483      C   ./saxpy                          8363MiB |\n|    2    1    0      17484      C   ./saxpy                          8313MiB |\n|    2    1    1      17485      C   ./saxpy                          8313MiB |\n|    2    5    0      17486      C   ./saxpy                          8313MiB |\n|    3    1    0      17487      C   ./saxpy                          8313MiB |\n|    3    1    1      17488      C   ./saxpy                          8313MiB |\n|    3    5    0      17489      C   ./saxpy                          8313MiB |\n+-----------------------------------------------------------------------------+\nascovel@x3209c0s19b0n0:~&gt;\n</code></pre>"},{"location":"polaris/workflows/parsl/","title":"Parsl on Polaris","text":"<p>Parsl is a flexible and scalable parallel programming library for Python.</p> <p>-- Parsl Documentation</p> <p>For many applications, managing an ensemble of jobs into a workflow is a critical step that can easily become a performance bottleneck.  Many tools exist to address this, of which <code>parsl</code> is just one.  On this page, we'll highlight some of the key pieces of information about <code>parsl</code> that are relevant to Polaris.  <code>Parsl</code> is also extensively documented, has a dedicated Slack Channel, and a large community of users and developers beyond ALCF.  We encourage you to engage with the <code>parsl</code> community for support with <code>parsl</code> specific questions, and for Polaris-specific questions or problems, please contact support@alcf.anl.gov.</p>"},{"location":"polaris/workflows/parsl/#getting-parsl-on-polaris","title":"Getting Parsl on Polaris","text":"<p>You can install parsl building off of the <code>conda</code> modules.  You have some flexibility in how you want to extend the <code>conda</code> module to include parsl, but here is an example way to do it:</p> <pre><code># Load the Conda Module (needed everytime you use parsl)\nmodule use /soft/modulefiles\nmodule load conda\nconda activate\n\n# Create a virtual env that uses the conda env as the system packages.\n# Only do the next line on initial set up:\npython -m venv --system-site-packages /path/to/your/virtualenv\n\n# Load the virtual env (every time):\nsource /path/to/your/virtualenv/bin/activate\n\n# Install parsl (only once)\npip install parsl\n</code></pre>"},{"location":"polaris/workflows/parsl/#using-parsl-on-polaris","title":"Using Parsl on Polaris","text":"<p>Parsl has a variety of possible configuration settings.  As an example, we provide the configuration below that will run one task per GPU:</p> <pre><code>from parsl.config import Config\n\n# PBSPro is the right provider for Polaris:\nfrom parsl.providers import PBSProProvider\n# The high throughput executor is for scaling to HPC systems:\nfrom parsl.executors import HighThroughputExecutor\n# You can use the MPI launcher, but may want the Gnu Parallel launcher, see below\nfrom parsl.launchers import MpiExecLauncher, GnuParallelLauncher\n# address_by_interface is needed for the HighThroughputExecutor:\nfrom parsl.addresses import address_by_interface\n# For checkpointing:\nfrom parsl.utils import get_all_checkpoints\n\n# Adjust your user-specific options here:\nrun_dir=\"/lus/grand/projects/yourproject/yourrundir/\"\n\nuser_opts = {\n    \"worker_init\":      f\"source /path/to/your/virtualenv/bin/activate; cd {run_dir}\", # load the environment where parsl is installed\n    \"scheduler_options\":\"#PBS -l filesystems=home:eagle:grand\" , # specify any PBS options here, like filesystems\n    \"account\":          \"YOURPROJECT\",\n    \"queue\":            \"debug-scaling\",\n    \"walltime\":         \"1:00:00\",\n    \"nodes_per_block\":  3, # think of a block as one job on polaris, so to run on the main queues, set this &gt;= 10\n    \"cpus_per_node\":    32, # Up to 64 with multithreading\n    \"available_accelerators\": 4, # Each Polaris node has 4 GPUs, setting this ensures one worker per GPU\n    \"cores_per_worker\": 8, # this will set the number of cpu hardware threads per worker.  \n}\n\ncheckpoints = get_all_checkpoints(run_dir)\nprint(\"Found the following checkpoints: \", checkpoints)\n\nconfig = Config(\n        executors=[\n            HighThroughputExecutor(\n                label=\"htex\",\n                heartbeat_period=15,\n                heartbeat_threshold=120,\n                worker_debug=True,\n                available_accelerators=user_opts[\"available_accelerators\"], # if this is set, it will override other settings for max_workers if set\n                cores_per_worker=user_opts[\"cores_per_worker\"],\n                address=address_by_interface(\"bond0\"),\n                cpu_affinity=\"block-reverse\",\n                prefetch_capacity=0,\n                start_method=\"spawn\",  # Needed to avoid interactions between MPI and os.fork\n                provider=PBSProProvider(\n                    launcher=MpiExecLauncher(bind_cmd=\"--cpu-bind\", overrides=\"--depth=64 --ppn 1\"),\n                    # Which launcher to use?  Check out the note below for some details.  Try MPI first!\n                    # launcher=GnuParallelLauncher(),\n                    account=user_opts[\"account\"],\n                    queue=user_opts[\"queue\"],\n                    select_options=\"ngpus=4\",\n                    # PBS directives (header lines): for array jobs pass '-J' option\n                    scheduler_options=user_opts[\"scheduler_options\"],\n                    # Command to be run before starting a worker, such as:\n                    worker_init=user_opts[\"worker_init\"],\n                    # number of compute nodes allocated for each block\n                    nodes_per_block=user_opts[\"nodes_per_block\"],\n                    init_blocks=1,\n                    min_blocks=0,\n                    max_blocks=1, # Can increase more to have more parallel jobs\n                    cpus_per_node=user_opts[\"cpus_per_node\"],\n                    walltime=user_opts[\"walltime\"]\n                ),\n            ),\n        ],\n        checkpoint_files = checkpoints,\n        run_dir=run_dir,\n        checkpoint_mode = 'task_exit',\n        retries=2,\n        app_cache=True,\n)\n</code></pre>"},{"location":"polaris/workflows/parsl/#special-notes-for-polaris","title":"Special notes for Polaris","text":"<p>On Polaris, there is a known bug where python applications launched with <code>mpi</code> and that use <code>fork</code> to spawn processes can sometimes have unexplaned hangs.  For this reason, it is recommended to use <code>start_method=\"spawn\"</code> on Polaris when using the <code>MpiExecLauncher</code> as is shown in the example config above.  Alternatively, another solution is to use the <code>GNUParallelLauncher</code> which uses <code>GNU Parallel</code> to spawn processes.  <code>GNU Parallel</code> can be loaded in your environment with the command <code>module load gnu-parallel</code>.  Both of these approaches will circumvent the hang issue from using <code>fork</code>.</p>"},{"location":"polaris/workflows/parsl/#updates","title":"Updates","text":"<p>For <code>parsl</code> versions after July 2023, the <code>address</code> passed in the <code>HighThroughputExecutor</code> needs to be set to <code>address = address_by_interface(\"bond0\")</code>.  With <code>parsl</code> versions prior to July 2023, it was recommended to use <code>address = address_by_hostname()</code> on Polaris, but with later versions this will not work on Polaris (or any other machine).</p>"},{"location":"polaris/workflows/smartsim/","title":"SmartSim and SmartRedis","text":"<p>SmartSim is an open source tool developed by the Hewlett Packard Enterprise (HPE) designed to facilitate the integration of traditional HPC simulation applications with machine learning workflows. There are two core components to SmartSim:</p> <ul> <li>Infrastructure library (IL)<ul> <li>Provides API to start, stop and monitor HPC applications from Python</li> <li>Interfaces with the scheduler launch jobs (PBSPro on Polaris and Cobalt on Theta/ThetaGPU)</li> <li>Deploys a distributed in-memory database called the Orchestrator</li> </ul> </li> <li>SmartRedis client library<ul> <li>Provides clients that connect to the Orchestrator from Fortran, C, C++, Python code</li> <li>The client API library enables data transfer to/from database and ability to load and run JIT-traced Python and ML runtimes acting on stored data</li> </ul> </li> </ul> <p>For more resources on SmartSim, follow the links below:</p> <ul> <li>Source code</li> <li>Documentation</li> <li>Zoo of examples</li> <li>Fall 2023 ALCF User Hands-On Workshop</li> <li>NekRS-ML</li> </ul>"},{"location":"polaris/workflows/smartsim/#installation-with-pytorch-gpu-backend","title":"Installation with PyTorch GPU Backend","text":"<p>SmartSim on Polaris can be installed creating a virtual environment based on the ML conda module.  From a compute node, execute <pre><code>module use /soft/modulefiles\nmodule load conda/2024-04-29\nconda activate base\npython -m venv --clear /path/to/_ssim_env --system-site-packages\nsource /path/to/_ssim_env/bin/activate\n</code></pre> Note that <code>/path/to/</code> can either be a user's home or project directory.</p> <p>Then set up the environment variables <pre><code>export CC=cc\nexport CXX=CC\nexport CUDNN_BASE=/soft/libraries/cudnn/cudnn-12-linux-x64-v9.1.0.70\nexport CUDNN_LIBRARY=$CUDNN_BASE/lib/\nexport CUDNN_INCLUDE_DIR=$CUDNN_BASE/include/\nexport LD_LIBRARY_PATH=$CUDNN_LIBRARY:$LD_LIBRARY_PATH\nexport TORCH_CMAKE_PATH=$( python -c 'import torch;print(torch.utils.cmake_prefix_path)' )\nexport TORCH_PATH=$( python -c 'import torch; print(torch.__path__[0])' )\nexport LD_LIBRARY_PATH=$TORCH_PATH/lib:$LD_LIBRARY_PATH\n</code></pre></p> <p>Now, install SmartSim and the PyTorch GPU backend <pre><code>git clone https://github.com/rickybalin/SmartSim.git\ncd SmartSim\npip install -e .\nsmart build -v --device gpu --torch_dir $TORCH_CMAKE_PATH --no_tf\ncd ..\n</code></pre></p> <p>and validate the build with <pre><code>smart validate\n</code></pre></p> <p>Finally, install the SmartRedis library <pre><code>git clone https://github.com/rickybalin/SmartRedis.git\ncd SmartRedis\nmake lib DEP_CC=cc DEP_CXX=CC\npip install -e .\ncd ..\n</code></pre></p> <p>To use SmartSim in the future, simply source the following environment. Note that the Torch libraries need to be prepended to <code>LD_LIBRARY_PATH</code>. <pre><code>module use /soft/modulefiles\nmodule load conda/2024-04-29\nconda activate base\nsource /path/to/_ssim_env/bin/activate\nexport TORCH_PATH=$( python -c 'import torch; print(torch.__path__[0])' )\nexport LD_LIBRARY_PATH=$TORCH_PATH/lib:$LD_LIBRARY_PATH\n</code></pre></p>"},{"location":"polaris/workflows/smartsim/#installation-with-tensorflow-gpu-backend","title":"Installation with TensorFlow GPU Backend","text":"<p>To use the TensorFlow backend to the SmartSim Orchestrator, the install steps are very similar but require downgrading the TensorFlow version to 1.13.1. Follow the same instructions outlined above for the PyTorch backend, with the following exceptions:</p> <ul> <li>After creating and sourcing the Python virtual environment, downgrade TensorFlow with <code>pip install tensorflow==2.13.1</code>. Note that this will also downgrade typing-extensions, which will cause compatibility issues with the PyTorch in the conda module.</li> <li>No need to export <code>TORCH_CMAKE_PATH</code> and <code>TORCH_PATH</code>, or modify <code>LD_LIBRARY_PATH</code></li> <li>Build the SmartSim backend with <code>smart build -v --device gpu --no_pt</code></li> </ul>"},{"location":"polaris/workflows/smartsim/#examples","title":"Examples","text":"<p>You can find examples of in situ training and inference of ML models from an ongoing CFD simulation at the NekRs-ML repository.  The <code>smartredis</code> branch has instructions on how to build and run the examples on Polaris.</p> <p>The Fall 2023 ALCF User Hands-On Workshop repository also contains information on how to use SmartSim and NekRS-ML on Polaris, but note that some of the instructions are specfic to the Fall of 2023.</p>"},{"location":"polaris/workflows/smartsim/#notes","title":"Notes","text":"<ul> <li>SmartSim workflows, such as online training, often require launching multiple MPI applications on the same set of nodes. On Polaris, the <code>MPICH_OFI_CXI_PID_BASE=0</code> must be exported before the first call to <code>mpiexec</code>, and then incremented by 1 and re-exported before each successive call. This is done with the SmartSim API by adding <code>env_vars={'MPICH_OFI_CXI_PID_BASE':str(0)}</code> to the <code>PalsMpiexecSettings()</code> API.</li> </ul>"},{"location":"policies/alcf-acknowledgement-policy/","title":"ALCF Acknowledgement Policy","text":"<p>As a U.S. Department of Energy user facility dedicated to the advancement of scientific discoveries, the Argonne Leadership Computing Facility (ALCF) provides unique computing resources and expertise to a user community that is bound by certain policies designed to acknowledge and promote the work of others as well as the resources used to accomplish this work.</p> <p>The ALCF requests your continued compliance with the terms of your program or discretionary award, specifically with regard to acknowledgments in publications and presentations based on work done with ALCF resources. Also, please forward your accepted publication citations to pubs@alcf.anl.gov.</p>"},{"location":"policies/alcf-acknowledgement-policy/#ai-testbeds-publication-guidance","title":"AI Testbeds Publication Guidance","text":"<p>To publish technical reports and research papers using the ALCF AI testbeds, we request you to provide us with a draft of your paper prior to submission by emailing a copy to us at support@alcf.anl.gov. We will work closely with the AI testbed vendors to provide feedback in a timely manner. We strongly recommend you engage us and the vendors early and often in this process to help us facilitate your research objectives.</p> <p>For guidance on acknowledgements, please see the following sample policies:</p>"},{"location":"policies/alcf-acknowledgement-policy/#alcf-only-acknowledgement","title":"ALCF Only Acknowledgement","text":"<p>Users, and ALCF staff scientists without direct project funding, should acknowledge the ALCF in all publications and presentations that speak to work performed on ALCF resources. </p> <p>This research used resources of the Argonne Leadership Computing Facility, a U.S. Department of Energy (DOE) Office of Science user facility at Argonne National Laboratory and is based on research supported by the U.S. DOE Office of Science-Advanced Scientific Computing Research Program, under Contract No. DE-AC02-06CH11357.</p>"},{"location":"policies/alcf-acknowledgement-policy/#incitealcf-acknowledgement","title":"INCITE/ALCF Acknowledgement","text":"<p>Users should acknowledge the ALCF in all publications and presentations that speak to INCITE work performed on ALCF resources. </p> <p>An award for computer time was provided by the U.S. Department of Energy\u2019s (DOE) Innovative and Novel Computational Impact on Theory and Experiment (INCITE) Program. This research used resources from the Argonne Leadership Computing Facility, a U.S. DOE Office of Science user facility at Argonne National Laboratory, which is supported by the Office of Science of the U.S. DOE under Contract No. DE-AC02-06CH11357.</p>"},{"location":"policies/alcf-acknowledgement-policy/#incitealcfolcf-acknowledgement","title":"INCITE/ALCF/OLCF Acknowledgement","text":"<p>Users should acknowledge the ALCF and OLCF in all publications and presentations that speak to INCITE work performed on ALCF and OLCF resources. </p> <p>An award for computer time was provided by the U.S. Department of Energy\u2019s (DOE) Innovative and Novel Computational Impact on Theory and Experiment (INCITE) Program. This research used supporting resources at the Argonne and the Oak Ridge Leadership Computing Facilities. The Argonne Leadership Computing Facility at Argonne National Laboratory is supported by the Office of Science of the U.S. DOE under Contract No. DE-AC02-06CH11357. The Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory is supported by the Office of Science of the U.S. DOE under Contract No. DE-AC05-00OR22725.</p>"},{"location":"policies/facility-policies/","title":"ALCF Facility Policies","text":"<p>Be sure to familiarize yourself with the various policies and procedures for ALCF users, categorized below.</p>"},{"location":"policies/facility-policies/#accounts","title":"Accounts","text":"<p>All holders of user accounts must comply with ALCF and Argonne National Laboratory computing usage policies, including meeting certain security requirements and executing specific science- or engineering-related computing jobs.</p> <ul> <li>Accounts Policy</li> <li>Account Sponsorship and Retention Policy</li> <li>User Authentication Policy</li> </ul>"},{"location":"policies/facility-policies/#alcf-acknowledgement-policy","title":"ALCF Acknowledgement Policy","text":"<p>As a U.S. Department of Energy Office of Science User Facility dedicated to the advancement of scientific discovery, the ALCF requests that its users acknowledge and promote the work of others and the resources with which this work was accomplished.</p> <ul> <li>ALCF Acknowledgement Policy</li> </ul>"},{"location":"policies/facility-policies/#data-and-allocation","title":"Data and Allocation","text":"<p>These policies detail data and software usage, as well as pullback and refunds of computing hours.</p> <ul> <li>Data Policy</li> <li>Pullback Policy</li> <li>Refund Policy</li> <li>Software Policy</li> </ul>"},{"location":"policies/facility-policies/#quarterly-reports","title":"Quarterly Reports","text":"<p>The ALCF is required to report the progress and accomplishments of its allocation projects. Policies are detailed by award type.</p> <ul> <li>Quarterly Report Policy</li> </ul>"},{"location":"policies/facility-policies/#queue-and-scheduling-policies","title":"Queue and Scheduling Policies","text":"<ul> <li>General Policies</li> </ul>"},{"location":"policies/accounts/account-sponsorship-retention-policy/","title":"Account Sponsorship &amp; Retention Policy","text":"<p>This page is designed to help you understand the different types of accounts that you will encounter at the ALCF. The policy outlined reviews the responsibilities of an account holder, an account sponsor, and those of a foreign national.</p>"},{"location":"policies/accounts/account-sponsorship-retention-policy/#alcf-account-types","title":"ALCF Account Types","text":"<p>Annual: This account applies to users who are not ALCF Regular Employees. The default renewal date (account deactivation date) for the account is a year from the day the account was requested. These accounts are renewed annually and must be approved by an ALCF Staff member or a Project PI (also known as the \u201capprover\u201d). Users are required to update their account information and agree to the Terms of Use each year. Users need to be a part of an active project for their account to be renewed.</p> <p>Permanent: This account applies to individuals who are Regular Employees within the ALCF and CPS Divisions. If you hold this type of account, periodic renewal is not necessary.</p> <p>Note: Foreign Nationals have a second date (apart from their account deactivation date) that controls their account access. Accounts held by foreign nationals require paperwork referred to as an ANL-593 (or just 593 for shorthand). This paperwork is also required for any on-site access, and also applies to computer accounts. DOE requirements state that the ALCF is to disable any account with expired 593 paperwork. </p> <p>A notification system has been established that issues a warning notice to users when expiration approaches and requests action to ensure that accounts are not needlessly turned off. An approval from the project PI is required to renew ANL 593 for project members that are foreign nationals.</p>"},{"location":"policies/accounts/account-sponsorship-retention-policy/#your-responsibilities-as-an-account-approver","title":"Your responsibilities as an account approver","text":"<p>If you approve any accounts, please take note of the following roles and responsibilities:</p> <p>By approving someone for an account at the ALCF, you are accepting responsibility for the account applicant and confirming that this individual is who they claim to be and is thus entitled to work on our computers. Do not simply \"rubber stamp\" any account application that claims you as an account approver/project PI.</p> <p>You are also responsible for approving account renewal requests. When an account is about to expire, we send a warning notification to the account holder. Among other things, the account holder is asked to contact the approver (the PI of any of the active projects the account holder is associated with) if they wish to renew their account. We cannot and will not extend someone's account without an approval. An important aspect of this process to note is that inaction will result in the account becoming deactivated on the expiration date.</p> <p>You are also responsible for approving ANL 593 renewals requests. When an account\u2019s 593 is about to expire, we send a warning notification to the account holder. Among other things, the account holder is asked to contact the approver (the PI of any of the active projects the account holder is associated with) if they wish to renew their 593. We cannot and will not extend someone's 593 without an approval. An important aspect of this process to note is that inaction will result in the account becoming deactivated at the expiration date.</p>"},{"location":"policies/accounts/account-sponsorship-retention-policy/#account-retention-policy","title":"Account Retention Policy","text":"<p>Accounts can exist in one of three states:</p> <ul> <li>Active: The active state is normal for an account.</li> <li>Inactive: The inactive state occurs when an account expires, and the ability to use ALCF resources is removed by changing the active status of the account to inactive. All files continue to exist in the user's home directory. An account will remain in the inactive state for at least 90 days before moving to the next state.</li> <li>Deleted: After 90 days, an inactive account will be deleted. This removes all references to the account from the system (except the accounts database), including any files and home directories.</li> </ul> <p>Users with inactive or deleted accounts can request a reactivation visiting https://accounts.alcf.anl.gov and clicking on the \u201cReactivate An Account\u201d link.</p>"},{"location":"policies/accounts/accounts-policy/","title":"Accounts Policy","text":"<p>All holders of user accounts must abide by all appropriate Argonne Leadership Computing Facility and Argonne National Laboratory computing usage policies.  The policy details are outlined in the following documents:</p> <ul> <li>ANL's Information Technology Access Agreement</li> <li>Addendum to ANL's Information Technology Access Agreement</li> </ul> <p>These are described at the time of the account request and include requirements such as using a sufficiently strong password, appropriate use of the system, and so on. Any user not following these requirements will have their account disabled.</p> <p>Furthermore, ALCF resources are intended to be used as a computing resource for specific computational science or engineering work, not as a general-purpose computing system. </p> <p>If someone is using the system extensively but not carrying out any computational activities, their account could be disabled.</p>"},{"location":"policies/accounts/user-authentication-policy/","title":"User Authentication Policy","text":"<p>Users of the ALCF systems are required to use a SafeNet token (physical or mobile) one time password, multifactor authentication system.</p> <p>This document explains the policies users must follow regarding SafeNet tokens for accessing the ALCF systems.</p>"},{"location":"policies/accounts/user-authentication-policy/#multifactor-authentication","title":"MultiFactor Authentication","text":"<p>\"Authentication systems are frequently described by the authentication factors that they incorporate. The three factors often considered as the cornerstone of authentication are: Something you know (for example, a password); Something you have (for example, an ID badge or a cryptographic key); and Something you are (for example, a voice print or other biometric measurement).\" -- NIST iTL Bulletin, Aug 2004</p> <p>By the NIST guidelines for identification and authentication (NIST 800-53, Revision 3, Control IA-2), ALCF aims for a Moderate level of security controls. All production systems in ALCF require multifactor authentication for users with network and local (privileged and non-privileged accounts) using the SafeNet tokens.</p>"},{"location":"policies/accounts/user-authentication-policy/#mobile-and-physical-tokens","title":"Mobile and Physical Tokens","text":"<p>ALCF provides every user of the production resources a physical or mobile token called a SafeNet Token. This is named after the company that developed the key fob and mobile software (the organization is now called SafeNet). \"Both tokens use AES-256 bit encryption to generate OTPs [One Time Passwords] comprised of digits, digits and letters or digits, letters and special characters...\"</p> <p>When you receive your physical token, it will be initialized, but it will have no access privileges until you have contacted us to verify your identity.</p> <p>At the end of your account or project lifecycle, please return the token to the ALCF help desk:</p> <p>ALCF Service Desk Argonne National Laboratory 9700 South Cass Avenue Building 240 Argonne, IL 60439</p>"},{"location":"policies/accounts/user-authentication-policy/#protect-your-passcode-token","title":"Protect Your Passcode token","text":"<p>Your passcode token should be protected by you as carefully as your credit cards or house keys. If your token is lost, stolen, or damaged, please contact us immediately so that we can deactivate the token and prevent unauthorized access. Sharing of tokens is strictly forbidden. Please do not mark on the token or alter it in any way.</p>"},{"location":"policies/accounts/user-authentication-policy/#more-information","title":"More information","text":"<p>[New User Guide] (http://www.alcf.anl.gov/user-guides/new-user-guide)</p> <p>Using Passcode Tokens</p>"},{"location":"policies/accounts/user-authentication-policy/#references","title":"References","text":"<ul> <li>http://www.itl.nist.gov/lab/bulletns/bltnaug04.htm</li> <li>http://csrc.nist.gov/publications/nistpubs/800-53-Rev3/sp800-53-rev3-final_updated-errata_05-01-2010.pdf</li> <li>http://csrc.nist.gov/publications/nistpubs/800-63-1/SP-800-63-1.pdf</li> <li>https://safenet.gemalto.com/multi-factor-authentication/authenticators/one-time-password-otp/</li> </ul>"},{"location":"policies/data-and-software-policies/data-policy/","title":"Data Policy","text":""},{"location":"policies/data-and-software-policies/data-policy/#alcf-data-confidentiality","title":"ALCF Data Confidentiality","text":"<p>The Argonne Leadership Computing Facility (ALCF) network is an open-research network. Because our resources and networks are open to many users and cannot be protected at a partitioned level, we cannot guarantee complete security for any data that resides here. It is up to users to provide the security they need.</p> <p>Data is not encrypted at rest. Data transferred via SSH (i.e., scp) is encrypted in transmission using SSH\u2019s mechanisms (e.g., AES256, etc.). Data transferred via Globus ( GridFTP) isn't normally fully encrypted.  The GridFTP control channel is encrypted, but the data channel by default is not (though the authentication processes for both channels are encrypted).  If you need full encryption of the data stream, you need to explicitly select \"encrypt transfer\" in the \"Transfer &amp; Timer Options\" in the Globus UI or use equivalent options in the CLI or transfer API if you're using those. More information here https://docs.globus.org/faq/security</p> <p>The basic level of protection provided is UNIX file level permissions; it is the user's responsibility to ensure that file permissions and umasks are set to match their needs.</p> <p>NOTE: The default permissions and umasks are group and world readable. For help determining or setting file permissions or umasks, or creating a UNIX group, contact support@alcf.anl.gov.</p>"},{"location":"policies/data-and-software-policies/data-policy/#alcf-staff-with-root-privileges","title":"ALCF Staff with Root Privileges","text":"<p>ALCF resource administrators with root privileges are not constrained by the file permissions, and they have the capability to open and/or copy all files on the system. They can also assume a user\u2019s identity on the system. There is no audit trail for access, touching, or moving data; however, ALCF staff does not view or modify project data unless directed by a PI or project member to help debug a problem. Data may be touched or accessed by the filesystem itself if data needs to be repaired or verified for integrity after a filesystem event (e.g., a fsck).</p> <p>The ALCF resources are Federal resources and are the property of the United States Government. Any or all uses of this system and all files on this system may be intercepted, monitored, recorded, copied, audited, inspected, and disclosed to authorized site, Department of Energy, and law enforcement personnel, as well as authorized officials of other agencies, both domestic and foreign.</p> <p>Administrators use elevated privileges for maintenance and system management. Following are instances where ALCF staff might look at your files: - We maintain copies of all .error, .output, and Cobalt log files and may review them to determine if a job failure was due to user error or a system failure. - If you request our assistance via any mechanism (for example, support ticket, direct personal email, in-person, etc.), be aware we may need to view your files using elevated privileges to aid us in resolving your issue.</p>"},{"location":"policies/data-and-software-policies/data-policy/#use-of-proprietarylicensed-software","title":"Use of Proprietary/Licensed Software","text":"<p>All software used on ALCF computers must be appropriately acquired and used according to the appropriate licensing. Possession or use of illegally copied software is prohibited. Likewise, users shall not copy copyrighted software, except as permitted by the owner of the copyright. Currently, the use of export-controlled codes is prohibited.</p>"},{"location":"policies/data-and-software-policies/data-policy/#prohibited-data","title":"Prohibited Data","text":"<p>The ALCF computer systems are operated as research systems and contain only data related to scientific research. Use of ALCF resources to store, manipulate, or remotely access any sensitive or national security information is prohibited unless documented and approved, by the PI and ALCF leadership. </p> <p>This includes, but is not limited to, personally identifiable information (data that falls under the Privacy Act of 1974, 5 U.S.C. 552a), controlled unclassified information (CUI) to include unclassified controlled nuclear information (UCNI), naval nuclear propulsion information (NNPI), International Traffic in Arms Relations (ITAR), the design or development of nuclear, biological, or chemical weapons, or any weapons of mass destruction. The use of ALCF resources for personal or non-work-related activities is also prohibited.</p>"},{"location":"policies/data-and-software-policies/data-policy/#export-control","title":"Export Control","text":"<p>All principal investigators using ALCF resources and ALCF staff members working with project teams are responsible for knowing whether their project generates any of these prohibited data types or information that falls under Export Control. For questions, contact ALCF Support at support@alcf.anl.gov.</p>"},{"location":"policies/data-and-software-policies/data-policy/#data-storage-systems","title":"Data Storage Systems","text":"<p>Data stored for any length of time on ALCF resources should only be data directly related to work done on any of the ALCF leadership computing systems. Specific policies apply to the three types of data storage systems maintained at ALCF. Read these policies carefully and plan accordingly in terms of space, usage, and data protection.</p>"},{"location":"policies/data-and-software-policies/data-policy/#home-file-system-space","title":"Home File System Space","text":"<p>swift-home</p> <p>The home file system (/home) is intended to hold your executable files, configuration files, etc. It is NOT meant to hold the output from your application runs (use the data/parallel file system for that purpose). The home file system space is generally moderate in size and is the best protected. Because of its size, backups are practical to accomplish.  The system performs tape backups, enabling the recovery of files more than seven days old or recovery from a catastrophic disk failure. Users should email support@alcf.anl.gov if they need assistance. The table below indicates the capabilities and characteristics of each file system.</p> <p>AI Testbed home</p> <p>/home shared across the ALCF AI testbed systems, including the ai testbed's login and compute nodes, is different from mira-home. Default user quota on the ai testbed's home is 1 TB storage and 1,000,000 files. This space is backed up.</p>"},{"location":"policies/data-and-software-policies/data-policy/#team-project-or-campaign-file-system","title":"Team Project or Campaign File System","text":"<p>Grand and Eagle</p> <p>The team project/campaign file system is intended primarily for results output from your computational runs on the ALCF computing systems. This space is accessible to the team members of your project that have an ALCF account. Default storage quota is 1 TB. Consider this space intermediate-term storage. Once any active production and/or analysis is complete and you no longer need regular access to the data, archive it within the ALCF (explained below) or transfer it to your home institution or move it to Eagle to share it with the broader community (explained below). </p> <p>This space has redundancy in the servers and storage but is so large that replication, snapshots, and backups are not practical. Grand and Eagle are Lustre global parallel file systems. All new projects will be given storage allocations on either Grand or Eagle. More information on Lustre File Striping Basics: Lustre File Striping Basics  </p> <p>Pullback Policy: Projects that do not use a minimum of 50% of their allocated space after 6 months will be subject to a quota limit reduction.</p> <p>AI Testbed projects file system</p> <p>The team project/campaign file system /projects mounted on AI Testbed's login and compute nodes is intended to facilitate project collaboration and is accessible to the team members of your project that have an ALCF account. Default group storage quota is 2 TB and 2,000,000 files. Please note that this space isn't backed up. Our policy is that data will be purged from disk 6 months after project completion.</p>"},{"location":"policies/data-and-software-policies/data-policy/#shared-community-project-or-campaign-file-system-eagle","title":"Shared Community Project or Campaign File System (Eagle)","text":"<p>The file system Eagle, a Lustre global parallel file system, has community sharing-abilities and is useful for sharing the project/campaign data with the broader research community via Globus. This space does not have redundancy in the servers or storage and is so large that replication, snapshots, and backups are not practical. The table below indicates the capabilities and characteristics of each file system. Default storage quota on Eagle is 1 TB and the default period is 1 year. More information on Lustre File Striping Basics: Lustre File Striping Basics  </p> <p>Eagle Data Pullback Policy:  Projects that do not use a minimum of 50% of their allocated space after 6 months will be subject to a quota limit reduction.</p> <p>Eagle Access Termination Policy:  Project endpoints that have exhibited no activity* for a period of 6 months will be disabled and the storage space will be reclaimed. Notification will be sent to the PI and project members 30 days prior to and the day of the action.</p> <p>Activity is defined as, but not limited to:</p> <ul> <li>Creation of the Globus endpoint</li> <li>Globus transfers to and from the endpoint</li> <li>atime audits of data files indicating access</li> <li>Other factors may include DOIs and citations referring to the project</li> </ul>"},{"location":"policies/data-and-software-policies/data-policy/#archive-space","title":"Archive Space","text":"<p>The archive space is intended for offline storage of results you wish to retain but either have no immediate need to access or no room in your parallel file system space. Archiving capabilities are available via HPSS. The primary HPSS access is via HSI. HTAR is available, but its path length and file size limitations often cause it to fail. Globus Online and GridFTP are clients that can also be used with HPSS.  Due to the possibility of data corruption or loss due to a bad tape, users can request dual writes for particularly critical data. Such requests will be handled on a case-by-case basis.</p>"},{"location":"policies/data-and-software-policies/data-policy/#data-storage-policies","title":"Data Storage Policies","text":""},{"location":"policies/data-and-software-policies/data-policy/#disk-capacity-and-retention-policies","title":"Disk Capacity and Retention Policies","text":"---- /home /lus/grand/projects or /grand lus/eagle/projects or /eagle Default Quota <sup>1</sup> 50 GB 1 TB / 1 million files 1 TB / 1 million files Quota Enforcement <sup>2</sup> hard/soft hard/soft hard/soft Disk Redundancy <sup>3</sup> dual parity dual parity dual parity File Server Snapshots <sup>6</sup> (frequency/retained) none none none File Server Metadata Redundancy yes yes yes File Server Metadata Replication <sup>4</sup> yes yes yes File Server Data Replication <sup>5</sup> yes no no Data Purged from Disk n/a 6 months after project completion <sup>8</sup> After 6 months of inactivity (see Eagle Access termination policy listed in the Eagle section above) <sup>8</sup>"},{"location":"policies/data-and-software-policies/data-policy/#tape-capacity-and-retention-policies","title":"Tape Capacity and Retention Policies","text":"---- /home /lus/grand/projects or /grand lus/eagle/projects or /eagle Automatic Backup to Tape? <sup>7</sup> yes no no Archived to Tape Before Deleted from Disk? <sup>9</sup> yes no no <ol> <li>While quotas are subject to negotiation on a case-by-case basis, disk space is a finite resource and projects must exercise good data management practices for their own sake and the sake of other users of the facility.  With Lustre, it has become necessary to enforce file quotas as well, which are also negotiable.</li> <li>\u201cHard quota enforcement\u201d means a job will fail when writing output if you exceed the hard quota limit.  \"Soft quota enforcement\" means you may exceed the soft quota limit (but never the higher hard quota value) for up to seven days.  If you do not drop back below the soft quota limit within seven days, writes will begin to fail.</li> <li>Hard drives are in redundancy groups of 10 disks (8 data + 2 parity). In other words, three out of 10 drives would have to fail before data loss occurred.</li> <li>Metadata (i.e., information listing which blocks are part of which files) is written twice to two different storage arrays. Thus, even if an entire array were lost, the metadata would be preserved.</li> <li>Refers to the fact that data (user output) is written twice with each block on two different storage arrays, so that even if an entire array were lost, the data would be preserved.</li> <li>Snapshots are stored in your home directory (see Home File System Space for more info). If you accidentally delete the directory or need a previous version, use the cp command to copy the file back to your home directory.</li> <li>\u201cYes\u201d denotes that ALCF does regular backups without intervention from the user. In case of project data, data is backed up to tape   after a stipulated period (see point 8 below) and is retained for 2 years (subject to change). In all other cases, user is responsible for archiving the data to HPSS or copying it to another facility as desired.</li> <li>The project directory is available on disk for the stipulated period but project quotas are reduced immediately following project end date (except Eagle). Access to the directory will be removed after 90 days. Requests to restore/extend access or reset the quota are reviewed on a case-by-case basis.</li> <li>Users who wish to retain data must archive or transfer their data elsewhere at the end of the project. Users need an active ALCF account to access archived data on HPSS. See Account Retention Policy for more information.</li> </ol>"},{"location":"policies/data-and-software-policies/software-policy/","title":"ALCF Resource Software Use","text":"<p>All software used on ALCF computers must be appropriately acquired and used according to the appropriate licensing. Possession or use of illegally copied software is prohibited. Likewise, users shall not copy copyrighted software, except as permitted by the owner of the copyright. Currently, the use of export-controlled codes is prohibited.</p>"},{"location":"policies/data-and-software-policies/software-policy/#community-software-policy","title":"Community Software Policy","text":"<p>ALCF supports the deployment of community software from active projects on production systems. A project may provide and support a code on ALCF systems for the ALCF user community as described in the [Community Software Service].</p> <p>User deployments are system-specific, and their maintenance is the sole responsibility of the project deploying it. There shall be no expectation of additional support from ALCF, other than for the provisioning of space and integration with the module system. Projects will be provided with an initial module file from a template, with the expectation that they will update and maintain the module, providing paths and instructions so that user communities can access the software.</p>"},{"location":"policies/queue-scheduling/pullback-policy/","title":"Pullback Policy","text":"<p>In an effort to ensure that valuable ALCF computing resources are used judiciously, a pullback policy has been instituted. Projects granted allocations under the INCITE and ALCC programs that have not used a significant amount of their allocation will be evaluated and adjusted during the year following the policies outlined on this page.</p> <p>The figures outlined below represent the maximum amount that will be pulled back from projects after specific dates during the allocation period. The decision to reduce allocations will be made on a case-by-case basis in discussion with the project's primary investigators (PIs).</p>"},{"location":"policies/queue-scheduling/pullback-policy/#incite-pullback-policy","title":"INCITE Pullback Policy","text":"<p>On May 1 of the current INCITE calendar year: - if usage is less than 15% remove up to 15% of the unused balance - if usage is less than 10% remove up to 30% of the unused balance</p> <p>On September 1 of the current INCITE calendar year: - if usage is less than 50% remove up to 33% of the unused balance - if usage is less than 33% remove up to 50% of the unused balance - if usage is less than 10% remove up to 75% of the unused balance</p>"},{"location":"policies/queue-scheduling/pullback-policy/#alcc-pullback-policy","title":"ALCC Pullback Policy","text":"<p>ALCC projects must use 50% of their allocation within the first seven months of the allocation cycle. Any unused time in excess of 50% will be deducted from the project allocation at the end of the seven month period.</p>"},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/","title":"Queue and Scheduling Policy","text":""},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/#general-policy","title":"General Policy","text":"<p>We ask that all users follow good etiquette and be excellent to one another.</p>"},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/#priority","title":"Priority","text":"<p>As with all Argonne Leadership Computing Facility production systems, job priority in the queue is based on several criteria: - positive balance of your project - size (in nodes) of the job, larger jobs receive higher priority - the type of project (e.g. INCITE, ALCC, or discretionary) - job duration - shorter duration jobs will accumulate priority more quickly, so it is best to specify the job run time as accurately as possible</p>"},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/#reservations-and-scheduling-policy","title":"Reservations and Scheduling Policy","text":"<p>Some work will require use of Polaris that requires deviation from regular policy. On such occasions, normal reservation policy applies. Please send the regular form no fewer than five (5) business days in advance.</p>"},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/#big-run-mondays","title":"Big Run Mondays","text":"<p>As part of our regular maintenance procedures on Mondays, we will promote to the highest priority any jobs in the queued state requesting 802 nodes or more. Promotion is subject to operational discretion.</p> <p>We may also, at our discretion, take the opportunity to promote the priority of capability jobs if the system has been drained of jobs for any other reason.</p>"},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/#monday-maintenance","title":"Monday Maintenance","text":"<p>On Mondays where the ALCF is on a regular business schedule the system may be expected to undergo maintenance from 9:00 am until 5:00 pm US Central Time. The showres command may be used to view pending and active maintenance reservations.</p>"},{"location":"policies/queue-scheduling/queue-and-scheduling-policy/#incitealcc-overburn-policy","title":"INCITE/ALCC Overburn Policy","text":"<p>If an INCITE or ALCC project has exhausted its allocation in the first 11 months of its allocation year, it is eligible for overburn running. At this point, capability jobs submitted by INCITE and ALCC projects will run in the default queue (instead of backfill) for the first 11 months of the allocation year until 125% of the project allocation has been consumed.</p> <p>INCITE and ALCC projects needing additional overburn hours should e-mail support@alcf.anl.gov with a short description of what they plan to do with the additional hours, highlighting specific goals or milestones and the time expected to accomplish them. This will be reviewed by the scheduling committee, allocations committee, and ALCF management. Requests should be submitted 15 days before the start of the next quarter of the allocation year for full consideration. Non-capability jobs from projects that have exhausted their allocation will continue to run in backfill. </p> <p>To be clear, this policy does not constitute a guarantee of extra time, and we reserve the right to prioritize the scheduling of jobs submitted by projects that have not yet used 100% of their allocations, so the earlier that an INCITE or ALCC project exhausts its allocation, the more likely it is to be able to take full advantage of this policy.</p>"},{"location":"policies/queue-scheduling/refund-policy/","title":"Refund Policy","text":"<p>If a system problem affects your run, ALCF will consider a refund of node hours. The ALCF expects all applications to regularly checkpoint, so refunds are typically capped at four hours of runtime for the affected job, unless the problem in question prevented checkpoints. </p> <p>ALCF strongly advises against symlinking between filesystems or hard-coding paths to a different filesytem.</p> <p>To request a refund, send the following information to support@alcf.anl.gov: - Job id - Machine - Reason for refund request</p> <p>For more information, contact support@alcf.anl.gov.</p>"},{"location":"running-jobs/example-job-scripts/","title":"Example Job Scripts","text":"<p>This page contains a small collection of example job scripts users may find useful for submitting their jobs on Polaris. Additional information on PBS and how to submit these job scripts is available here.</p> <p>A simple example using a similar script on Polaris is available in the Getting Started Repo.</p> <p>NOTE: Since <code>#</code> is required prior to each PBS directive, comments should be added after the directives have been listed in your submission script. If you try to add comments within the directive list, you could experience submission issues due to PBS attempting to read your comment as an additional directive. This includes adding comments on the same line as a directive (i.e., <code>#PBS -q &lt;queue_name&gt;  #comment</code>).</p>"},{"location":"running-jobs/example-job-scripts/#cpu-mpi-openmp-examples","title":"CPU MPI-OpenMP Examples","text":"<p>The following <code>submit.sh</code> example submits a 1-node job to Polaris with 16 MPI ranks per node and 2 OpenMP threads per rank. See Queues for details on practical limits to node counts and job times for different sizes of jobs.</p> <p>The <code>hello_affinity</code> program is a compiled C++ code, which is built via <code>make -f Makefile.nvhpc</code> in the linked directory after cloning the Getting Started repository.</p> <pre><code>#!/bin/bash -l\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:grand\n#PBS -q debug\n#PBS -A Catalyst\n\n# Change to working directory\ncd ${PBS_O_WORKDIR}\n\n# MPI and OpenMP settings\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=16\nNDEPTH=2\nNTHREADS=2\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth --env OMP_NUM_THREADS=${NTHREADS} -env OMP_PLACES=threads ./hello_affinity\n</code></pre> <p>The following function in the <code>hello_affinity</code> source code is essential for uniquely identifying the CUDA device even when Multi-Instance GPU (MIG) is enabled, as each physical device will be partitioned into multiple virtual devices, each with unique UUIDs differentiated by the last few characters:</p> Identifying physical or virtual GPU by UUID <pre><code>//https://stackoverflow.com/questions/68823023/set-cuda-device-by-uuid\nvoid uuid_print(cudaUUID_t a){\n  std::cout &lt;&lt; \"GPU\";\n  std::vector&lt;std::tuple&lt;int, int&gt; &gt; r = {{0,4}, {4,6}, {6,8}, {8,10}, {10,16}};\n  for (auto t : r){\n    std::cout &lt;&lt; \"-\";\n    for (int i = std::get&lt;0&gt;(t); i &lt; std::get&lt;1&gt;(t); i++)\n      std::cout &lt;&lt; std::hex &lt;&lt; std::setfill('0') &lt;&lt; std::setw(2) &lt;&lt; (unsigned)(unsigned char)a.bytes[i];\n  }\n  std::cout &lt;&lt; std::endl;\n}\n</code></pre> <p>NOTE: If you are a <code>zsh</code> user, you will need to ensure ALL submission and shell scripts include the <code>-l</code> flag following <code>#!/bin/bash</code> as seen in the example above to ensure your environment is being instantiated properly. <code>zsh</code> is NOT supported by HPE and support from ALCF will be best effort only.</p> <p>Each Polaris compute node has 1 Milan CPU with a total of 32 physical cores, with each core supporting 2 hardware threads (for a total of 64 logical cores). </p> <p>The process affinity in this example is setup to map each MPI rank to 2 physical cores. Each MPI rank spawns 2 OpenMP threads, so 1 thread per physical core. The OpenMP settings bind each OpenMP thread to a single hardware thread within a core, such that all 32 physical cores are utilized. CPU core IDs <code>32</code> to <code>63</code> are not mapped to any MPI rank, since they correspond to simultaneous multithreaded (SMT) sibling hardware threads that share the execution resources of the core ids <code>0</code> to <code>31</code>, respectively.</p> <ul> <li><code>cd ${PBS_O_WORKDIR}</code> : change into the working directory from where <code>qsub</code> was executed.</li> <li><code>NNODES= `wc -l &lt; $PBS_NODEFILE`</code>: one method for determine the total number of nodes allocated to a job.</li> <li><code>NRANKS_PER_NODE=16</code> : This is a helper variable to set the number of MPI ranks for each node to 16.</li> <li><code>NDEPTH=2</code> : This is a helper variable to space MPI ranks 2 \"slots\" from each other. In this example, individual threads correspond to a slot. This will be used together with the <code>--cpu-bind</code> option from <code>mpiexec</code> and additional binding options are available (e.g. <code>numa</code>, <code>socket</code>, <code>core</code>, etc.).</li> <li><code>NTHREADS=2</code> : This is a helper variable to set the number of OpenMP threads per MPI rank.</li> <li><code>NTOTRANKS=$(( NNODES * NRANKS_PER_NODE))</code> : This is a helper variable calculating the total number of MPI ranks spanning all nodes in the job.</li> </ul> <p>Information on the use of <code>mpiexec</code> is available via <code>man mpiexec</code>. Some notes on the specific options used in the above example follow.</p> <ul> <li><code>-n ${NTOTRANKS}</code> : This is specifying the total number of MPI ranks to start.</li> <li><code>--ppn ${NRANKS_PER_NODE}</code> : This is specifying the number of MPI ranks to start on each node.</li> <li><code>--depth=${NDEPTH}</code> : This is specifying how many cores/threads to space MPI ranks apart on each node.</li> <li><code>--cpu bind depth</code> : This is indicating the number of cores/threads will be bound to MPI ranks based on the <code>depth</code> argument.</li> <li><code>--env OMP_NUM_THREADS=${NTHREADS}</code> : This is setting the environment variable <code>OMP_NUM_THREADS</code> : to determine the number of OpenMP threads per MPI rank.</li> <li><code>--env OMP_PLACES=threads</code> : This is indicating how OpenMP should distribute threads across the resource, in this case across hardware threads.</li> </ul>"},{"location":"running-jobs/example-job-scripts/#hardware-threads","title":"Hardware threads","text":"<p>This example is similar to the previous, but it exhausts all 64 logical cores available on each compute node CPU. We double the number of MPI ranks to 32, one per each physical core. Using <code>--cpu-bind=core</code>, the <code>--depth</code> flag value becomes interpreted by Cray MPICH as spacing in number of physical cores, so <code>NDEPTH=1</code> ensures that rank 0 is bound to CPU core IDs <code>(0,32)</code>, the 2 SMT sibling hardware threads that share the first physical core.</p> <p><pre><code>#!/bin/bash -l\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:grand\n#PBS -q debug\n#PBS -A Catalyst\n\n# Change to working directory\ncd ${PBS_O_WORKDIR}\n\n# MPI and OpenMP settings\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=32\nNDEPTH=1\nNTHREADS=2\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind core --env OMP_NUM_THREADS=${NTHREADS} -env OMP_PLACES=threads ./hello_affinity\n</code></pre> Many HPC applications do not benefit from utilizing the CPU's SMT2 capabilities, and such software may achieve better performance by using the previous script such that each of the 32 physical cores only runs a single OpenMP thread.</p>"},{"location":"running-jobs/example-job-scripts/#gpu-mpi-examples","title":"GPU MPI Examples","text":"<p>Using the CPU job submission examples above as a baseline, there are not many additional changes needed to enable an application to make use of the 4 NVIDIA A100 GPUs on each Polaris node. In the following 2-node example (because <code>#PBS -l select=2</code> indicates the number of nodes requested), 4 MPI ranks will be started on each node assigning 1 MPI rank to each GPU in a round-robin fashion. A simple example using a similar job submission script on Polaris is available in the Getting Started Repo.</p> <pre><code>#!/bin/bash -l\n#PBS -l select=2:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:eagle\n#PBS -j oe\n#PBS -q debug\n#PBS -A Catalyst\n\n# Enable GPU-MPI (if supported by application)\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\n# Change to working directory\ncd ${PBS_O_WORKDIR}\n\n# MPI and OpenMP settings\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=$(nvidia-smi -L | wc -l)\nNDEPTH=8\nNTHREADS=1\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\n# For applications that internally handle binding MPI/OpenMP processes to GPUs\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth --env OMP_NUM_THREADS=${NTHREADS} -env OMP_PLACES=threads ./hello_affinity\n\n# For applications that need mpiexec to bind MPI ranks to GPUs\n#mpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth --env OMP_NUM_THREADS=${NTHREADS} -env OMP_PLACES=threads ./set_affinity_gpu_polaris.sh ./hello_affinity\n</code></pre> <p>The affinity options <code>NDEPTH=8;</code> and <code>--cpu-bind depth</code> or <code>core</code> are set to ensure that each MPI rank is bound to a separate NUMA node. If OpenMP threading is desired, set <code>NTHREADS=8</code> for each MPI rank to spawn 1 thread per physical core (all in the same NUMA domain that the rank is bound to). The OpenMP-related options are not needed if your application does not use OpenMP. Nothing additional is required on the <code>mpiexec</code> command for applications that internally manage GPU devices and handle the binding of MPI/OpenMP processes to GPUs. A small helper script is available for those with applications that rely on MPI to handle the binding of MPI ranks to GPUs. Some notes on this helper script and other key differences with the early CPU example follow.</p> <p><code>export MPICH_GPU_SUPPORT_ENABLED=1</code></p> <p>For applications that support GPU-enabled MPI (i.e. use MPI to communicate data directly between GPUs), this environment variable is required to enable GPU support in Cray's MPICH. Omitting this will result in a segfault. Support for this also requires that the application was linked against the the GPU Transport Layer library (e.g. -lmpi_gtl_cuda), which is automatically included for users by the <code>craype-accel-nvidia80</code> module in the default environment on Polaris. If this gtl library is not properly linked, then users will see a error message indicating that upon executing the first MPI command that uses a device pointer.</p> <p><code>./set_affinity_gpu_polaris.sh</code></p> <p>This script is useful for those applications that rely on MPI to bind MPI ranks to GPUs on each node. Such a script is not necessary when the application handles process-gpu binding. This script simply sets the environment variable <code>CUDA_VISIBLE_DEVICES</code> to a restricted set of GPUs (e.g. each MPI rank sees only one GPU). Otherwise, users would find that all MPI ranks on a node will target the first GPU likely having a negative impact on performance. An example for this script is available in the Getting Started repo and copied below.   </p>"},{"location":"running-jobs/example-job-scripts/#hardware-threads_1","title":"Hardware threads","text":"<pre><code>#!/bin/bash -l\n#PBS -l select=2:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -l filesystems=home:eagle\n#PBS -q debug\n#PBS -A Catalyst\n\n# Enable GPU-MPI (if supported by application)\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\n# Change to working directory\ncd ${PBS_O_WORKDIR}\n\n# MPI and OpenMP settings\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=$(nvidia-smi -L | wc -l)\nNDEPTH=16\nNTHREADS=16\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\n# For applications that internally handle binding MPI/OpenMP processes to GPUs\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind numa --env OMP_NUM_THREADS=${NTHREADS} -env OMP_PLACES=threads ./hello_affinity\n\n# For applications that need mpiexec to bind MPI ranks to GPUs\n#mpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind numa --env OMP_NUM_THREADS=${NTHREADS} -env OMP_PLACES=threads ./set_affinity_gpu_polaris.sh ./hello_affinity\n</code></pre> <p>As in the previous hardware threads example, the MPI ranks are spaced apart assuming the user wants to utilize all 64 logical cores (achieved by setting <code>NTHREADS=$NDEPTH=16</code> and <code>--cpu-bind numa</code> here).</p> <p>In this script, we have added <code>-j oe</code> to the list of PBS options; <code>-j oe</code> combines stdout and stderr to the same file and uses the stdout filename provided (if provided). <code>-j eo</code> would do the same but use the stderr filename provided. Without these options, separate files containing stdout and stderr of the job are produced.</p> <p>Here we compare two bare-bones PBS submission scripts for a CUDA example with and without MPI:</p> No MPIWith MPI <pre><code>#!/bin/bash\n#PBS -l select=1\n#PBS -l walltime=00:10:00\n#PBS -q debug\n#PBS -l filesystems=home\n#PBS -A &lt;project-name&gt;\n#PBS -o logs/\n#PBS -e logs/\n\n\n\n$HOME/ALCFBeginnersGuide/polaris/examples/01_example_cu\n</code></pre> <pre><code>#!/bin/bash\n#PBS -l select=2\n#PBS -l walltime=00:10:00\n#PBS -q debug\n#PBS -l filesystems=home\n#PBS -A &lt;project-name&gt;\n#PBS -o logs/\n#PBS -e logs/\n\n# Count number of nodes assigned\nNNODES=`wc -l &lt; $PBS_NODEFILE`\n# set 1 MPI rank per GPU\nNRANKS_PER_NODE=4\n# calculate total ranks\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE}\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} $HOME/ALCFBeginnersGuide/polaris/examples/01_example_mpi\n</code></pre>"},{"location":"running-jobs/example-job-scripts/#setting-gpu-affinity-for-each-mpi-rank","title":"Setting GPU affinity for each MPI rank","text":"<p>The <code>CUDA_VISIBLE_DEVICES</code> environment variable is provided for users to set which GPUs on a node are accessible to an application or MPI ranks started on a node.</p> <p>A copy of the small helper script provided in the Getting Started repo is provided below for reference:</p> GPU affinity script <pre><code>#!/bin/bash -l\nnum_gpus=4\n# need to assign GPUs in reverse order due to topology\n# See Polaris Device Affinity Information https://www.alcf.anl.gov/support/user-guides/polaris/hardware-overview/machine-overview/index.html\ngpu=$((${num_gpus} - 1 - ${PMI_LOCAL_RANK} % ${num_gpus}))\nexport CUDA_VISIBLE_DEVICES=$gpu\necho \u201cRANK= ${PMI_RANK} LOCAL_RANK= ${PMI_LOCAL_RANK} gpu= ${gpu}\u201d\nexec \"$@\"\n</code></pre> <p>Note</p> <p>The <code>echo</code> command prints a helpful message for the user to confirm the desired mapping is achieved. Users are encouraged to edit this file as necessary for their particular needs.</p> <p>Warning</p> <p>If planning large-scale runs with many thousands of MPI ranks, it is advised to comment out the <code>echo</code> command above so as not to have thousands of lines of output written to <code>stdout</code>.</p>"},{"location":"running-jobs/example-job-scripts/#using-mps-on-the-gpus","title":"Using MPS on the GPUs","text":"<p>Documentation for the NVIDIA Multi-Process Service (MPS) can be found here</p> <p>In the script below, note that if you are going to run this as a multi-node job you will need to do this on every compute node, and you will need to ensure that the paths you specify for <code>CUDA_MPS_PIPE_DIRECTORY</code> and <code>CUDA_MPS_LOG_DIRECTORY</code> do not \"collide\" and end up with all the nodes writing to the same place.</p> <p>An example is available in the Getting Started Repo and discussed below. The local SSDs or <code>/dev/shm</code> or incorporation of the node name into the path would all be possible ways of dealing with that issue.</p> <pre><code>#!/bin/bash -l\nexport CUDA_MPS_PIPE_DIRECTORY=&lt;/path/writeable/by/you&gt;\nexport CUDA_MPS_LOG_DIRECTORY=&lt;/path/writeable/by/you&gt;\nCUDA_VISIBLE_DEVICES=0,1,2,3 nvidia-cuda-mps-control -d\necho \"start_server -uid $( id -u )\" | nvidia-cuda-mps-control\n</code></pre> <p>to verify the control service is running:</p> <pre><code>$ nvidia-smi | grep -B1 -A15 Processes\n</code></pre> <p>and the output should look similar to this:</p> <pre><code>+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A     58874      C   nvidia-cuda-mps-server             27MiB |\n|    1   N/A  N/A     58874      C   nvidia-cuda-mps-server             27MiB |\n|    2   N/A  N/A     58874      C   nvidia-cuda-mps-server             27MiB |\n|    3   N/A  N/A     58874      C   nvidia-cuda-mps-server             27MiB |\n+-----------------------------------------------------------------------------+\n</code></pre> <p>to shut down the service:</p> <p><code>echo \"quit\" | nvidia-cuda-mps-control</code></p> <p>to verify the service shut down properly:</p> <p><code>nvidia-smi | grep -B1 -A15 Processes</code></p> <p>and the output should look like this:</p> <pre><code>+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre>"},{"location":"running-jobs/example-job-scripts/#using-mps-in-multi-node-jobs","title":"Using MPS in Multi-node Jobs","text":"<p>As stated earlier, it is important to start the MPS control service on each node in a job that requires it.  An example is available in the Getting Started Repo. The helper script <code>enable_mps_polaris.sh</code> can be used to start the MPS on a node.</p> <p><pre><code>#!/bin/bash -l\n\nexport CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps\nexport CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log\nCUDA_VISIBLE_DEVICES=0,1,2,3 nvidia-cuda-mps-control -d\necho \"start_server -uid $( id -u )\" | nvidia-cuda-mps-control\n</code></pre> The helper script <code>disable_mps_polaris.sh</code> can be used to disable MPS at appropriate points during a job script, if needed.</p> <p><pre><code>#!/bin/bash -l\n\necho quit | nvidia-cuda-mps-control\n</code></pre> In the example job script <code>submit.sh</code> below, MPS is first enabled on all nodes in the job using <code>mpiexec -n ${NNODES} --ppn 1</code> to launch the enablement script using a single MPI rank on each compute node. The application is then run as normally. If desired, a similar one-rank-per-node <code>mpiexec</code> command can be used to disable MPS on all the nodes in a job.</p> <pre><code>#!/bin/bash -l\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q debug\n#PBS -A Catalyst\n#PBS -l filesystems=home:grand:eagle\n\ncd ${PBS_O_WORKDIR}\n\n# MPI example w/ 8 MPI ranks per node spread evenly across cores\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=8\nNDEPTH=8\nNTHREADS=1\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\n# Enable MPS on each node allocated to job\nmpiexec -n ${NNODES} --ppn 1 ./enable_mps_polaris.sh\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./hello_affinity\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ./set_affinity_gpu_polaris.sh ./hello_affinity\n\n# Disable MPS on each node allocated to job\nmpiexec -n ${NNODES} --ppn 1 ./disable_mps_polaris.sh\n</code></pre>"},{"location":"running-jobs/example-job-scripts/#single-node-ensemble-calculations-example","title":"Single-node Ensemble Calculations Example","text":"<p>In the script below, a set of four applications are launched simultaneously on a single node. Each application runs on 8 MPI ranks and targets a specific GPU using the <code>CUDA_VISIBLE_DEVICES</code> environment variable. In the first instance, MPI ranks 0-7 will spawn on CPUs 24-31, and GPU 0 is used. This pairing of CPUs and GPU is based on output of the <code>nvidia-smi topo-m</code> command showing which CPUs share a NUMA domain with each GPU. It is important to background processes using <code>&amp;</code> and to <code>wait</code> for all runs to complete before exiting the script or continuing on with additional work. Note, multiple applications can run on the same set of CPU resources, but it may not be optimal depending on the workload. An example is available in the Getting Started Repo.</p> <pre><code>#!/bin/bash -l\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q debug\n#PBS -A Catalyst\n#PBS -l filesystems=home:grand:eagle\n\n#cd ${PBS_O_WORKDIR}\n\n# MPI example w/ 8 MPI ranks per node spread evenly across cores\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=8\nNTHREADS=1\n\nnvidia-smi topo -m\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\nexport CUDA_VISIBLE_DEVICES=0\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --cpu-bind list:24:25:26:27:28:29:30:31 ./hello_affinity &amp;\n\nexport CUDA_VISIBLE_DEVICES=1\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --cpu-bind list:16:17:18:19:20:21:22:23 ./hello_affinity &amp;\n\nexport CUDA_VISIBLE_DEVICES=2\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --cpu-bind list:8:9:10:11:12:13:14:15 ./hello_affinity &amp;\n\nexport CUDA_VISIBLE_DEVICES=3\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --cpu-bind list:0:1:2:3:4:5:6:7 ./hello_affinity &amp;\n\nwait\n</code></pre>"},{"location":"running-jobs/example-job-scripts/#multi-node-ensemble-calculations-example","title":"Multi-node Ensemble Calculations Example","text":"<p>To run multiple concurrent applications on distinct sets of nodes, one simply needs to provide appropriate hostfiles to the <code>mpiexec</code> command. The <code>split</code> unix command is one convenient way to create several unique hostfiles, each containing a subset of nodes available to the job. In the 8-node example below, a total of four applications will be launched on separate sets of nodes. The <code>$PBS_NODEFILE</code> file will be split into several hostfiles, each containing two lines (nodes). These smaller hostfiles are then used as the argument to the <code>--hostfile</code> argument of <code>mpiexec</code> to the launch applications. It is important to background processes using <code>&amp;</code> and to <code>wait</code> for applications to finish running before leaving the script or continuing on with additional work. Note, multiple applications can run on the same set of CPU resources, but it may not be optimal depending on the workload. An example is available in the Getting Started Repo.</p> <pre><code>#!/bin/bash -l\n#PBS -l select=8:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:30:00\n#PBS -q debug-scaling\n#PBS -A Catalyst\n#PBS -l filesystems=home:grand:eagle\n\ncd ${PBS_O_WORKDIR}\n\n# MPI example w/ multiple runs per batch job\nNNODES=`wc -l &lt; $PBS_NODEFILE`\n\n# Settings for each run: 2 nodes, 4 MPI ranks per node spread evenly across cores\n# User must ensure there are enough nodes in job to support all concurrent runs\nNUM_NODES_PER_MPI=2\nNRANKS_PER_NODE=4\nNDEPTH=8\nNTHREADS=1\n\nNTOTRANKS=$(( NUM_NODES_PER_MPI * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} NUM_NODES_PER_MPI= ${NUM_NODES_PER_MPI} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\n# Increase value of suffix-length if more than 99 jobs\nsplit --lines=${NUM_NODES_PER_MPI} --numeric-suffixes=1 --suffix-length=2 $PBS_NODEFILE local_hostfile.\n\nfor lh in local_hostfile*\ndo\n  echo \"Launching mpiexec w/ ${lh}\"\n  mpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --hostfile ${lh} --depth=${NDEPTH} --cpu-bind depth ./hello_affinity &amp;\n  sleep 1s\ndone\n\nwait\n</code></pre>"},{"location":"running-jobs/example-job-scripts/#job-array-example","title":"Job array example","text":"<p>In situations where you wish to repeat a job multiple times with a small change each time, such as in a parameter space study, a job array may be an option.  Unlike the multi-node ensemble case above, each subjob in a job array is its own job and will have its own initialization and tear-down by PBS.  Also, a job array will not block all nodes for the length of the longest running task, as is the case for an ensemble job.  Jobs on polaris cannot share nodes with other jobs, so job arrays on Polaris cannot be used to distribute work to different cpu cores or gpus on a node.  In that case, an ensemble job or using <code>mpiexec</code> as a parallel launcher can accomplish that goal.</p> <p>Both ensemble jobs and job arrays become unwieldy and inefficient for very large numbers of tasks.  They either have limits to the number of tasks that can be created at once (job arrays) or are unable to refill idle nodes when tasks complete (ensemble jobs).  In such cases, a workflow management tool that can manage the running of tasks is recommended.</p>"},{"location":"running-jobs/example-job-scripts/#job-array-submission-scripts","title":"Job array submission scripts","text":"<p>An example job array submission script:</p> <p><pre><code>#!/bin/bash -l\n#PBS -l select=1:system=polaris\n#PBS -l place=scatter\n#PBS -l walltime=0:10:00\n#PBS -q preemptable\n#PBS -A datascience\n#PBS -l filesystems=home:grand:eagle\n#PBS -j oe\n#PBS -r y\n#PBS -J 0-7:2\n\ncd ${PBS_O_WORKDIR}\n\n# Create a unique subdirectory for subjob with PBS_ARRAY_INDEX\nSUBDIRECTORY=\"${PBS_ARRAY_INDEX}\"\nmkdir -p ${SUBDIRECTORY}\ncd ${SUBDIRECTORY}\n\n# File name where stdout and stderr of application will be directed in subjob subdirectory\nOUT_FILE=\"subjob_${PBS_ARRAY_INDEX}.out\"\n\necho \"Running subjob ${SUBDIRECTORY}\"\necho \"Directing application output to ${SUBDIRECTORY}/${OUT_FILE}\"\n\n# MPI example w/ 16 MPI ranks per node spread evenly across cores\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS_PER_NODE=16\nNDEPTH=4\nNTHREADS=1\n\nNTOTRANKS=$(( NNODES * NRANKS_PER_NODE ))\necho \"NUM_OF_NODES= ${NNODES} TOTAL_NUM_RANKS= ${NTOTRANKS} RANKS_PER_NODE= ${NRANKS_PER_NODE} THREADS_PER_RANK= ${NTHREADS}\"\n\nAPP_PATH=${PBS_O_WORKDIR}/hello_affinity\n\nmpiexec -n ${NTOTRANKS} --ppn ${NRANKS_PER_NODE} --depth=${NDEPTH} --cpu-bind depth ${APP_PATH} &amp;&gt; ${OUT_FILE}\n</code></pre> There are two required options for job arrays in PBS: <code>-r</code> and <code>-J</code>.  </p> <p>The <code>-r</code> option must be set like this: <pre><code>#PBS -r y\n</code></pre></p> <p>The <code>-J</code> option sets the number of subjobs in the array and the value of their array indices.  The example script above will run 4 subjobs and space their array indices in increments of 2, so the array indices will be 0, 2, 4 and 6.</p> <p>The form the <code>-J</code> option takes is <pre><code>#PBS -J &lt;start_index&gt;-&lt;end_index&gt;:&lt;skip_index&gt;%&lt;num_concurrent&gt;\n</code></pre> * <code>&lt;start_index&gt;</code> is the index of the first job in the array * <code>&lt;end_index&gt;</code> is the index of the last job in the array * <code>&lt;skip_index&gt;</code> is the number of index integers to skip between subjobs * <code>&lt;num_concurrent&gt;</code> is the maximum number of subjobs that will run concurrently at one time</p> <p>The within a subjob the environment variable <code>PBS_ARRAY_INDEX</code> will contains the index of the subjob in the array.  It can be used in the job script to set the value or paths of inputs or outputs.</p>"},{"location":"running-jobs/example-job-scripts/#interacting-with-job-arrays","title":"Interacting with job arrays","text":"<p>The status of job arrays can be queried with the command: <pre><code>qstat -t\n</code></pre></p> <p>When interacting with a job array with commands like <code>qdel</code> or <code>qalter</code>, include the brackets with the jobid, e.g.: <pre><code>qdel 1991684[]\n</code></pre></p>"},{"location":"running-jobs/example-job-scripts/#limits-on-job-arrays","title":"Limits on job arrays","text":"<p>The number of subjobs in a job array is limited by the number of jobs that can be submitted to the queue.</p> <p>On Polaris, for the debug queue, that is 1, for preemptable, that is 20, and for prod that is 10.  </p> <p>The limit for prod on Polaris is 10 because 10 is the maximum number of jobs that can be routed by prod to one of the execution queues (small, medium, or large).  One note, PBS will allow job array submissions of up to 100 subjobs in prod, however, these job arrays will not run because they will not route to an execution queue.  This is a known issue on Polaris.</p>"},{"location":"running-jobs/job-and-queue-scheduling/","title":"Running Jobs using PBS","text":""},{"location":"running-jobs/job-and-queue-scheduling/#documentation-tools","title":"Documentation / Tools","text":"<ul> <li>The PBS \"BigBook\": This is really excellent.  We highly suggest you download it and search through it when you have questions.  However, it is big at about 2000 pages / 40MB and contains a bunch of stuff you don't really need, so you can also download the guides separately here:<ul> <li>The PBS Users Guide: This is the users guide.</li> <li>The PBS Reference Guide: This is the Reference Guide.  It shows every option and gives you details on how to format various elements on the command line.</li> </ul> </li> <li>Cobalt qsub options to PBS qsub options: shows how to map cobalt command line options to PBS command line options.  Can be found at the link above.</li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#introduction","title":"Introduction","text":"<p>At a high level, getting computational tasks run on an HPC system is a two-step process:</p> <ol> <li> <p>You request and get allocated resources (we allocate at the node level, but some facilities you request number of cores and RAM, etc.) on one or more of the systems.    This is accomplished by interacting with the job scheduler / workload manager.  In the ALCF we use PBS Professional.</p> </li> <li> <p>You execute your tasks on those resources.    This is accomplished in your job script by interacting with various system services (MPI, OpenMP, the HPE PALS task launch system, etc.)</p> </li> </ol> <p>Our documentation is organized in two sections aligned with the two steps described above.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Obtaining and managing compute resources at ALCF - General PBS information common to all systems<ul> <li>Definitions and Notes</li> <li>Quick Start</li> <li>qsub - submit a job to run</li> <li>qstat - query the status of jobs/queues</li> <li>qalter - alter a queued job</li> <li>qdel - delete a queued or running job</li> <li>qmove - move a job to a different queue</li> <li>qhold,qrls - place/release a hold on a job in a queue</li> <li>qselect - utility to select jobids that meet criteria</li> <li>qmsg - write a message into a jobs output file</li> <li>qsig - send a signal to a job</li> <li>pbsnodes - Get information about the current state of nodes</li> <li>Using Fakeroot with Singularity</li> </ul> </li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#obtaining-and-managing-compute-resources-at-alcf","title":"Obtaining and managing compute resources at ALCF","text":""},{"location":"running-jobs/job-and-queue-scheduling/#definitions-and-notes","title":"Definitions and Notes","text":"<p><code>chunk</code>: A set of resources allocated as a unit to a job. Specified inside a selection directive. All parts of a chunk come from the same host.  In a typical MPI (Message-Passing Interface) job, there is one chunk per MPI process.</p> <p><code>vnode</code>: A virtual node, or vnode, is an abstract object representing a host or a set of resources which form a usable part of an execution host. This could be an entire host, or a nodeboard or a blade. A single host can be made up of multiple vnodes. Each vnode can be managed and scheduled independently. Each vnode in a complex must have a unique name. Vnodes on a host can share resources, such as node-locked licenses.  PBS operates on vnodes.  A vnode can, and in ALCF often will, represent an entire host, but it doesn't have to.  For instance, there is a mode on Polaris where we could have each physical host look like four vnodes, each with 16 threads, 1/4 of the RAM and one A100.</p> <p><code>ncpus</code>: Number of resources available to execute a program. In ALCF, given the way we configure PBS, this equates to a hardware thread.  For example, a single socket node with a 32 core CPU, each with two hardware threads would report that as ncpus=64.</p> <p><code>ngpus</code>: The number of allocable GPUs on the vnode.  For an NVIDIA A100, this could be one, however, if we enable Multi Instance GPU (MIG) mode and use cgroups it could be as high as 7.</p> <p><code>job</code>: A job equates to a qsub.  A set of resources allocated to you for a period of time.  Your will execute one or more <code>tasks</code> on those resources during your job.</p> <p><code>task</code>: A single execution on the resources of your job, often an <code>mpiexec</code> invocation launched by PALS or PMIx.  You may run one task or many tasks during your job.  You may run tasks sequentially or divide your resources up and run several tasks concurrently.  Also sometimes referred to as job steps.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#quick-start","title":"Quick Start","text":"<p>If you are an ALCF user and are familiar with Cobalt, you will find the PBS commands very similar though the options to qsub are quite different.  Here are the \"Big Four\" commands you will use:</p> <ol> <li><code>qsub</code>: request resources (generally compute nodes) to run your job and start your script/executable on the head node.  Here is the minimal qsub allowed at the ALCF:<ul> <li><code>qsub -A &lt;project&gt; -l select=&lt;# of nodes&gt;,walltime=HH:MM:SS,filesystems=fs1:fs2 &lt;your job script&gt;</code></li> <li>The <code>-A</code>, <code>walltime</code>, and <code>filesystems</code> are mandatory.  You will receive errors if they are not specified.</li> <li>We automatically add <code>-k doe</code> for you.  This streams your output back rather than spooling it and copying it back at the end of the job.  It probably isn't a bad idea to specify it in your script, but we enforce that option, so if you try and change it, you will get an error.</li> <li>It is highly likely you will also want to add <code>-l place=scatter</code> so that each of your chunks (<code>&lt;# of nodes&gt;</code>) gets its own vnode.</li> <li>If you want to run an executable rather than a script replace <code>&lt;your jobs script&gt;</code> in the example above with <code>-- &lt;your executable&gt;</code> (that is dash dash)</li> <li>PBS Documentation: Users Guide, Chapter 2, page UG-11 and Reference Guide Chapter 2, section 2.57, page RG-216</li> </ul> </li> <li><code>qstat</code>: check on the status of your jobs or queues<ul> <li>Try these variations and see which you like best: <code>qstat</code>, <code>qstat -was</code>, <code>qstat -was1</code>, <code>qstat -wan</code>, <code>qstat -wan1</code>. Add <code>-x</code> to see jobs that have completed.  We keep two weeks of history.</li> <li><code>qstat -Q</code> will list all the queues in case you forget.</li> <li>PBS Documentation: Users Guide Sec. 10.2, page UG-175; Reference Guide Sec. 2.55, page RG-200</li> </ul> </li> <li><code>qalter</code>: update your request for resources<ul> <li>Just like qsub, just add a jobid at the end.  Only works before the job starts;</li> <li>If you want to change the walltime to 30 minutes: <code>qalter -l walltime=30:00:00 &lt;jobid&gt;</code></li> <li>PBS Documentation: Users Guide Sec. 9.2, page UG-168; Reference Guide Sec. 2.40, page RG-130</li> </ul> </li> <li><code>qdel</code>: cancel a job that you don't need. This will also kill a running job<ul> <li><code>qdel &lt;jobid&gt;</code></li> <li>PBS Documentation: Users Guide Sec. 9.3, page UG-170; Reference Guide Sec. 2.41, page RG-143</li> </ul> </li> </ol> <p>Note: The page numbers in the PBS guides are unique.  If you search for the specified page number it will take you directly to the relevant page.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#qsub-submit-a-job-to-run","title":"<code>qsub</code>: submit a job to run","text":"<p>Users Guide, Chapter 2, page UG-11 and Reference Guide Chapter 2, section 2.57, page RG-216</p> <p>At the ALCF, your qsub will likely use the following parameters:</p> <p><code>qsub -A &lt;project&gt; -k doe -l select=&lt;#&gt;:system=&lt;name&gt;, walltime=HH:MM:SS, filesystems=fs1:fs2, place=scatter &lt;your job script&gt;</code></p> <p>Where:</p> <ul> <li>project is the project name associated with your allocation.  What you check the balance of with the <code>sbank</code> command.  This is a mandatory option at the ALCF.  If you don't include it you will get <code>qsub: Account_Name is required to be set.</code></li> <li>-k doe is telling pbs to stream your output rather than buffer it on the compute nodes and then scp it at the end of the job.  Note we will automatically add this if you don't specify it.  We enforce this option, so if  you try and specify any other output handling you will get an error.</li> <li># of chunks (typically nodes). Each of our systems has a PBS \"resource\" called <code>system</code> defined and set to the system name (polaris, sunspot, etc)</li> <li><code>walltime=HH:MM:SS</code> specifying a wall time is mandatory at the ALCF.  Valid wall times depend on the queue you are using.  There is a table with the queues for each machine at the end of this section and in the machine specific documentation.</li> <li><code>filesystems=fs1:fs2:...</code> Specifying which filesystems your application uses is mandatory at ALCF.  The reason for this is if a filesystem goes down, we have a way of making PBS aware of that and it won't run jobs that need that filesystem.  If you don't specify filesystems you will receive the following error: <code>qsub: Resource: filesystems is required to be set.</code></li> <li><code>place=scatter</code> is telling PBS you want each of your chunks on a separate vnode.  By default, PBS will pack your chunks to get maximum utilization.  If you requested <code>ncpus=1</code> and <code>chunks=64</code> without <code>place=scatter</code> on a system with <code>ncpus=64</code>, all your chunks would end up on one node.</li> <li>Your job script:  See Example Job Scripts for more information about how to build your job script.  For options that wont change, you do have the option of taking things off the command line and putting them in your job script.  For instance the above command line could be simplified to <code>qsub -l select=&lt;#&gt; &lt;your job script&gt;</code> if you added the following to the top (the PBS directives have to be before any executable line) of your job script:</li> </ul> <pre><code>#PBS -A &lt;project&gt;\n#PBS -k doe\n#PBS -l walltime=HH:MM:SS\n#PBS -l filesystems=fs1:fs2\n</code></pre> <p>Also note that if you want to run an executable directly rather than a script you use two dashes and the executable name in place of your script name like this: <code>-- /usr/bin/sleep 600</code></p>"},{"location":"running-jobs/job-and-queue-scheduling/#more-detail","title":"More detail:","text":"<p>The single biggest difference between Cobalt and PBS is the way you select resources when submitting a job.  In Cobalt, every system had its own Cobalt server and you just specified the number of nodes you wanted (-n).  With PBS, we are planning on running a single \"PBS Complex\" which means there will be a single PBS server for all systems in the ALCF and you need to specify enough constraints to get your job to run on the resources you want/need.  One advantage of this is that getting resources from two different systems or \"co-scheduling\" is trivially possible.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#resource-selection-and-job-placement","title":"Resource Selection and Job Placement","text":"<p>Section 2.57.2.6 RG-219 Requesting Resources and Placing jobs in the Reference Guide.</p> <p>Resources come in two flavors:</p> <ul> <li>Job Wide: Walltime is the most common example of a job wide resource.  You use the <code>-l</code> option to specify job wide resources, i.e. <code>-l walltime=06:00:00</code>.  All the resources in the job have the same walltime.</li> <li><code>-l &lt;resource name&gt;=&lt;value&gt;[,&lt;resource name&gt;=&lt;value&gt; ...]</code></li> <li>Chunks: (see the definition above) This is how you describe what your needs are to run your job.  You do this with the <code>-l select=</code> syntax.  In the ALCF, we do whole node scheduling and every node has a resource called <code>system</code> which is set to the system name it belongs to (Polaris, Aurora, etc).  This means you can typically get away with the very simple <code>-l select=128:system=foo</code> which will give you 128 complete nodes on system foo.</li> <li><code>-l select=[&lt;N&gt;:]&lt;chunk&gt;[+[&lt;N&gt;:]&lt;chunk&gt; ...]</code> where N specifies how many of that chunk and a chunk is of the form:</li> <li><code>&lt;resource name&gt;=&lt;value&gt;[:&lt;resource name&gt;=&lt;value&gt; ...]</code></li> <li>Here is a hypothetical example that would select resources with A100s and other resources with A40 GPUs.  PBS takes care of co-scheduling the nodes on the two systems for you transparently.  Note that in this case since we did not specify <code>system=</code> if there were multiple systems that could satisfy a chunk you wouldn't know ahead of time which system you would get.</li> </ul> <p><code>-l select=128:ncpus=64:ngpus=4:gputype=A100+32:ncpus=64:ngpus=2:gputype=A40</code></p> <p>You also have to tell PBS how you want the chunks distributed across the physical hardware.  You do that via the <code>-l place</code> option:</p> <ul> <li><code>-l place=[&lt;arrangement&gt;][: &lt;sharing&gt; ][: &lt;grouping&gt;]</code> where</li> <li>arrangement is one of <code>free | pack | scatter | vscatter</code><ul> <li>unless you have a specific reason to do otherwise, you probably want to set this to <code>scatter</code>, otherwise you may not get what you expect.  For instance on a host with ncpus=64, if you requested <code>-l select=8:ncpus=8</code> you could end up with all of our chunks on one node.</li> <li><code>free</code> means PBS can distribute them as it sees fit</li> <li><code>pack</code> means all chunks from one host.  Note that this is not the minimum number of hosts, it is one host.  If the chunks can't fit on one host, the qsub will fail.</li> <li><code>scatter</code> means take only one chunk from any given host.</li> <li><code>vscatter</code> means take only one chunk from any given vnode.  If a host has multiple vnodes, you could end up with more than one chunk on the host.</li> </ul> </li> <li>sharing is one of <code>excl | shared | exclhost</code> where<ul> <li>NOTE: Node configuration can override your requested sharing mode.  For instance, in most cases ALCF sets the nodes to <code>force_exclhost</code>, so normally you don't have to specify this.</li> <li><code>excl</code> means this job gets the entire vnode</li> <li><code>shared</code> means the vnode could be shared with another job from another user.</li> <li><code>exclhost</code> means this job gets the entire host, even if it has multiple vnodes.</li> </ul> </li> <li>group=<code>&lt;resource name&gt;</code><ul> <li>As an example, for machines that use a dragonfly network topology, we provide a PBS resource named <code>tier1</code> indicating which dragonfly group a node is in.  If you wanted to ensure that all the chunks came from a single dragonfly group, you could specify <code>place=group=tier1</code> as part of your qsub.  <code>tier0</code> is rack granularity, so <code>group=tier0</code> would ensure your nodes all came from one rack.  Note that if you requested more nodes than were available in a rack your job would never run and you would see something like <code>Not Running: Insufficient amount of resource: tier0</code>.</li> </ul> </li> </ul> <p>We have defined placement sets for the tier0 and tier1 resources.  As a result, if you don't specify a grouping PBS will preferentially group your nodes in a placement set, but it won't drain or delay your job start to do so.  For example, if you request 10 nodes and don't specify a grouping, if 10 nodes are available in the same rack, all your nodes will be in one rack.  If not, but there are 10 nodes in a single dragonfly group, all your nodes will be in one dragonfly group.  If you wish to specify a specific rack or dragonfly group, that is accomplished via the select syntax.  For instance, <code>qsub ... -l select=10:tier1=g0</code> would force your 10 nodes to be in dragonfly group 0.</p> <p>Here is a heavily commented sample PBS submission script that shows some more of the options, but remember that the PBS manuals referenced at the top of this page are the ultimate resource.</p> <pre><code>#!/bin/bash -l\n# UG Section 2.5, page UG-24 Job Submission Options\n# Add another # at the beginning of the line to comment out a line\n# NOTE: adding a switch to the command line will override values in this file.\n\n# These options are MANDATORY at ALCF; Your qsub will fail if you don't provide them.\n#PBS -A &lt;short project name&gt;\n#PBS -l walltime=HH:MM:SS\n#file systems used by the job\n#PBS -l filesystems=home:eagle\n\n\n# Highly recommended\n# The first 15 characters of the job name are displayed in the qstat output:\n#PBS -N &lt;name&gt;\n\n# If you need a queue other than the default, which is prod (uncomment to use)\n##PBS -q &lt;queue name&gt;\n\n# Controlling the output of your application\n# UG Sec 3.3 page UG-42 Managing Output and Error Files\n# By default, PBS spools your output on the compute node and then uses scp to move it the\n# destination directory after the job finishes.  Since we have globally mounted file systems\n# it is highly recommended that you use the -k option to write directly to the destination\n# the doe stands for direct, output, error\n#PBS -k doe\n#PBS -o &lt;path for stdout&gt;\n#PBS -e &lt;path for stderr&gt;\n\n# If you want to merge stdout and stderr, use the -j option\n# oe=merge stdout/stderr to stdout, eo=merge stderr/stdout to stderr, n=don't merge\n#PBS -j n\n\n# Controlling email notifications\n# UG Sec 2.5.1, page UG-25 Specifying Email Notification\n# When to send email b=job begin, e=job end, a=job abort, j=subjobs (job arrays), n=no mail\n#PBS -m be\n# Be default, mail goes to the submitter, use this option to add others (uncomment to use)\n#PBS -M &lt;email addresses&gt;\n\n# Setting job dependencies\n# UG Section 6.2, page UG-109 Using Job Dependencies\n# There are many options for how to set up dependencies;  afterok will give behavior similar\n# to Cobalt (uncomment to use)\n##PBS depend=afterok:&lt;jobid&gt;:&lt;jobid&gt;\n\n# Environment variables (uncomment to use)\n# UG Section 6.12, page UG-126 Using Environment Variables\n# RG Sect 2.57.7, page RG-233 Environment variables PBS puts in the job environment\n##PBS -v &lt;variable list&gt;\n## -v a=10, \"var2='A,B'\", c=20, HOME=/home/zzz\n##PBS -V exports all the environment variables in your environment to the compute node\n\n\n# The rest is an example of how an MPI job might be set up\necho Working directory is $PBS_O_WORKDIR\ncd $PBS_O_WORKDIR\n\necho Jobid: $PBS_JOBID\necho Running on host `hostname`\necho Running on nodes `cat $PBS_NODEFILE`\n\nNNODES=`wc -l &lt; $PBS_NODEFILE`\nNRANKS=1           # Number of MPI ranks per node\nNDEPTH=1           # Number of hardware threads per rank, spacing between MPI ranks on a node\nNTHREADS=1         # Number of OMP threads per rank, given to OMP_NUM_THREADS\n\nNTOTRANKS=$(( NNODES * NRANKS ))\n\necho \"NUM_OF_NODES=${NNODES}  TOTAL_NUM_RANKS=${NTOTRANKS}  RANKS_PER_NODE=${NRANKS}  THREADS_PER_RANK=${NTHREADS}\"\n\nmpiexec --np ${NTOTRANKS} -ppn ${NRANKS} -d ${NDEPTH} -env OMP_NUM_THREADS=${NTHREADS} ./hello_mpi\n</code></pre>"},{"location":"running-jobs/job-and-queue-scheduling/#email-notifications","title":"Email Notifications","text":"<p>Users should add <code>-M &lt;email address&gt;</code> if they want notifications as a best practice.</p> <p>Note: For users with '@alcf.anl.gov' email addressed, PBS will send out an email once the job has ended by default. If you do not want to receive these notifications, you will need to add <code>#PBS -m n</code> to your script."},{"location":"running-jobs/job-and-queue-scheduling/#specifying-filesystems","title":"Specifying Filesystems","text":"<p>Note: The <code>filesystems</code> attribute is mandatory. If you do not specify a filesystem(s) you will receive the following error message upon submission:</p> <p><code>qsub: Resource: filesystems is required to be set.</code></p> <p>Valid filesystems are <code>home</code>, <code>eagle</code>, and <code>grand</code>.  For example, to request the home and eagle filesystems for your job you would add <code>-l filesystems=home:eagle</code> to your qsub command.</p> <p>If a job is submitted while a filesystem it requested is marked down, the job will be queued but will not run, with a message in the comment field of the job as to why it is not running. Run <code>qstat -f &lt;jobid&gt;</code> to see the comment field. For example, if the job requested for eagle and if Eagle is unavailable, the comment field will have <code>Can Never Run: Insufficient amount of server resource: eagle_fs (True != False)</code>).  Once the affected filesystem has been returned to normal operation, and the filesystem is marked as being available, the job will then be scheduled normally. The job cannot run until all filesystems requested by the job are available.</p> <p>If a job requesting a filesystem that is marked down is already in the queue, the job will be not run until all of its requested filesystems are available.</p> <p>An example of a job requesting filesystems:</p> <p><code>qsub -l select=10:ncpus=64,walltime=30:00,filesystems=grand:home -A ProjectX -q prod my_job.sh</code></p> <p>To update the filesystems list for your job, use <code>qalter</code>.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#qsub-examples","title":"qsub examples","text":"<ul> <li><code>qsub -A my_allocation -l select=4:system=polaris -l filesystems=home:eagle -l walltime=30:00 -q debug-scaling -- a.out</code><ul> <li>run a.out on 4 chunks on polaris with a walltime of 30 minutes in debug-scaling queue; charge my_allocation;</li> <li>Since we allocate full nodes on Polaris, 4 chunks will be 4 nodes.  If we shared nodes, that would be 4 threads.</li> <li>use the -- (dash dash) syntax when directly running an executable.</li> </ul> </li> <li><code>qsub -A my_allocation -l place=scatter  -l filesystems=home:eagle -l select=32:ncpus=32 -q prod -l walltime=30:00 mpi_mm_64.sh</code><ul> <li>32 chunks on any system that meets the requirements. Each chunk must have 32 HW threads; <code>place=scatter</code> means use a different vnode for each chunk, even if you could fit more than one on a vnode. Use the queue named <code>prod</code>.</li> </ul> </li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#qstat-query-the-status-of-jobsqueues","title":"<code>qstat</code>: Query the status of jobs/queues","text":"<p>Users Guide Sec. 10.2, page UG-175; Reference Guide Sec. 2.55, page RG-200</p>"},{"location":"running-jobs/job-and-queue-scheduling/#jobs","title":"Jobs","text":"<p>At it's most basic, you just type <code>qstat</code> and it will list all the jobs currently running, queued, or held on the system.  If you are interested in a specific job or jobs, you can provide a space separated list on the command line: <code>qstat job1 job2...</code>.</p> <pre><code>Job id            Name             User              Time Use S Queue\n----------------  ---------------- ----------------  -------- - -----\n349726.polaris-p* PDE2             user1                    0 Q prod\n336987.polaris-p* inf_clDB         user2                    0 H large\n353205.polaris-p* 3d-2.sub         user3             2044:14* R large\n</code></pre> <p>One of the annoying things about <code>qstat</code> is that the output fields are fixed with and it will truncate the output.  This is indicated by an asterisk as the last character.  You can add <code>-w</code> for wide.  It doesn't prevent truncation, but makes it less likely.  A useful variant is <code>qstat -was1</code>.  It shows the number of nodes, tasks, the requested walltime, and the comment, all on one line.  <code>qstat -wan</code> will give you the node list you ran on, just remember that can be long.  If you want an estimate of when the job will start, add the <code>-T</code> option.  Note that start time is not available for all jobs, just the next N jobs that are expected to run.  If you want to know everything there is to know about the job, add the <code>-f</code> flag.</p> <p><pre><code>                                                            Req'd  Req'd   Elap\nJob ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time\n--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----\n353201.polaris* user1    large    3d-1.sub    34449  60 38*    --  24:00 R 08:25    Job run at Tue Nov 15 at 16:44 on (x3006c0s13b1n0:ngpus=4:ncpus=64)+(x...\n353289.polaris* user2    medium   run_mae_l*    --   32 20*    --  12:00 Q   --     Not Running: Job would conflict with reservation or top job\n353411.polaris* user3    large    1310W60       --   64  64    --  06:00 Q   --     Not Running: Not enough free nodes available\n336990.polaris* user4    large    inf_clDB      --  464 29*    --  01:00 H   --     Job held by user4 on Mon Oct  3 20:16:26 2022\n</code></pre> The <code>comment</code> field is your friend.  Wondering why your job isn't running?  Check the comment.  Wondering about the fate of a finished job? Add the <code>-x</code> option to see finished jobs (our history retention is currently set at two weeks) and check the comment. This cannot be stressed enough.  Often, when a user ticket comes in about PBS, we answer it by looking at the comment.</p> <p>If you are familiar with <code>jq</code> or some other command line JSON tool, the <code>-F JSON</code> option can be quite handy. <code>grep</code> is great, but when you grep the <code>-f</code> output for something, you probably want to know which node the found lines belong to.  With the JSON output that is trivial.</p> <pre><code>allcock@polaris-login-02:~/.ssh&gt;  qstat -fF JSON | jq '.Jobs | map_values(select(.job_state == \"R\") | {Job_Name, Account_Name, qtime, stime})'\n{\n  \"349710.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\": {\n    \"Job_Name\": \"P38\",\n    \"Account_Name\": \"CompBioAffin\",\n    \"qtime\": \"Fri Nov  4 11:04:12 2022\",\n    \"stime\": \"Fri Nov 11 07:52:12 2022\"\n  },\n  \"352220.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov\": {\n    \"Job_Name\": \"mdsim_10000_run1.pbs\",\n    \"Account_Name\": \"RL-fold\",\n    \"qtime\": \"Thu Nov 10 22:41:55 2022\",\n    \"stime\": \"Fri Nov 11 09:00:12 2022\"\n  },\n</code></pre>"},{"location":"running-jobs/job-and-queue-scheduling/#queues","title":"Queues","text":"<p><code>qstat -Q</code> Will show you the names of all the queues and tell you their status.  If they are enabled (Ena column), you can queue jobs into them.  If they are started (Str column) then the scheduler will try and run jobs from it.  There is a <code>-f</code> (full) option but that is mostly for admins, though you can find the min and max node count <code>(resources_[min|max].nodect)</code> and min and max walltime <code>(resources_[min|max]walltime)</code> from the output.  Those values are also available in this documentation.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#qalter-alter-a-queued-job","title":"<code>qalter</code>: Alter a queued job","text":"<p>Users Guide Sec. 9.2, page UG-168; Reference Guide Sec. 2.40, page RG-130</p> <p>Basically takes the same options as <code>qsub</code>;  Say you typoed and set the walltime to 300 minutes instead of 30 minutes.  You could fix it (if the job had not started running) by doing <code>qalter -l walltime=30:00 &lt;jobid&gt; [&lt;jobid&gt; &lt;jobid&gt;...]</code>  The new value overwrites any previous value.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#qdel-delete-a-queued-or-running-job","title":"<code>qdel</code>: Delete a queued or running job:","text":"<p>Users Guide Sec. 9.3, page UG-170; Reference Guide Sec. 2.41, page RG-143</p> <p><code>qdel &lt;jobid&gt; [&lt;jobid&gt; &lt;jobid&gt;...]</code></p>"},{"location":"running-jobs/job-and-queue-scheduling/#qmove-move-a-job-to-a-different-queue","title":"<code>qmove</code>: Move a job to a different queue","text":"<p>Users Guide Sec. 9.7, page UG-173; Reference Guide Sec. 2.46, page RG-175</p> <ul> <li><code>qmove &lt;new queue&gt; &lt;jobid&gt; [&lt;jobid&gt; &lt;jobid&gt;...]</code></li> <li>Only works before a job starts running</li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#qholdqrls-place-release-a-user-hold-on-a-job","title":"<code>qhold,qrls</code>: Place / release a user hold on a job","text":"<p>Reference Guide Sec 2.44, page RG-150 and Sec 2.50, page RG-183</p> <ul> <li><code>[qhold | qrls] &lt;jobid&gt; [&lt;jobid&gt; &lt;jobid&gt;...]</code></li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#qselect-query-jobids-for-use-in-commands","title":"<code>qselect</code>: Query jobids for use in commands","text":"<p>Users Guide Sec. 10.1, page UG-175; Reference Guide Sec. 2.52, page RG-189</p> <ul> <li><code>qdel `qselect -N test1`</code> will delete all the jobs that had the job name set to <code>test1</code>.</li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#qmsg-write-a-message-into-a-jobs-output-file","title":"<code>qmsg</code> Write a message into a jobs output file","text":"<p>Users Guide Sec. 9.4, page UG-171; Reference Guide Sec. 2.47, page RG-177</p> <ul> <li><code>qmsg -E -O \"This is the message\" &lt;jobid&gt; [&lt;jobid&gt; &lt;jobid&gt;...]</code></li> <li><code>-E</code> writes it to standard error, <code>-O</code> writes it to standard out</li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#qsig-send-a-signal-to-a-job","title":"<code>qsig</code> Send a signal to a job","text":"<p>Users Guide Sec. 9.5, page UG-172; Reference Guide Sec. 2.53, page RG-195</p> <ul> <li><code>qsig -s &lt;signal&gt; &lt;jobid&gt; [&lt;jobid&gt; &lt;jobid&gt;...]</code></li> <li>If you don't specify a signal, <code>SIGTERM</code> is sent.</li> </ul>"},{"location":"running-jobs/job-and-queue-scheduling/#pbsnodes-get-information-about-the-current-state-of-nodes","title":"<code>pbsnodes</code> Get information about the current state of nodes","text":"<p>Reference Guide Sec 2.7 page RG-36</p> <p>This is more for admins, but it can tell you what nodes are free (state), how many \"CPUs\" which is actually the number of threads (ncpus), how many GPUs (ngpus) which with some GPUs like NVIDIA A100s can change depending on the MIG mode, and if the node is shared or not (sharing).</p> <p><code>pbsnodes &lt;node name&gt;</code>: Everything there is to know about a node</p> <pre><code>&gt; pbsnodes x3002c0s7b1n0\nx3002c0s7b1n0\n     Mom = x3002c0s7b1n0.hsn.cm.polaris.alcf.anl.gov\n     Port = 15002\n     pbs_version = 2022.1.1.20220926110806\n     ntype = PBS\n     state = free\n     pcpus = 64\n     resources_available.arch = linux\n     resources_available.demand = False\n     resources_available.gputype = A100\n     resources_available.host = x3002c0s7b1n0\n     resources_available.mem = 527672492kb\n     resources_available.ncpus = 64\n     resources_available.ngpus = 4\n     resources_available.system = polaris\n     resources_available.tier0 = x3002-g0\n     resources_available.tier1 = g0\n     resources_available.vnode = x3002c0s7b1n0\n     resources_assigned.accelerator_memory = 0kb\n     resources_assigned.hbmem = 0kb\n     resources_assigned.mem = 0kb\n     resources_assigned.naccelerators = 0\n     resources_assigned.ncpus = 0\n     resources_assigned.ngpus = 0\n     resources_assigned.vmem = 0kb\n     resv_enable = True\n     sharing = force_exclhost\n     license = l\n     last_state_change_time = Tue Nov 15 19:26:39 2022\n     last_used_time = Tue Nov 15 19:26:39 2022\n     server_instance_id = polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov:15001\n```bash\n`pbsnodes -avSj`: A nice table to see what is free and in use\n\n```bash\n&gt; pbsnodes -avSj\n                                                        mem       ncpus   nmics   ngpus\nvnode           state           njobs   run   susp      f/t        f/t     f/t     f/t   jobs\n--------------- --------------- ------ ----- ------ ------------ ------- ------- ------- -------\nx3014c0s19b0n0  job-exclusive        1     1      0  503gb/503gb   63/64     0/0     4/4 353394\nx3014c0s19b1n0  resv-exclusive       0     0      0  503gb/503gb    0/64     0/0     4/4 --\nx3014c0s1b0n0   offline              0     0      0  503gb/503gb   64/64     0/0     4/4 --\n</code></pre> <p><code>pbsnodes -avSj | grep free | wc -l</code>: A quick way to see how many nodes are free</p> <pre><code>[20220217-21:09:30]&gt; pbsnodes -avSj | grep free | wc -l\n38\n</code></pre> <p><code>pbsnodes -avSj | grep free | awk '{print $1}'</code>: Lists the free nodes</p> <pre><code>[20220217-21:09:30]&gt; pbsnodes -avSj | grep free | awk '{print $1}'\nx3201c0s25b0n0\nx3209c0s13b0n0\nx3209c0s19b0n0\nx3209c0s1b1n0\n</code></pre> <p><code>pbsnodes -l</code>: (lowercase  l) see which nodes are down. The comment often indicates why it is down</p> <pre><code>[20220217-21:10:31]&gt; pbsnodes -l\nx3014c0s19b0n0       offline,resv-exclusive Xid 74 -- GPUs need reseat\nx3014c0s25b0n0       offline,resv-exclusive Checking on ConnectX-5 firmware\n</code></pre>"},{"location":"running-jobs/job-and-queue-scheduling/#job-priority","title":"Job Priority","text":"<p>In PBS it is not easy to see a priority order for which jobs will run next.  The best way is to use the <code>-T</code> option on qsub and look at the estimated start times.  ALCF runs a custom scheduler algorithm, but in general, the job priority in the queue is based on several criteria:</p> <ol> <li>positive balance of your project</li> <li>size (in nodes) of the job, larger jobs receive higher priority</li> <li>the type of project (e.g. INCITE, ALCC, or discretionary)</li> <li>job duration: shorter duration jobs will accumulate priority more quickly, so it is best to specify the job run time as accurately as possible</li> </ol>"},{"location":"running-jobs/job-and-queue-scheduling/#troubleshooting-common-errors","title":"Troubleshooting / Common Errors","text":"<p>If you receive a <code>qsub: Job rejected by all possible destinations</code> error, then check your submission parameters. The issue is most likely that your walltime or node count do not fall within the ranges listed above for the production execution queues. Please see the table above for limits on production queue job sizes.</p> <p>NOTE: If you receive a job ID but you cannot find your job with <code>qsub</code>, then this may be a submission parameter issue. This can happen for batch submission because the job is being accepted into the routing (<code>prod</code>) queue.  The routing/<code>prod</code> queue's parameters are more broad since it needs to accommodate for all three production queues (<code>small</code>, <code>medium</code>, &amp; <code>large</code>). The prod routing queue accepts the job, generating a job ID.  Depending on what is going on with the system, the routing may or may not occur before the qsub returns (i.e., if the queues are backed-up the routing queue can't route the job before the qsub returns). If the routing is delayed then a job ID is returned, and routing is completed later.  Since the qsub has ended then there isn't a way to inform the user that this has been rejected by all routing destinations. If you run a qstat on the jobid, it will return <code>qstat: Unknown Job Id &lt;jobid&gt;</code>.</p>"},{"location":"running-jobs/job-and-queue-scheduling/#using-fakeroot-with-singularity","title":"Using Fakeroot with Singularity","text":"<p>The fakeroot feature (commonly referred as rootless mode) allows an unprivileged user to run a container as a \u201cfake root\u201d user by leveraging user namespace UID/GID mapping.  To request this feature be enabled on your job add the following to your <code>qsub</code> command line:</p> <p><code>-l singularity_fakeroot=true</code></p>"},{"location":"running-jobs/machine-reservations/","title":"Machine Reservations","text":"<p>To get a reservation, you must first demonstrate a need to run outside of the normal queueing policies. Reservations are available only to projects with a positive allocation. </p> <p>5 business day lead time is recommended to ensure timely approval. Scheduling is contingent on machine availability.</p> <p>Disclaimer: Approval for reservation requests are subject to their appropriateness and machine availability. Not all requests will be approved. It is particularly difficult to accommodate reservation requests during busy times of the year, e.g. Supercomputing, end of the ALCC and INCITE allocation cycles.</p> <p>Kindness Policy: We do monitor reservation utilization. The Scheduling Committee reserves the right to cancel reservations without notice if we decide a reservation is underutilized, not being properly utilized, or otherwise wasting resources. For instance, requesting a 12 hour reservation for interactive work, but then going to lunch leaving the reservation empty with no work.</p> <p>Early Completion Policy: If you have finished running your jobs before your reservation has ended, please reach out to the support team (support@alcf.anl.gov) to have to release it for other users. At this time, there is no way for a user to release a reservation early.</p>"},{"location":"running-jobs/machine-reservations/#some-definitions","title":"Some definitions","text":"<p>A reservation means we set aside some number of nodes on a system for a limited time. Only certain users or projects can submit to the queues assigned to the reservation.</p> <p>A score boost means we give your job(s) a boost in score to help it move ahead in the queue, but it still allows the scheduler to more efficiently fill the machine. </p>"},{"location":"running-jobs/machine-reservations/#-fill-out-the-form-","title":"-&gt; Fill out the Form &lt;-","text":""},{"location":"running-jobs/machine-reservations/#querying-reservations-via-command-line","title":"Querying Reservations via Command Line","text":"<p>You can see reservations using the <code>pbs_rstat</code> command:</p>"},{"location":"running-jobs/machine-reservations/#submitting-to-a-reservation","title":"Submitting to a reservation","text":"<p>Use <code>pbs_rstat</code> command on the login node to view the list of all reservations. <pre><code>pbs_rstat\nResv ID      Queue     User     State               Start / Duration / End             \n---------------------------------------------------------------------------\nA123456.po   A123456   smith@   CO       Mon Aug 18 09:00 / 43200 / Tue Aug 19 11:00\n</code></pre></p> <p>For recurring reservations, the <code>reserve_start</code> and <code>reserve_end</code> are always the first instance.  <code>reserve_index</code> and <code>reserve_count</code> tell you where you are in the recurrence.</p>"},{"location":"running-jobs/machine-reservations/#using-a-reservation","title":"Using a Reservation","text":"<p>Once the reservation is set up, jobs can be submitted to the reservation queue prior to the reservation start time. In the example above, the queue name is shown in the <code>Queue</code> column.</p> <pre><code>qsub -q A123456 walltime=60:00 -l select=1024:system=polaris -l filesystems=eagle myprog.exe\n</code></pre> <p>For jobs using 33 percent or more of a system, place your job in the queue at least 12 hours prior to the start of the reservation or your reservation may be canceled. The machine will start to drain for your reservation, and it is important that your job is ready to run.</p> <p>You can also move jobs from the regular queue to the reservation queue at any time using the \u201cqmove\u201d command.  Keep in mind that a job won't start unless enough time is left in the reservation. </p> <p>NOTE: There is NOT any padding at the end of the reservation.  When the reservation ends all jobs are terminated, deleted, and the reservation queue is deleted.  If a routing queue is used for the reservation, then jobs may be preserved, but any running job(s) are still terminated.</p>"},{"location":"running-jobs/pbs-qsub-options-table/","title":"PBS Pro <code>qsub</code> Options","text":"<p>Version 1.2 2021-04-28 </p> <p><code>-l select</code> and similar options use a lower case \"L\", <code>-I</code> for interactive is an upper case \"I\"</p> Cobalt CLI PBS CLI PBS Directive Function and Page Reference <code>-A &lt;account_string&gt;</code> <code>-A &lt;account_string&gt;</code> <code>#PBS Account_Name=&lt;accounting string&gt;</code> \"Specifying Accounting String\u201d UG-29 <code>-n NODES</code><code>--nodecount NODES</code> <code>-l select=NODES:system=&lt;hostname&gt;</code> One or more <code>#PBS -l &lt;resource name&gt;=&lt;value&gt;</code> directives \"Requesting Resources\u201d UG-51 <code>-t</code> <code>--walltime</code> <code>-l walltime=H:MM:SS</code> One or more <code>#PBS -l &lt;resource name&gt;=&lt;value&gt;</code> directives \"Requesting Resources\u201d UG-51 <code>--attrs</code> <code>filesystems=&lt;resouce&gt;</code> <code>-l filesystems=&lt;resource&gt;</code> One or more <code>#PBS -l &lt;resource name&gt;=&lt;value&gt;</code> directives \"Requesting Resources\u201d UG-51 <code>-q</code> <code>-q &lt;destination&gt;</code> <code>#PBS -q &lt;queue name&gt;</code> <code>#PBS -q @&lt;server name&gt;</code> <code>#PBS -q &lt;queue name&gt;@&lt;server name&gt;</code> \"Specifying Server and/or Queue\u201d UG-29 <code>--env</code> <code>-v &lt;variable list&gt;</code> \"Exporting Specific Environment Variables\u201d UG-126 <code>--env</code> <code>-V</code> <code>#PBS -V</code> \"Exporting All Environment Variables\u201d UG-126 <code>--attrs</code> Done via custom resources and select statements \"Setting Job Attributes\u201d UG-16 <code>--dependencies=&lt;list&gt;</code> <code>-W depend=afterok:&lt;list&gt;</code> <code>#PBS depend=...</code> \"Using Job Dependencies\u201d UG-107 <code>-I</code><code>--interactive</code> <code>-I</code> Deprecated for use in a script \"Running Your Job Interactively\u201d UG-121 <code>--jobname</code> <code>-N &lt;name&gt;</code> <code>#PBS -N &lt;job name&gt;</code> <code>#PBS -WJob_Name=&lt;job name&gt;</code> \"Specifying Job Name\u201d UG-27 <code>-e</code><code>--error=</code> <code>-e &lt;path&gt;</code> <code>#PBS -e &lt;path&gt;</code><code>#PBS Error_Path=&lt;path&gt;</code> \"Paths for Output and Error Files\u201d UG-42 <code>-o</code>--output= <code>-o &lt;path&gt;</code> <code>#PBS -o &lt;path&gt;</code><code>#PBS Output_Path=&lt;path&gt;</code> \"Paths for Output and Error Files\u201d UG-42 <code>-M</code>--notify see note #1 <code>-M &lt;user list&gt;</code> <code>-m &lt;mail options&gt;</code> (<code>-m be</code> is suggested) <code>#PBS -M &lt;mail recipients&gt;</code> <code>#PBS -WMail_Users=&lt;mail recipients&gt;</code> <code>#PBS -m &lt;mail points&gt;</code> <code>#PBS -WMail_Points=&lt;mail points&gt;</code> \"Setting Email Recipient List\u201d UG-26 <code>-u</code><code>--umask</code> <code>-W umask=&lt;value&gt;</code> <code>#PBS umask=&lt;value&gt;</code> \"Changing Linux Job umask\u201d UG-45 <code>-h</code> <code>-h</code> <code>#PBS -h</code> \"Holding and Releasing Jobs\u201d UG-115 <code>--proccount</code> See Note #2 <code>-l mpiprocs</code>Not needed to get equivalent Cobalt functionality One or more <code>#PBS -l &lt;resource name&gt;=&lt;value&gt;</code> directives \"Requesting Resources\u201d UG-51"},{"location":"running-jobs/pbs-qsub-options-table/#pbs-options-that-provide-functionality-above-and-beyond-cobalt","title":"PBS options that provide functionality above and beyond Cobalt","text":"<p>Depending on policy decisions not all of these options may be available.</p> Cobalt CLI PBS CLI PBS Directive Function and Page Reference N/A <code>-a &lt;date_time&gt;</code> <code>#PBS -a</code> \"Deferring Execution\u201d UG-119 N/A <code>-C &lt;directive prefix&gt;</code> \"Changing the Directive Prefix\u201d UG-16 N/A <code>-c &lt;interval&gt;</code> <code>#PBS -c</code> \"Using Checkpointing\u201d UG-113 N/A <code>-G</code> \"Submitting Interactive GUI Jobs on Windows\u201d UG-125 N/A <code>-J X-Y[:Z]</code> <code>#PBS -J</code> \"Submitting a Job Array\u201d UG-150 N/A <code>-j &lt;join&gt;</code> <code>#PBS Join_Path=&lt;joining option&gt;</code> \"Merging Output and Error Files\u201d UG-43 N/A <code>-k &lt;keep&gt;</code> <code>#PBS Keep_Files=&lt;keep option&gt;</code> \"Keeping Output and Error Files on Execution Host\u201d UG-44 N/A <code>-p &lt;priority&gt;</code> <code>#PBS -p</code> \"Setting Priority for Your Job\u201d UG-120 N/A <code>-P &lt;project&gt;</code> <code>#PBS project=&lt;project name&gt;</code> \"Specifying a Project for a Job\u201d UG-27 N/A <code>-r &lt;value&gt;</code> <code>#PBS -r</code> \"Allowing Your Job to be Re-run\u201d UG-118 N/A <code>-R &lt;remove options&gt;</code> \"Avoiding Creation of stdout and/or stderr\u201d UG-43 N/A <code>-S &lt;path list&gt;</code> \"Specifying the Top Shell for Your Job\u201d UG-19 N/A See Note #3 <code>-u &lt;user list&gt;</code> <code>#PBS User_List=&lt;username list&gt;</code> \"Specifying Job Username\u201d UG-28 N/A <code>-W block=true</code> <code>#PBS block=true</code> \"Making qsub Wait Until Job Ends\u201d UG-120 N/A <code>-W group_list=&lt;list&gt;</code> <code>#PBS group_list=&lt;group list&gt;</code> \"Specifying Job Group ID\u201d UG-28 N/A <code>-W release_nodes_on_stageout=&lt;value&gt;</code> \"Releasing Unneeded Vnodes from Your Job\u201d UG-127 N/A <code>-W run_count=&lt;value&gt;</code> \"Controlling Number of Times Job is Re-run\u201d UG-119 N/A <code>-W sandbox=&lt;value&gt;</code> \"Staging and Execution Directory: User Home vs. Job-specific\u201d UG-31 N/A <code>-W stagein=&lt;list&gt;</code> <code>#PBS -W stagein=&lt;execution path&gt;@&lt;input file storage host&gt;:&lt;input file storage path&gt;[,...]</code> \"Input/Output File Staging\u201d UG-31 N/A <code>-W stageout=&lt;list&gt;</code> <code>#PBS -W stageout=&lt;execution path&gt;@&lt;output file storage host&gt;:&lt;output file storage path&gt;[,...]</code> \"Input/Output File Staging\u201d UG-31 N/A <code>-X</code> \"Receiving X Output from Interactive Linux Jobs\u201d UG-124 N/A <code>-z</code> <code>#PBS -z</code> \"Suppressing Printing Job Identifier to stdout\u201d UG-30"},{"location":"running-jobs/pbs-qsub-options-table/#notes","title":"Notes","text":"<ol> <li>To get the equivalent mail notifications from PBS it requires two parameters: the <code>-M</code> just like Cobalt, but also <code>-m be</code> (the <code>be</code> stands for \"beginning\" and \"end\") to specify when the mails should go out. This will give you the same behavior as Cobalt.</li> <li><code>--proccount</code>, while available, only changed behavior on the Blue Gene machines.  To get equivalent functionality just drop it from the CLI.  In PBS it does influence the <code>PBS_NODES</code> file.  See Section 5.1.3 in the PBS Users Guide page UG-78</li> <li>The following Cobalt options have no equivalent in PBS:<ul> <li><code>--cwd</code>: use a script and <code>cd</code> to the directory you want to run from.</li> <li><code>--user_list</code>: There is no way to do this.  We will work on adding this functionality.</li> <li><code>--debuglog</code>: Are we going to try and generate the equivalent of a <code>.cobalt</code> file?</li> </ul> </li> <li>The following Cobalt options were Blue Gene specific and no longer apply:<ul> <li><code>--kernel</code></li> <li><code>-K KERNELOPTIONS</code></li> <li><code>--ion_kernel</code></li> <li><code>--ion_kerneloption</code></li> <li><code>--mode</code>: see notes on running scripts, Python, and other executables</li> <li><code>--geometry</code></li> <li><code>--disable_preboot</code></li> </ul> </li> </ol>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/","title":"PBS Admin Quick Start Guide","text":"<p>The single most important thing I can tell you is where to get the PBS BigBook.  It is very good and a search will usually get you what you need if it isn't in here.</p> <ul> <li>PBS Admin Quick Start Guide</li> <li>Checking Server Status</li> <li>Checking / Setting Node Status</li> <li>Troubleshooting</li> <li>Starting, stopping, restarting, status of the daemons:</li> <li>Starting, stopping scheduling across the entire complex</li> <li>Starting, stopping queues:</li> <li>\"Boosting\" jobs (running them sooner)</li> <li>Reservations</li> <li>MIG Mode</li> <li>Rack and Dragonfly group mappings</li> <li>Restricting a Reservation to Vnodes With Specific Resources</li> <li>Removing Blocking Resources</li> </ul>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#checking-server-status","title":"Checking Server Status","text":"<p>You can check overall server status and settings with: <code>qmgr -c \"list server\"</code> or <code>qstat -Bf</code> (add <code>-w</code> to <code>qstat</code> if you want to remove wrapping) This will show current server parameters.  If you have manager/operator permissions you will also see any hidden resources. You may also check parameters of the scheduler with <code>qmgr -c \"list sched\"</code>, and by checking <code>$PBS_HOME/sched_priv/sched_config</code>. Hook information can be checked with <code>qmgr -c \"list hook\"</code> and <code>qmgr -c \"list pbshook\"</code>.  Due to permissions all hook operations require root.</p>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#checking-setting-node-status","title":"Checking / Setting Node Status","text":"<p>The <code>pbsnodes</code> command is your friend.</p> <ul> <li>check status</li> <li><code>pbsnodes -av</code> gives you everything; grep will be useful here</li> <li><code>pbsnodes -v &lt;node&gt; &lt;node&gt; ...</code> will give you all information on the listed nodes</li> <li><code>pbsnodes -avSj</code> gives you a nice table summary</li> <li><code>pbsnodes -l</code> lists the nodes that are offline</li> <li>Taking nodes on and offline</li> <li><code>pbsnodes -C &lt;comment&gt; -o &lt;nodelist&gt;</code> will mark a node offline in PBS (unschedulable)<ul> <li>Adding the time and date and why you took it offline in the comment is helpful </li> <li><code>&lt;nodelist&gt;</code> is space separated </li> </ul> </li> <li><code>pbsnodes -r &lt;node list&gt;</code> will attempt to bring a node back online     This will only remove the \"offline\" state from a node, if the node would be down for other reasons, that will not change.         * Use -C \"\" to remove any comment that was set when the node was originally marked offline.  </li> </ul>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>PBS_EXEC (where all the executables are): <code>/opt/pbs/[bin|sbin]</code></li> <li>PBS_HOME (where all the data is): <code>/var/spool/pbs</code></li> <li>logs: <code>/var/spool/pbs/[server|mom|sched|comm]_logs</code></li> <li>config: <code>/var/spool/pbs/[server|mom|sched]_priv/</code></li> <li><code>/etc/pbs.conf</code> - Reference Guide Section 9.1, page RG-371 </li> <li><code>qstat -[x]f [jobid]</code></li> <li>the -x shows jobs that have already completed.  We are currently holding two weeks history.</li> <li>the <code>comment</code> field is particularly useful.  It will tell you why it failed, got held, couldn't run, etc..</li> <li>The jobid is optional.  Without it you get all jobs.</li> <li><code>tracejob &lt;jobid&gt;</code> </li> <li>This will pull all of the logs related to the jobid on that node.  Run on the pbs.server host to get most of the job information</li> <li>If this is run on a compute node involved in jobid then it will aggregate all logs from the mom on that job from that node.</li> <li>You may pass it the <code>-n #</code> option where # is number of days to look back to tell the command to search more days back in the logs.  This defaults to 1 day.</li> <li>This does a rudimentary aggregation and filter of the logs for you.</li> <li><code>qselect</code> -  Reference Guide Section 2.54 page RG-187.</li> <li>allows you to query and return jobids that meet criteria for instance the command below would delete all the jobs from Yankee Doodle Dandy, username yddandy:</li> <li><code>qdel `qselect -u yddandy`</code></li> <li>Error Code Table (Reference Guide Chapter 14, RG-391)</li> <li>If a CLI command (qmgr, qsub, whatever) spits out an error code at you, go look it up in the table, you may well save yourself a good bit of time.</li> <li>We are going to try and either get the error text to come with the code or write a utility to look it up and have that on all the systems.</li> </ul>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#starting-stopping-restarting-status-of-the-daemons","title":"Starting, stopping, restarting, status of the daemons:","text":"<ul> <li>Server: on pbs0 run <code>systemctl [start | stop |restart | status] pbs</code></li> <li>MoM:</li> <li>If you only want to restart a single MoM, ssh to the host and issue the same commands as above for ther server.</li> <li>If you want to restart the MoM on every compute node, <code>ssh admin.polaris</code> then do: <code>pdsh -g custom-compute \"systemctl [start | stop |restart | status] pbs\"</code> </li> </ul>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#starting-stopping-scheduling-across-the-entire-complex","title":"Starting, stopping scheduling across the entire complex","text":"<p><code>qmgr -c \"set server scheduling = [True | False]\"</code></p> <p>IMPORTANT NOTE: If we are running a single PBS complex for all our systems (same server is handling Polaris, Aurora, Cooley2, etc) this will stop scheduling on everything.</p> <p>To check the current status you may do: <code>qmgr -c \"list server scheduling\"</code></p>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#starting-stopping-queues","title":"Starting, stopping queues:","text":"<ul> <li>started: Can you queue a job or not</li> <li>enabled: Will the scheduler run jobs that are in the queue</li> </ul> <p>So if a queue is started, but not enabled, users can issue qsubs and the job will get queued, but nothing will run until we renable the queue.  Running jobs are unaffected.</p> <p><code>qmgr -c \"set queue &lt;queue name&gt; started = [True | False]\"</code> <code>qmgr -c \"set queue &lt;queue name&gt; enabled = [True | False]\"</code></p>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#boosting-jobs-running-them-sooner","title":"\"Boosting\" jobs (running them sooner)","text":"<p>There are two ways you can run a job sooner:</p> <ol> <li><code>qmove run_next &lt;jobid&gt;</code> <ol> <li>Because of the way policy is set for the acceptance testing period, any job in the <code>run_next</code> queue will run before jobs in the default <code>workq</code> with the exception of jobs that are backfilled.  So by moving the job into the <code>run_next</code> queue, you moved it to the front of the line.  There are no restrictions on this, so please do not abuse it.</li> </ol> </li> <li><code>qorder &lt;jobid&gt; &lt;jobid&gt;</code></li> <li>If you don't necessarily need it to run next, but just want to rearrange the order a bit, you can use <code>qorder</code> which swaps the positions of the specified jobids.  So, if one of them was 10th in line and one was 20th, they would switch positions. </li> <li><code>qalter -l score_boost=NNNNN &lt;jobid&gt; &lt;jobid&gt;</code>     If the <code>job_sort_function</code> is enabled and shows up when querying the server, you can add a numeric boost to the score of a job to push it further ahead in the queue. You have to be a manager or operator to alter this value.</li> </ol>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#reservations","title":"Reservations","text":"<p>Most of the reservation commands are similar to the job commands, but prefixed with <code>pbs_r</code> instead of <code>q</code>: <code>pbs_rsub, pbs_rstat, pbs_ralter, pbs_rdel</code>.  You get the picture.  In general, their behavior is reasonably similar to the equivalent jobs commands.  Note that by default, users can set their own reservations.  We have to use a hook, no_user_rsub, to prevent that.  The hook does allow anyone with manager or operator permissions to set reservations.</p> <ul> <li>There are three types of reservations:</li> <li>Advance and standing reservations - reservations for users;  Note that you typically don't specify the nodes.  You do a resource request like with qsub and PBS will find the nodes for you.</li> <li>job-specific now reservations - we have not used these.  Where they could come in handy is for debugging.  A user gets a job through, we convert it to a job-specific reservation, then if their job dies, they don't have to wait through the queue again, they can keep iterating until the wall time runs out.</li> <li>maintenance reservations. - You can explicitly set which hosts to include in the reservation.</li> <li>Also note that reservations occur in two steps.  The <code>pbs_rsub</code> will return with an ID but will say <code>unconfirmed</code>.  That means it was syntactically correct, but PBS hasn't figured out if the resources are available yet.  Once it has the resources, it will switch to confirmed.  This normally is done as fast as you can run <code>pbs_rstat</code>.  A reservation can only be confirmed if scheduling is enabled on the server.</li> <li>-R (start) -E (end) are in \"datetime\" format: [[[[CC]YY]MM]DD]hhmm[.SS] </li> <li>1315, 171315, 12171315, 2112171315 and 202112171315 would all be Dec 17th, 2021 @ 13:15<ul> <li>If that is in the future they are all equivalent and valid</li> <li>If it were Dec 17th, 2021 @ 1400, then 1315 would default to the next day @ 14:00, the rest would be errors because they are in the past. </li> <li>Be careful or this will bite you.  It will confirm the reservation and you will expect it to start in a few minutes, but it is actually for tomorrow.</li> </ul> </li> <li><code>pbs_rsub -N rsub_test -R 2023 -D 05:00 -l select=4</code></li> <li>probably not what you think: <code>resv_nodes = (edtb-03[0]:ncpus=1)+(edtb-03[0]:ncpus=1)+(edtb-03[0]:ncpus=1)+(edtb-03[0]:ncpus=1)</code>  It gave me 4 cores on the same node.</li> <li><code>pbs_rsub -N rsub_test -R 2023 -D 05:00 -l select=2 -l place=scatter</code></li> <li>Getting closer: <code>resv_nodes = (edtb-01[0]:ncpus=1)+(edtb-02[0]:ncpus=1)</code></li> <li>The <code>-l place=scatter</code> got me two different nodes, but edtb allows sharing, so I got one thread on each node, but there were actually jobs running on those nodes at the time. On Polaris, since the nodes are <code>force_exclhost</code> that wouldn't have been an issue.</li> <li><code>pbs_rsub -N rsub_test -R 2217 -D 05:00 -l select=2:ncpus=64 -l place=scatter:excl</code> This gave me what I wanted:<ul> <li><code>resv_nodes = (edtb-03[0]:ncpus=64)+(edtb-04[0]:ncpus=64)</code></li> <li>Leaving it to default to <code>ncpus=1</code> should work, but asking for them all isn't a bad idea.</li> </ul> </li> <li><code>pbs_rsub -N rsub_test -R 1200 -D 05:00 --hosts x3004c0s1b0n0 x3003c0s25b0n0...</code></li> <li>If you use <code>--hosts</code> it makes it a maintenance reservation.  You can't / don't need to add <code>-l select</code> or <code>-l place</code> on a maintenance reservation.  PBS will set it for you and will make it the entire host and exclusive access.  Nodes don't have to be up.  If jobs are running they will continue to run.  This will override any other reservation.</li> <li><code>pbs_ralter</code> You can use this to change attributes of the reservation (start time, end time, how many nodes, which users can access it, etc).  Works just like <code>qalter</code> for jobs. </li> <li><code>pbs_rdel &lt;reservation id&gt;</code>  This will kill all running jobs, delete the queue, meaning you lose any jobs that were in the queue, and release all the resources.</li> <li>NOTE: once the reservation queue is in place, you use all the normal jobs commands (qsub, qalter, qdel, etc.) to manipulate the jobs in the queue.  On the qsub you have to add <code>-q &lt;reservation queue name&gt;</code></li> </ul>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#giving-users-access-to-the-reservation","title":"Giving users access to the reservation","text":"<p>By default, only the person submitting the reservation will be able to submit jobs to the reservation queue.  You change this with the <code>-U +username@*,+username@*,...</code>.  You can add this to the initial <code>pbs_rsub</code> or use <code>pbs_ralter</code> after the fact.  The plus is basically ALLOW. We haven't tested it, but you can also theoretically use a minus for DENY.  You may also gate on group membership by setting <code>qmgr -c \"set queue &lt;reservation queue name&gt; acl_group_enable=True\"</code> and then adding groups to <code>acl_groups</code> on the reservation queue, using the same sort of syntax as you use for acl_users.  This is a bit of a hack, but if you want anyone to be able to run you can do <code>qmgr -c \"set queue &lt;reservation queue name&gt; acl_user_enable=False\"</code> </p> <p>WARNING: if you have both acl_users and acl_groups enabled, then the submitting user must be in the group and the user ACL list otherwise the job will be rejected! It is recommended that only one or the other be used on a queue.</p>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#mig-mode","title":"MIG mode","text":"<ul> <li>See the Nvidia Multi-Instance GPU User Guide for more details.</li> <li><code>sudo nvidia-smi mig -lgip</code> List GPU Instance Profiles;  This is how you find the magic numbers used to configure it below.</li> <li><code>sudo nvidia-smi mig -lgipp</code> list all the possible placements;  The syntax of the placement is <code>{&lt;index&gt;}:&lt;GPU Slice Count&gt;</code> </li> <li><code>nvidia-smi --query-gpu=mig.mode.current --format=csv,noheader</code> - check the status of all the GPUs on the node;  add <code>-i &lt;GPU number&gt;</code> to check a specific GPU</li> <li><code>systemctl stop nvidia-dcgm.service ; systemctl stop nvsm ; sleep 5 ; /usr/bin/nvidia-smi -mig 1</code> Put the node in MIG mode;  <code>-mig 0</code> will take it out of MIG mode.</li> <li><code>nvidia-smi mig -i 3 -cgi 19,19,19,19,19,19,19 -C</code> configure GPU #3 to have 7 instances.</li> <li><code>nvidia-smi mig --destroy-compute-instance; nvidia-smi mig --destroy-gpu-instance</code> Will free up the resources;  You have to do this before you can change the configuration.</li> </ul>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#polaris-rack-and-dragonfly-group-mappings","title":"Polaris Rack and Dragonfly group mappings","text":"<ul> <li>Racks contain (7) 6U chassis; Each chassis has 2 nodes for 14 nodes per rack</li> <li>The hostnames are of the form xRRPPc0sUUb[0|10]n0 where:<ul> <li>RR is the row {30, 31, 32}</li> <li>PP is the position in the row {30 goes 01-16, 31 and 32 go 01-12}</li> <li>c is chassis and is always 0 (I wish they would have counted up chasses, oh well)</li> <li>s stands for slot, but in this case is the RU in the rack. Values are {1,7,13,19,25,31,37}</li> <li>b is BMC controller and is 0 or 1 (each node has its own BMC)</li> <li>n is node, but is always 0 since there is only one node per BMC </li> </ul> </li> <li>So, 16+12+12 = 40 racks * 14 nodes per rack = 560 nodes.</li> <li>Note that in production group 9 (the last 4 racks) will be the designated on-demand racks</li> <li>The management racks are x3000 and X3100 and are dragonfly group 10</li> <li>The TDS rack is x3200 and is dragonfly group 11 </li> </ul> Group 0 Group 1 Group 2 Group 3 Group 4 Group 5 Group 6 Group 7 Group 8 Group 9 x3001-g0 x3005-g1 x3009-g2 x3013-g3 x3101-g4 x3105-g5 x3109-g6 x3201-g7 x3205-g8 x3209-g9 x3002-g0 x3006-g1 x3010-g2 x3014-g3 x3102-g4 x3106-g5 x3110-g6 x3202-g7 x3206-g8 x3210-g9 x3003-g0 x3007-g1 x3011-g2 x3015-g3 x3103-g4 x3107-g5 x3111-g6 x3203-g7 x3207-g8 x3211-g9 x3004-g0 x3008-g1 x3012-g2 x3016-g3 x3104-g4 x3108-g5 x3112-g6 x3204-g7 x3208-g8 x3212-g9"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#restricting-a-reservation-to-vnodes-with-specific-resources","title":"Restricting a Reservation to Vnodes With Specific Resources","text":"<p>You can restrict a reservation to particular resources in the select statement just like you can with job placement.  For instance, to restrict replacement to nodes that are not in the on-demand queue you can use <code>-l select=256:demand=False</code> in your select statement for a regular or repeating reservation.</p>"},{"location":"running-jobs/unused/pbs-admin-quick-start-guide/#removing-blocking-resources","title":"Removing Blocking Resources","text":"<p>There is a current behavior in PBS where reservations may inherit server defaults as restrictions and may not check other server values.  This may result in jobs running unexpectedly, or may cause a job to not be queued.  </p> <p>To fix jobs not being queued, some resources_max restrictions may have to be removed from the reservation queue, for example, you can clear filesystems and project_priority with the following: <code>gmgr -c \"unset queue &lt;reservation queue name&gt; resources_max.filesystems\"</code> <code>gmgr -c \"unset queue &lt;reservation queue name&gt; resources_max.project_priority\"</code></p> <p>If you need to add an additional restriction, you can likewise set a resource on the queue as a resources_max restrictions, for instance, to forbid eagle_fs from being used you can run: <code>qmgr -c \"set queue &lt;reservation queue name&gt; resources_max.eagle_fs=False\"</code> <code>qmgr -c \"set queue &lt;reservation queue name&gt; resources_mix.eagle_fs=False\"</code></p> <p>You can also set this as a part of the -l flag options at reservation creation.</p>"},{"location":"services/continuous-integration/","title":"Continuous Integration","text":""},{"location":"services/continuous-integration/#continuous-integration_1","title":"Continuous Integration","text":"<p>Continuous Integration (CI) in software development is the practice of committing code changes regularly to a version control system and having automated processes perform build, test, package, and deploy activities.</p> <p>The key concepts of CI include high frequency, repeatability, and automation in order to realize increased quality and ease of delivery. The main goal CI aims to achieve is the elimination of build and deployment issues, which in turn improves development cycles, provides a timely feedback loop with developers, and results in higher quality deliverables with reduced development time.</p> <p>CI usually describes the work that is done by a deployment or operations team to build and deploy code throughout an environment and make it available to the different interested teams involved in the SDLC. The steps that make up this process are referred to as a workflow or pipeline, which, when combined with automation, provides the mechanism for Continuous Integration.</p> <p>Today it is a common practice to use a CI tool for defining pipelines and executing the tasks required to take code from a source stored in a version control system to compiled and packaged artifacts executing in production. Two excellent examples of CI tools are Jenkins and GitLab.</p>"},{"location":"services/continuous-integration/#ci-tools-at-alcf","title":"CI Tools at ALCF","text":""},{"location":"services/continuous-integration/#gitlab-ci","title":"Gitlab-CI","text":"<p>Gitlab  is an application that offers combined functionality as git repository, issue tracker, and CI/CD platform.  The ALCF implementation of the Gitlab-CI environment leverages upstream gitlab runners combined with the ECP's Jacamar custom executor. As CI/CD is built directly into Gitlab, it can allow for tighter devops processes. Gitlab-CI is meant to provide CI/CD services for projects using Gitlab-CI to store their git repositories. ALCF does not allow users to join their own private runners to our existing gitlab ci environment and provides runners on our supported systems.</p>"},{"location":"services/getting-started/","title":"ALCF Services","text":"<p>Below is a list of some of the services ALCF offers.</p> <ul> <li>JupyterHub: An interactive computing environment for   Python and other languages.</li> <li>Continuous Integration: An automated processes to   help build, test, package, and deploy on ALCF systems.</li> </ul>"},{"location":"services/gitlab-ci/","title":"Continuous Integration via GitLab-CI","text":""},{"location":"services/gitlab-ci/#gitlab-ci","title":"GitLab-CI","text":"<p>GitLab is an application that offers combined functionality as git repository, issue tracker, and CI/CD platform. The ALCF implementation of the GitLab-CI environment leverages upstream GitLab runners combined with the ECP's Jacamar custom executor. As CI/CD is built directly into GitLab, it can allow for tighter devops processes.</p> <p>GitLab-CI is meant to provide CI/CD services for projects using GitLab-CI to store your git repositories and executing code on our HPC clusters. ALCF does not allow users to join your own private runners to our existing GitLab CI/CD environment and provides dedicated runners for our supported systems.</p> <p>Additional information, technical and user documentation, and community support can be found on GitLab's Runner website.</p> <p>Also see GitLab's CI/CD YAML syntax reference for the full list of keywords supported by GitLab CI.</p> <p>ALCF's GitLab-CI environment can be accessed by logging into the ALCF GitLab-CI web portal using your ALCF credentials (ALCF username and cryptocard token password).</p>"},{"location":"services/gitlab-ci/#quickstart","title":"Quickstart","text":"<ul> <li>A user Emails ALCF Support requesting access for your ALCF Project for gitlab-ci.alcf.anl.gov .</li> <li>ALCF Support will add the ALCF Project to the appropriate system(s) via the Account and Project management system.</li> <li>ALCF will create a <code>GitLab Group/SubGroup</code> for the ALCF Project and map it to the appropriate ldap group that maps to the ALCF Project</li> <li>ALCF Support will reply back to the user and inform them that the project is created.</li> <li>User(s) will need to login to gitlab-ci.alcf.anl.gov and configure their initial GitLab profile. Users will add an SSH key so they can pull/push code to the gitlab server.</li> <li>User will then need to create a <code>GitLab Project</code> in your assigned <code>GitLab Group/SubGroup</code>.</li> <li>CI/CD needs to be enabled for the GitLab Project.</li> <li>When ready to run CI/CD jobs, users will add add a <code>.gitlab-ci.yml</code> file to your git repositories.</li> <li>They will need to set any ALCF specific variable(s).</li> </ul> <p>Example: <code>.gitlab-ci.yml</code> file <pre><code># this include allows us to reference defaults in anl/ci-resource/defaults\ninclude:\n  - project: 'anl/ci-resources/defaults'\n    ref: main\n    file:\n      - '/runners.yml'\n\nstages:\n  - polaris_batch # stages may have any name\n\n# the below submits a batch job to the scheduler\nsubmit_batch: # CI jobs may have any name\n  stage: polaris_batch  # from the stages list above\n  extends: .polaris-batch-runner # this includes the defaults provided in the 'anl/ci-resources/defaults' project\n  variables:  # scheduler parameters must be included, adjust the below to match your values\n    ANL_POLARIS_SCHEDULER_PARAMETERS: \"-A ProjectName -l select=1,walltime=10:00,filesystems=home -q myQueue\"\n  script:\n    - id\n    - hostname\n    - echo \"Running on $(hostname) with setuid shell runner\"\n</code></pre> For a more complete example, see the .gitlab-ci.yml file in the large-example project.</p>"},{"location":"services/gitlab-ci/#glossary","title":"Glossary","text":"<ul> <li>Group - A collection of projects. Certain settings can be applied at the <code>Group</code> level and apply down to all child <code>SubGroups</code> and/or <code>Projects</code>. When a ALCF Project is allocated resources on the GitLab-CI environment we will create a GitLab <code>Group</code> that will map to your ALCF Project allocation.</li> <li>Jacamar-CI - A Custom Executor we use that runs jobs as a given user on the shell and is capable of submitting jobs to schedulers like Cobalt and PBS.</li> <li>Job - An individual set of commands that are ran. This is the lowest unit of GitLab-CI abstraction.</li> <li>Pipeline - GitLab organizes your jobs for each run into a <code>pipeline</code>.</li> <li>Project - GitLab Projects can be thought of as an individual  git repository plus all services and features GitLab layers on top. This term is unrelated to the ALCF Project concept. ALCF Projects often map to ldap groups and/or quotas and allocations.</li> <li>Stage - A collection of jobs in a pipeline. Jobs in the next stage will not start till the jobs in the current stage complete. If a job fails, the pipeline will not run the following stages by default.</li> <li>Triggering User - The user whose actions causes a CI/CD job to run and who the Jacamar-CI executor will run the jobs as. Examples include pushing commits up to the server, creating a merge request, and/or merging one branch into another branch.</li> </ul>"},{"location":"services/gitlab-ci/#projects-using-cicd","title":"Projects Using CI/CD","text":"<p>Any project with a git repository on the GitLab-CI environment has access to the CI/CD environment by default. In order to launch a shell job on a system you must already have access to that system.</p>"},{"location":"services/gitlab-ci/#on-boarding-with-cicd","title":"On-Boarding with CI/CD","text":"<p>To gain access to the GitLab-CI environment, send an email to support@alcf.anl.gov requesting access for your project(s). Include with the request:</p> <ul> <li>That you are requesting access to the GitLab-CI environment at https://gitlab-ci.alcf.anl.gov</li> <li>The ALCF Project shortname</li> <li>The PI\u2019s name </li> </ul> <p>GitLab-CI jobs run as the triggering user on relevant systems. The triggering user's home directory will be used by Jacamar-CI to copy the git repository and cache files into <code>~/.jacamar-ci</code>. This job will run out of their home directory and consume filesystem quota. If you need more space you should try to reference files in any ALCF Project allocations you have on shared filesystems. Unfortunately the initial git clone must run out of <code>~/.jacamar-ci</code> in your home directory.</p> <p>The triggering user is defined as the user account who caused the CI/CD pipeline to execute. Via scheduling a re-occurring job, pushing commits up to the server, creating a merge request, and/or merging a branch. When the CI/CD jobs run they will run as that user on the relevant systems. For a job to succeed the <code>triggering user</code> must have appropriate permissions and access to all relevant systems and files.</p>"},{"location":"services/gitlab-ci/#initial-login-and-profile-setup-of-gitlab-ci","title":"Initial Login and Profile setup of GitLab-CI","text":"<ul> <li>Login to gitlab-ci.alcf.anl.gov using your username and Cryptocard token.</li> <li>Once logged in, add your public key you already have or created earlier so that it can be associated with your account.</li> <li>Click Profile icon on the upper right hand corner, then click \"Edit Profile\"      GitLab Profile Dropdown screenshot </li> <li>Click \"SSH Keys\" on the left hand menu.      GitLab Profile Add SSH Key screenshot </li> <li>Copy/Paste in your  SSH public key into the large text box under the word Key<ul> <li>On Linux, Unix, and OSX based systems using OpenSSH your SSH public key is commonly found at <code>~/.ssh/id_rsa.pub</code>. If using windows you will need to consult your applications documentation on the location of your public key.</li> <li>Give it a descriptive title such as the where the key resides, by default it will extract the name from the end of the public key if possible.</li> </ul> </li> <li>Click the <code>Add Key</code> button. The button is disabled until you paste a key.</li> </ul>"},{"location":"services/gitlab-ci/#gitlab-projects-repositories","title":"GitLab Projects (repositories)","text":"<p>GitLab takes a git repository, adds additional functionality, and calls it a <code>GitLab Project</code>. This is the most common level you will be interacting with GitLab at. Please do not confuse ALCF Projects with <code>GitLab Projects</code> as they are two separate things. ALCF Projects more closely map to the <code>GitLab Group/SubGroup</code> concept; which we explain in the next section.    Once you are assigned access to a <code>GitLab Group/SubGroup</code> you will be able to create arbitrary <code>GitLab Projects</code> underneath. Configuring CI/CD jobs for each independently.</p> <p>To create a new <code>GitLab Project</code>:</p> <ul> <li>In the left pane, click \"Groups\", and then click \"Explore groups\" link on the right.</li> </ul> <p> </p> GitLab Your Groups Page screenshot <ul> <li>From the list in the \"Explore groups\" page, click the group you were informed corresponds to your <code>ALCF Project</code></li> </ul> <p> </p> GitLab Explore Groups Page screenshot <ul> <li>Click the <code>New project</code> button near the upper right. If this is the first project you are creating you will have two large square buttons near the middle of the screen to create <code>GitLab SubGroups</code> or <code>GitLab Projects</code></li> </ul> <p> </p> GitLab Empty Group Page screenshot <ul> <li>On the <code>Create new project</code> page, click <code>Create blank project</code></li> </ul> <p> </p> GitLab Create New Project screenshot <ul> <li>Fill in the <code>Project Name</code> field. The <code>Project slug</code> field will auto populate based on the <code>Project Name</code>, do not change it. If you are pushing an existing repository, you MUST uncheck the default <code>Initialize repository with a README</code> option. Failure to uncheck this option will result in a merge conflict that you will need to resolve manually between your existing \"local\" git repository and the one you just created on the server.</li> </ul> <p> </p> GitLab Create New Project screenshot <ul> <li>Click <code>Create project</code> button near the bottom</li> <li>After creating the project, navigate to \"Settings\" &gt; \"General\", Expand the \"Visibility, project features, permissions\" section and enable the \"CI/CD\" toggle.</li> </ul>"},{"location":"services/gitlab-ci/#gitlab-groupssubgroups-folders","title":"GitLab Groups/SubGroups (Folders)","text":"<p>GitLab organizes <code>GitLab Projects</code> into \"folders\" called <code>Groups</code> or <code>SubGroups</code>. When an ALCF Project is granted access to GitLab-CI a GitLab <code>Group</code> will be created with access for all members of that ALCF Project. Users will then be able to create arbitrary GitLab <code>Projects</code>. </p> <p>Each ALCF Project will have a top-level <code>Group</code> or <code>SubGroup</code> created with the ALCF Project\u2019s name. It is used for organization in the multi-project environment and is required for implementing the needed level of security. The <code>Group</code> folder is where all of the your <code>GitLab Projects</code> are to be stored, you can additionally create new <code>SubGroups</code>, <code>Projects</code>, group variables, etc within your designated <code>Group</code>, <code>SubGroups</code>, and/or <code>Projects</code>.</p> <p>To create a new <code>GitLab SubGroup</code>:</p> <ul> <li>In the left pane, click \"Groups\", and then click \"Explore groups\" link on the right.</li> </ul> <p> </p> GitLab Your Groups Page screenshot <ul> <li>From the list in the \"Explore groups\" page, click the group you were informed corresponds to your <code>ALCF Project</code></li> </ul> <p> </p> GitLab Explore Groups Page screenshot <ul> <li>Click the <code>New subgroup</code> button near the upper right. If this is the first project you are creating you will have two large square buttons near the middle of the screen to create <code>GitLab SubGroups</code> or <code>GitLab Projects</code></li> </ul> <p> </p> GitLab Empty Group Page screenshot <ul> <li>On the <code>Create subgroup</code> page, enter the <code>Subgroup name</code>. <code>Subgroup slug</code> will auto populate, do not change it.</li> </ul> <p> </p> GitLab Create New SubGroup screenshot <ul> <li>Click <code>Create subgroup</code> button near the bottom</li> </ul>"},{"location":"services/gitlab-ci/#gitlab-runner-nodes","title":"GitLab Runner Nodes","text":"<p>Each system is assigned one or more GitLab runner node(s) that are shared by all users in GitLab-CI. Each runner is only capable of running one users pipeline at a time. While multiple jobs in that pipeline may run in parallel.</p> <p>Each node will have two runners available, <code>shell</code> and <code>batch</code>. <code>shell</code> will run shell jobs directly on the runner node as the user. <code>batch</code> will submit the job to the HPC cluster's scheduler that is paired to that node. You will need to select the appropriate runner in your <code>.gitlab-ci.yml</code> file for the job to be executed properly. For more details on the <code>.gitlab-ci.yml</code> file, please see upstream docs.</p>"},{"location":"services/gitlab-ci/#gitlab-ciyml-configuration-sections","title":"<code>.gitlab-ci.yml</code> Configuration Sections","text":"<p>GitLab uses a per repository <code>.gitlab-ci.yml</code> file. On any commit, merge request, or merge gitlab will attempt to trigger a CI/CD pipeline based on the contents of this file. Within the <code>.gitlab-ci.yml</code> file you can limit jobs to only run under certain conditions. A common workflow is to have linting and validation happen on every commit to a non-master/non-main branch. Larger more complex tasks are then performed when that branch is merged back into master/main. All jobs launched on a given event are organized into a <code>Pipeline</code>. You can watch the progress of your pipeline via the CI/CD pipeline page for your <code>Project</code>.</p> <p> </p> GitLab Group and Projects screenshot <p> </p> GitLab Group and Projects screenshot"},{"location":"services/gitlab-ci/#tags","title":"Tags","text":"<p>Tags are used to select which runner a job will be sent to. Improper tags can prevent your job from running and result in a failed job. Tags should be added by extending the defaults in the 'anl/ci-resources/defaults' runner.yml file. ALCF specific tags are described here in case overrides are needed.</p>"},{"location":"services/gitlab-ci/#alcf-specific-tags","title":"ALCF Specific tags","text":"<p>Two tags are necessary to run on our systems. One tag will select which cluster the jobs are sent to. The other will determine if the job is to be run locally on the gitlab runner host, or if it is to be submitted to a job scheduler on an HPC cluster.</p> <p>Cluster Tag(s)</p> Cluster tag Description Polaris polaris This tag will send jobs to the Polaris HPC runners <p>Job Type Tag(s)</p> tag Description shell This tag will execute the job locally on the gitlab runner host. batch This tag will submit the job to the HPC cluster's job scheduler."},{"location":"services/gitlab-ci/#variables","title":"Variables","text":"<p>Variables can be stored two ways, inline in the <code>.gitlab-ci.yml</code> file or as a setting in the GitLab <code>Group</code> or <code>Project</code> itself. Variables are exported as environment variables by gitlab-runner for each job and can be used inside the <code>.gitlab-ci.yml</code> file.</p> <p>GitLab also has a list of predefined variables available in every GitLab CI/CD pipeline.</p> <p>To set a variable directly in the <code>.gitlab-ci.yml</code> file, declare a <code>variables:</code> section with each <code>VariableName: \"VariableValue\"</code> being on its own line. <code>variables:</code> can be declared globally or in individual jobs.</p> <p>Example: Declaring variables <pre><code>variables:\n  GlobalVariable1: \"Global Value 1\"\n  GlobalVariable2: \"Global Value 2\"\n\njob:\n  variables:\n    LocalVariable: 'This is a local variable'\n  script:\n    - 'echo $LocalVariable'\n</code></pre></p> <p>To store variables in the <code>Group</code> or <code>Project</code> settings, in the left side menu, click <code>Settings&gt;CI/CD</code>. Expand the Variables option on the right side frame. You can then add variables by clicking <code>Add variable</code>.</p> <p>For for more details please set please see the upstream docs</p> <p> </p> GitLab Group and Projects screenshot <p> </p> GitLab Group and Projects screenshot"},{"location":"services/gitlab-ci/#alcf-specific-variables","title":"ALCF Specific Variables","text":"<p>If you are planning to submit jobs to a scheduler then you will need to specify a per system variable <code>ANL_${CLUSTER}_SCHEDULER_PARAMETERS</code>; where <code>${CLUSTER}</code> is the name of the cluster. This variable will contain any command line flags you would need to submit jobs as if you were on the command line / scripting. Please consult the below table for more info.</p> Cluster Scheduler Variable Name Support docs Polaris PBS ANL_POLARIS_SCHEDULER_PARAMETERS Polaris Getting Started <p>Example: Running a batch job <pre><code>include:\n  - project: 'anl/ci-resources/defaults'\n    ref: main\n    file:\n      - '/runners.yml'\n\nbatch_test:\n  extends: .polaris-batch-runner\n  variables:\n    ANL_POLARIS_SCHEDULER_PARAMETERS: \"-A ProjectName -l select=1,walltime=10:00,filesystems=home -q myQueue\"\n  script:\n    - echo \"Job start\"\n    - aprun -n 1 id\n    - aprun -n 1 hostname\n    - aprun -n 1 echo \"Running with setuid batch runner\"\n    - echo \"Job end\"\n</code></pre></p>"},{"location":"services/gitlab-ci/#stages","title":"Stages","text":"<p>Jobs can be organized into <code>stages</code>. Jobs in the next stage will not start until all dependencies in the previous stage have completed. This is often used if there building and testing steps required before code may be ran or packaged. These stages are assembled in a <code>Pipeline</code>, a directed graph of <code>stages</code>. By default GitLab includes the following stages executed in the below order : <pre><code>.pre\nbuild\ntest\ndeploy\n.post\n</code></pre></p> <p>You may declare your own stages by first declaring a <code>stages:</code> array near the top of your <code>.gitlab-ci.yml</code> file. Stages will be processed in the order given in the array.</p> <p>Example: Declaring Stages <pre><code>stages:\n  - stage1\n  - stage2\n  - stage3\n</code></pre></p> <p>Example: Pipeline with custom stages <pre><code># this include allows us to reference defaults in anl/ci-resource/defaults\ninclude:\n  - project: 'anl/ci-resources/defaults'\n    ref: main\n    file:\n      - '/runners.yml'\n\nvariables:\n  ANL_POLARIS_SCHEDULER_PARAMETERS: \"-A ProjectName -l select=1,walltime=10:00,filesystems=home -q myQueue\"\nstages:\n  - stage1\n  - stage2\ntest1:\n  stage: stage1\n  extends: .polaris-shell-runner\n  script:\n    - export\n    - id\n    - hostname\n    - echo \"Running with setuid shell runner\" \n    - echo test &gt; test.txt\ntest2:\n  stage: stage2\n  extends: .polaris-batch-runner\n  script:\n    - echo \"Job 2 start\"\n    - aprun -n 1 id\n    - aprun -n 1 hostname\n    - aprun -n 1 echo \"Running with setuid batch runner\"\n    - echo \"Job 2 end\"\n</code></pre></p>"},{"location":"services/gitlab-ci/#rules","title":"Rules","text":"<p>GitLab allows for CI/CD jobs to be launched only if certain conditions are met. GitLab sets a series of variables in addition to any the user explicitly sets when a job launches. A job can check these variables and choose to run or not based on the results. This is often used to ensure certain jobs only run on commits, merge requests, and/or merges. By default if any rule matches it will run. You can override this behavior with commands like <code>when: never</code> when a conditional matches.</p> <p>For more details please set please see the upstream docs</p> <p>Rules can use the following conditional checks: <pre><code>if\nchanges\nexists\nallow_failure\nvariables\nwhen\n</code></pre></p> <p>Example: GitLab job designed to only run on merge requests <pre><code>test1:\n  rules:\n    - if: $CI_COMMIT_TAG                    # Do not execute jobs for tag context\n      when: never\n    - if: $CI_COMMIT_BRANCH == \"master\"     # Do not run on master, since will run on the merge request just prior\n      when: never\n    - if: $CI_MERGE_REQUEST_IID             # CI_MERGE_REQUEST_IID exists, so run job\n  stage: stage1\n  extends: .polaris-shell-runner\n  script:\n    - echo \"Run test 1\"\n</code></pre></p>"},{"location":"services/gitlab-ci/#job-scheduling","title":"Job Scheduling","text":"<p>GitLab provides pipeline scheduling functionality to support recurring pipelines, specified using a Cron-like syntax. Pipeline scheduling is available in the project sidebar under \"Build\" &gt; \"Pipeline schedules\". For more details, see the upstream documentation.</p>"},{"location":"services/gitlab-ci/#template-jobs","title":"Template Jobs","text":"<p>GitLab allows you to create <code>template jobs</code>, these are pieces of job specifications which can be included in jobs. Each <code>template job</code> name must begin with a period (.) and follow the same syntax as normal jobs. To instantiate a job based on the <code>template job</code> use the keyword <code>extends</code>. If your specific job declares a key/value already in the template, the specific job will overwrite it.</p> <p>Example: Use a job template so two tests will only run on merge requests <pre><code>.MR_rules:\n  rules:\n    - if: $CI_COMMIT_TAG                    # Do not execute jobs for tag context\n      when: never\n    - if: $CI_COMMIT_BRANCH == \"master\"     # Do not run on master, otherwise runs everything from scratch on merge\n      when: never\n    - if: $CI_MERGE_REQUEST_IID\n    - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'    \n\ntest1:\n  extends: .MR_rules\n  stage: stage1\n  extends: .polaris-shell-runner\n  script:\n    - echo \"Run test 1\"\ntest2:\n  extends: .MR_rules\n  stage: stage2\n  extends: .polaris-shell-runner\n  script:\n    - echo \"Run test 2\"\n</code></pre></p>"},{"location":"services/gitlab-ci/#console-output","title":"Console Output","text":"<p>To see the output of a job click on it in the GUI and it will show the STDOUT and STDERR from the job run. If the job did not launch successfully it will have error messages from gitlab-runner or Jacamar-CI or both.   Please be aware of any sensitive data you do not want exported or saved to the output console, such as passwords.   Please do not output large amounts of data from your jobs to the stdout. If your CI/CD job outputs large amounts of text to STDOUT or STDERR, consider redirecting it into a job log.</p> <p> </p> GitLab Group Job Console"},{"location":"services/gitlab-ci/#storage-use-and-policy","title":"Storage Use and Policy","text":""},{"location":"services/gitlab-ci/#gitlab-project-quota","title":"GitLab Project Quota","text":"<p>Each repository has a default quota of 1GB. Quota increases may be requested by emailing Support. This quota is separate from the storage quotas allocated to ALCF Projects and ALCF Users on the HPC clusters and shared filesystems.</p>"},{"location":"services/gitlab-ci/#cicd-filesystem-usage","title":"CI/CD Filesystem usage","text":"<p>CI/CD jobs will run out of your home directory by default. Each job will begin by cloning the repository into a path under <code>~/.jacamar-ci</code> and will continue to write there unless you reference other destinations in your CI/CD job. You will need to ensure that you have the minimum amount of space for this runner operation. If you do not, the job will fail to run. Each gitlab runner will create a new sub directory under <code>~/.jacamar-ci</code> for itself, however it will reuse space for subsequent pipelines launched for that project on that runner.</p> <p>It is recommended that if you need more space then your home directory can provide, that you leverage any ALCF Project space you may have been allocated on a shared filesystem.</p>"},{"location":"services/gitlab-ci/#gitlab-ci-access-termination-policy","title":"GitLab-CI Access Termination Policy","text":"<p>Projects that have been inactive for at least 6 months will have their access disabled and their repositories deleted. Notification will be sent to the PI 30 days prior to the day of the action. </p> <p>Inactivity is defined as, but not limited to:</p> <ul> <li>No new projects created</li> <li>No new commits to an existing project</li> <li>Prolonged period of continuously failing CI/CD jobs (In the case of re-occurring scheduled jobs)</li> </ul>"},{"location":"services/jupyter-hub/","title":"JupyterHub","text":"<p>JupyterHub is an open-source service application that enables users to launch separate Jupyter instances on a remote server. ALCF JupyterHub provides access to Polaris with the same authentication protocol that is used to access these systems, but through a web interface rather than a terminal. On the ALCF JupyterHub home page, users can choose their desired system. Upon selection, they'll be directed to the sign-in page to enter their ALCF username and passcode token.</p> <p> </p> ALCF JupyterHub home page and sign-in screen <p>We describe below how to use JupyterHub on Polaris in more detail.</p>"},{"location":"services/jupyter-hub/#polaris","title":"Polaris","text":"<p>The Polaris JupyterHub server runs on a Polaris login node and launches individual  users' environments on the compute nodes through the PBS job scheduler.  After the authentication step, the user will be presented with the menu of the available job options to start the Jupyter instance.</p> <ul> <li>Select a job profile:  This field lists the available profiles, which is   limited to \u201cPolaris Compute Node\u201d at this time.</li> <li>Queue Name: This field provide a list of available queues on the system.</li> <li>Project List: This field displays the active projects associated with the user   on Polaris.</li> <li>Number of Nodes: This field allows the user to select the number of compute   nodes to be allocated.</li> <li>Runtime (minutes:seconds): This field allows the user to set the runtime of   the job in minutes and seconds. The user should refer to the Polaris queue   scheduling policy   for minimum and maximum runtime allowed for the selected queue.</li> <li>File Systems: This field allows the user to select the file systems to be   mounted. By default all the file systems are selected.</li> </ul> <p> </p> Polaris Job options <p>Once the appropriate information is provided the user will click the \u201cStart\u201d button and wait for the job to spawn. If there's an extended wait time due to a lengthy job queue, the interface might time out, leading to the job's removal from the queue. If not, the job kicks off and it begins to use up the user's allocation based on the chosen job options. It's crucial for users to shut down the server when resources are no longer required. Failing to do so will result in continued consumption of the allocated time until the predetermined runtime concludes.</p> <p> </p> Job queued <p>NOTE: If you would like to change your selection about where to run the Jupyter instance after the Notebook started, you need to stop the server to be able to see the drop-down menu again.</p>"},{"location":"services/jupyter-hub/#known-issues","title":"Known Issues","text":""},{"location":"services/jupyter-hub/#spawn-failed-timeout","title":"Spawn Failed: Timeout","text":"<p>This happens when the queue is backed up. Since Jupyter is interactive, it expects an immediate connect.  Therefore, it waits 5 minutes for your job to begin before throwing this error.  You can monitor the queue usage with Gronk and submit when there isn't a wait. </p>"},{"location":"services/jupyter-hub/#additional-notes","title":"Additional Notes","text":""},{"location":"services/jupyter-hub/#custom-ipython-kernels","title":"Custom IPython Kernels","text":"<p>ALCF JupyterHub provides a set of pre-configured IPython kernels for the users to select. However, users may need custom kernels with additional packages installed. This can be achieved by first creating custom Python environments either through venv or conda. More information on creating custom Python environments can be found in our documentation for Polaris. After activating the custom environment, <code>ipykernel</code> package needs to be installed with the following command: <pre><code>pip install ipykernel\n</code></pre> Once <code>ipykernel</code> is installed, the custom kernel can be added to the list of available kernels with the following command: <pre><code>python -m ipykernel install --user --name custom_kernel_name \n</code></pre> where <code>custom_kernel_name</code> is the name of the kernel that will appear in the kernel list. This name does not have to match the name of the environment, but should not contain spaces. If you want more flexibility in naming, you can add the <code>--display-name</code> argument as shown below. <pre><code>python -m ipykernel install --user --name custom_kernel_name --display-name \"Polaris Python 3.11 Tensorflow 2.4.1\" \n</code></pre> Note that, you still need to provide <code>--name</code> with a simple name that does not contain spaces. Additionally, you can also set environment variables for the kernel with the <code>--env</code> argument, i.e: <pre><code>python -m ipykernel install --user --name custom_kernel_name --env http_proxy http://proxy.alcf.anl.gov:3128 --env https_proxy http://proxy.alcf.anl.gov:3128\n</code></pre> You can see the list of available kernels with the following command: <pre><code>jupyter kernelspec list\n</code></pre> By default, the kernels are installed in the user's home directory under <code>~/.local/share/jupyter/kernels/</code>. All the configuration is specified in the <code>kernel.json</code> file under the kernel directory. For the example above, the path for the json file will be <code>~/.local/share/jupyter/kernels/custom_kernel_name/kernel.json</code>. You can edit this file to add additional environment variables or change the display name.</p> <p>Once you've followed the steps above, your new kernel will be visible on JupyterHub. It's recommended to perform these steps in a terminal, ideally on the login node of the system you're using. After setting up a custom kernel, you can easily add more packages directly within JupyterHub. Simply create a new notebook using your custom kernel and use the %pip or %conda magic commands to install packages. If you're on a compute node, remember to enable internet access by configuring the <code>http_proxy</code> and <code>https_proxy</code> environment variables as previously mentioned.</p>"},{"location":"services/jupyter-hub/#accessing-project-folders","title":"Accessing Project Folders","text":"<p>Jupyter file browser limits the user to view files and directories within their home directory. To access directories located outside of the user home directory a symbolic link to the directory must be created within the user home directory. An example of this is:</p> <p><pre><code>ln -s /project/ABC ~/ABC_project_link\n</code></pre> Please note that one can run any shell command directly on a Jupyter notebook by simply adding an exclamation mark, <code>!</code>, to the beginning of the command. For example, the above command can be run from a notebook cell as follows:</p> <pre><code>!ln -s /project/ABC ~/ABC_project_link\n</code></pre>"},{"location":"services/jupyter-hub/#ending-a-jupyter-notebook-running-on-a-compute-node","title":"Ending a Jupyter Notebook running on a compute node","text":"<p>Failing to correctly end a running Jupyter Notebook will continue to consume the selected project's allocation on the resource in question. When a user has completed their task in Jupyter the user should stop the Jupyter instance running on the compute node before logging out.  To stop the Notebook, click the \u201cControl Panel\u201d button in the top right, then click \u201cStop My Server\u201d.</p> <p> </p> Stop panel <p> </p> Stop server"},{"location":"services/jupyter-hub/#resources","title":"Resources","text":"<ul> <li>Jupyter Lab documentation.</li> <li>ALCF Hands-on HPC Workshop presentation on Python and Jupyter on Polaris: slides and video.</li> <li>ALCF webinar on JupyterHub: slides and video.</li> </ul>"},{"location":"sophia/getting-started/","title":"Getting Started on ThetaGPU","text":""},{"location":"sophia/getting-started/#references","title":"References","text":"<p>In addition to the content below, here is a getting started video covering the basics of using ThetaGPU and a  related video on Lustre File Striping Basics. This should help you get up and running quickly on the GPU nodes.</p> <ul> <li>Video on Getting Started on ThetaGPU </li> <li>Lustre File Striping Basics</li> </ul>"},{"location":"sophia/getting-started/#login-to-thetagpu","title":"Login to ThetaGPU","text":"<p><pre><code>ssh -A username@theta.alcf.anl.gov\n</code></pre> Replace the username with your ALCF username. You will prompted to type in your MFA password. Note: In order to log in to ALCF systems, you need to have an active ALCF account.</p>"},{"location":"sophia/getting-started/#setup-thetagpu-environment","title":"Setup ThetaGPU environment","text":"<p>Once logged in, you land on theta login nodes (thetalogin1 - thetalogin6). </p> <p>You can set an environment variable to control which instance the default commands (qsub, qstat, etc) will interact with. The primary use case here will be users who only use GPU nodes, but are working from the Theta login nodes.  To do so, you may do: <pre><code>module load cobalt/cobalt-gpu\n</code></pre> To switch back you may do <code>module load cobalt/cobalt-knl</code> which would make cobalt commands interact with the original Cobalt instance and launch jobs on the KNL nodes.</p> <p>Alternatively, If you are on a GPU node, for instance, the service nodes (thetagpusn1-2), then commands will default to the GPU instance. To head to a service node from the theta login nodes use: <pre><code>ssh thetagpusnX\n</code></pre> you can also set <code>COBALT_CONFIG_FILES=&lt;path to cobalt config&gt;</code> </p> <ul> <li>knl config: /etc/cobalt.knl.conf</li> <li>gpu config: /etc/cobalt.gpu.conf</li> </ul> <p>You can use suffixed commands to explicitly control which instance you are interacting with. If you regularly use both types of nodes, this is the recommended path to avoid confusion and to prevent launching jobs on the wrong architecture.</p> <p>All the commands you are used to are there, they take the same command line parameters, etc., they just have either -knl or a -gpu suffix on them. For instance:</p> <ul> <li>qsub-knl  would submit a job to the KNL nodes <li>qstat-gpu would check the queue status for the GPU nodes</li> <p>For all the build and development please use ThetaGPU compute nodes. Please avoid using the service nodes thetagpusn[1,2] as they have not been set up for development. </p> <p>Using \"qstat -Q\" to see all available queues. You can submit your job to a specific queue (as long as you are part of that queue) using \"qsub -q queue_name\".</p> <ul> <li>For more information on all ThetaGPU queues visit: Queue Policy on ThetaGPU</li> <li>For more information on submitting a job visit: Submit a job on ThetaGPU</li> </ul>"},{"location":"sophia/getting-started/#project-space-and-home","title":"Project Space and Home:","text":"<p>Every user has a home directory located at /home/username.</p> <p>The project folder is located at: <pre><code>/grand/project_name or /eagle/project_name\n/lus/grand/projects/project_name or /lus/eagle/projects/project_name\n</code></pre> Please use the project folder when building and running on ThetaGPUcompute nodes</p> <p>/grand is on an HDR network directly connected to ThetaGPU</p> <p>/home is on an FDR network which is up-linked to the HDR network via a straw, heavy use of this file system will result in Bad Things\u2122 again.</p> <p>For more information on all available file systems visit: File Systems </p>"},{"location":"sophia/getting-started/#software","title":"Software","text":"<p>ThetaGPU is new, so it has limited ALCF provided software. ThetaGPU compute nodes are setup with CUDA11</p> <p>Default Nvidia installed software will just be in your PATH</p> <p><code>which nvcc</code></p> <p>To see the available software via modules</p> <p><code>module avail</code></p> <p>Other ThetaGPU software can be found in</p> <p><code>/soft/thetagpu</code></p> <p>Theta software can be found in  <code>/soft</code>   \u2013 Anything that is not compute specific will be useable on the AMD host CPUs   \u2013 cmake is good example of something that can be used</p> <p>For more information on compiling and linking on ThetaGPU visit: Compiling and Linking on ThetaGPU</p>"},{"location":"sophia/getting-started/#nvidia-hpc-sdk","title":"NVIDIA HPC SDK","text":"<p><code>module use /soft/thetagpu/hpc-sdk/modulefiles</code></p> <p>\u2013 Adds more modules for Nvidia SDK</p> <p><code>module avail</code></p> <p>\u2013 Shows you the new modules you have available \u2013 20.9 version will be loaded by default \u2013 21.2 version available using CUDA11 driver \u2013 21.3 version available using CUDA11 driver</p> <p><code>nvhpc</code></p> <p>\u2013 Loads the SDK and sets various compiler environment variables so that build tools will likely pick up the compilers by default \u2013 MPI wrappers disabled</p> <p><code>nvhpc-byo-compiler</code></p> <p>\u2013 Identical to nvhpc but doesn\u2019t set compiler environment variables</p> <p><code>nvhpc-nompi</code></p> <p>\u2013 Excludes MPI libraries</p>"},{"location":"sophia/getting-started/#proxy","title":"Proxy","text":"<p>If the node you are on doesn\u2019t have outbound network connectivity, add the following to your ~/.bash_profile file to access the proxy host <pre><code># proxy settings\nexport HTTP_PROXY=http://theta-proxy.tmi.alcf.anl.gov:3128\nexport HTTPS_PROXY=http://theta-proxy.tmi.alcf.anl.gov:3128\nexport http_proxy=http://theta-proxy.tmi.alcf.anl.gov:3128\nexport https_proxy=http://theta-proxy.tmi.alcf.anl.gov:3128\n</code></pre></p>"},{"location":"sophia/getting-started/#io","title":"I/O","text":"<p>/grand is a Lustre file system. Default stripe size is 1MiB and stripe count is 1. If you have a large file to read or write with high performance (in parallel) \u2013 Set the stripe count higher than 1 \u2013 Use a specific directory for these files <pre><code>mkdir big_files\nlfs setstripe -c 8 big_files\n/raid/scratch for local disk in RAID-0 config\n</code></pre></p>"},{"location":"sophia/getting-started/#mpi","title":"MPI","text":"<p>ALCF provides a few MPI package built specifically for ThetaGPU \u2013 UCX is enabled</p> <p><code>module load openmpi</code></p> <p>\u2013 Default module is openmpi/openmpi-4.1.0</p> <p><code>module av openmpi</code></p> <p>List of possible openmpi modules</p>"},{"location":"sophia/applications-and-libraries/applications/gromacs/","title":"Gromacs on ThetaGPU","text":""},{"location":"sophia/applications-and-libraries/applications/gromacs/#what-is-gromacs","title":"What is Gromacs?","text":"<p>GROMACS is a versatile package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids, and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers.</p>"},{"location":"sophia/applications-and-libraries/applications/gromacs/#using-gromacs-at-alcf","title":"Using GROMACS at ALCF","text":"<p>ALCF offers assistance with building binaries and compiling instructions for GROMACS. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"sophia/applications-and-libraries/applications/gromacs/#building-gromacs","title":"Building Gromacs","text":"<ol> <li>Download latest source code: http://manual.gromacs.org/documentation/2022.1/download.html</li> <li><code>tar -xzf gromacs-2022.1.tar.gz</code></li> <li>Submit an interactive job to a ThetaGPU compute node from Theta login node: <pre><code>user@thetalogin4:~&gt;module load cobalt/cobalt-gpu\nuser@thetalogin4:~&gt;qsub -I -n 1 -t 60 -q single-gpu -A PROJECT --attrs filesystems=home\nJob routed to queue \"single-gpu\".\nWait for job 10108666 to start...\nOpening interactive session to thetagpu06-gpu0\n...\nuser@thetagpu06:~$ \n</code></pre></li> <li><code>cd gromacs-2022.1</code></li> <li><code>mkdir build</code></li> <li><code>module load cmake</code></li> <li><pre><code>cmake -DCMAKE_C_COMPILER=mpicc -DCMAKE_CXX_COMPILER=mpicxx \\\n      -DBUILD_SHARED_LIBS=OFF -DGMX_BUILD_OWN_FFTW=ON \\\n      -DCMAKE_INSTALL_PREFIX=/path-to/gromacs-2022.1/build \\\n      -DGMX_MPI=ON -DGMX_OPENMP=ON -DGMX_GPU=CUDA \\\n      -DCUDA_TOOLKIT_ROOT_DIR=/user/local/cuda-11.4\n</code></pre></li> <li><code>make \u2013j 16</code></li> <li><code>make install</code></li> <li>The installed binary is <code>build/bin/gmx_mpi</code>.</li> </ol>"},{"location":"sophia/applications-and-libraries/applications/gromacs/#running-gromacs-on-thetagpu","title":"Running Gromacs on ThetaGPU","text":"<p>Prebuilt Gromacs binaries can be found in the directory <code>/soft/applications/gromacs/gromacs_cuda</code>.</p> <p>A sample qsub script follows that will run GROMACS on a full node using all eight GPUs available.</p> <pre><code>#!/bin/bash -l\n#COBALT -n 1\n#COBALT -t 30 \n#COBALT -q full-node \n#COBALT -project catalyst \n#COBALT --attrs filesystems=home,theta-fs0\n\nNODES=`cat $COBALT_NODEFILE | wc -l`\n\nmpirun -hostfile $COBALT_NODEFILE --np 8 \\\n      /soft/applications/gromacs/gromacs_cuda/gmx_mpi.2022.1 \\\n      mdrun -ntomp 8 -gputasks 01234567 -nb gpu -pme gpu -npme 1 \\\n      -dlb yes -resethway -pin on -v deffnm step5_1 -g test.log\n</code></pre> <p>We strongly suggest that users try combinations of different numbers of nodes, MPI ranks per node, number of GPU tasks/devices, GPU task decomposition between nonbonded and PME kernels, and OMP threads per rank to find the optimal throughput for their particular workload.</p> <p>The following is a representative benchmark for a system with 30,000 atoms generated on a single ThetaGPU node with above example.</p> Core time(sec) Wall time(sec) (%) Time 691.769 10.810 6399.6 ns/day hour/ns Performance 399.661 0.060"},{"location":"sophia/compiling-and-linking/compiling-and-linking-overview/","title":"Compiling and Linking on ThetaGPU","text":""},{"location":"sophia/compiling-and-linking/compiling-and-linking-overview/#overview","title":"Overview","text":"<p>ThetaGPU has AMD processors on the service nodes (thetagpusn1,2) and AMD processors and NVIDIA A100 GPUs on the compute nodes [see overview page]. The service nodes can be used to create containers and launch jobs.</p> <p>Note: Until the cross-compiling environment is set up or dedicated build nodes get added, the compute nodes will have to be used for compiling. Do not compile codes on service nodes (thetagpusn1,2).</p> <p>The default programming environment on the ThetaGPU compute nodes is the GNU compiler tools coupled with NVIDIA\u2019s CUDA toolkit. </p> <p>Note: Symlinks to the project directories are not available on the compute nodes. Use the full path (eg: /lus/theta-fs0/projects/) to access the project directory. <p>For non-GPU codes:   - gcc \u2013 for C compiler   - g++ \u2013 for C++   - gfortran \u2013 for Fortran</p> <p>For CUDA codes, please note that there is a new driver(v470) and default cuda toolkit(v11.4); the old toolkit is still available on the compute nodes at /usrlocal/cuda-11.3   - nvcc</p> <p>For MPI, the latest MPI is in /lus/theta-fs0/software/thetagpu/openmpi-4.0.5.   - mpicc   - mpicxx   - mpif77/mpif90 not configured yet</p> <p>mpirun is a wrapper in /usr/local/bin that sets the appropriate options and uses the mpirun in the MPI directory above.</p> <p>On the service nodes, GNU compilers are available.</p>"},{"location":"sophia/compiling-and-linking/compiling-and-linking-overview/#modules-on-thetagpu","title":"Modules on ThetaGPU","text":"<p>Available modules can be listed (on thetagpusn1,2) via the command: <pre><code>user@thetagpusn1:~$ module avail\n\n\n----------------------- /usr/local/lmod/lmod/modulefiles -----------------------\n\n   Core/lmod    Core/settarg\n\n\n-------- /lus/theta-fs0/software/environment/thetagpu/lmod/modulefiles ---------\n\n   Core/StdEnv                 (L,D)    conda/tensorflow/2021-01-08\n\n   aocl/blis-3.0                        conda/tensorflow/2021-03-02      (D)\n\n   conda/pytorch/2020-11-25             nccl/nccl-v2.8.4-1_CUDA11\n\n   conda/pytorch/2021-03-02    (D)      openmpi/openmpi-4.0.5            (L)\n\n   conda/tensorflow/2020-11-11          openmpi/openmpi-4.1.0_ucx-1.10.0\n\n   conda/tensorflow/2020-12-17          openmpi/openmpi-4.1.0            (D)\n\n   conda/tensorflow/2020-12-23\n\n\n-- /lus/theta-fs0/software/spack/share/spack/modules/linux-ubuntu18.04-x86_64 --\n\n   autoconf-2.69-gcc-7.5.0-wmttzuv\n\n   autoconf-archive-2019.01.06-gcc-7.5.0-bdyarrk\n\n....\n</code></pre> Loaded modules in your environment can be listed (on thetagpusn1,2) via the command:</p> <p><pre><code>user@thetagpusn1:~$ module list\n\n\nCurrently Loaded Modules:\n\n1) openmpi/openmpi-4.0.5   2) Core/StdEnv\n</code></pre> To load new modules use: <pre><code>user@thetagpusn1:~$ module load &lt;module_name&gt;\n</code></pre> There are few modules available at this time, but the number will grow as more packages become available.</p> <p>Usage: csh and zsh users do not have to do anything special to their environments. bash users, however, will need to add the following to any job scripts: <pre><code>#!/bin/bash\n. /etc/profile\n</code></pre> bash users are also encouraged to modify their ~/.bashrc to ensure the ubuntu system /etc/bash.bashrc file is sourced properly: <pre><code># Source global definitions\nif [ -f /etc/bashrc ]\nthen\n    . /etc/bashrc\nelif [ -f /etc/bash.bashrc ]\nthen\n    . /etc/bash.bashrc\nfi\n</code></pre></p>"},{"location":"sophia/compiling-and-linking/continuous-integration/","title":"Continuous Integration on ThetaGPU","text":""},{"location":"sophia/compiling-and-linking/continuous-integration/#overview","title":"Overview","text":"<p>Continuous Integration (CI) in software development is the practice of committing code changes regularly to a version control system and having automated processes perform build, test, package, and deploy activities. The key concepts of CI include high frequency, repeatability, and automation in order to realize increased quality and ease of delivery. The main goal CI aims to achieve is the elimination of build and deployment issues, which in turn improves development cycles, provides a timely feedback loop with developers, and results in higher quality deliverables with reduced development time.</p> <p>CI usually describes the work that is done by a deployment or operations team to build and deploy code throughout an environment and make it available to the different interested teams involved in the SDLC. The steps that make up this process are referred to as a workflow or pipeline, which, when combined with automation, provides the mechanism for Continuous Integration.</p> <p>Today it is a common practice to use a CI tool for defining pipelines and executing the tasks required to take code from a source stored in a version control system to compiled and packaged artifacts executing in production. Two excellent examples of CI tools are Jenkins and GitLab.</p>"},{"location":"sophia/compiling-and-linking/continuous-integration/#ci-tools-at-alcf","title":"CI Tools at ALCF","text":"<p>The ALCF provides a tool for implementing CI processes named Jenkins. Using the Jenkins tool, ALCF projects can make use of CI functionality. The Jenkins CI tool enables projects to auto-compile their custom software code, automate testing cycles, provide a feedback loop, and submit jobs to HPC resources. The custom pipelines needed for each project can be defined in Jenkins by project users, and execution can be controlled through triggers.</p>"},{"location":"sophia/compiling-and-linking/continuous-integration/#jenkins","title":"Jenkins","text":"<p>Jenkins \"is a self-contained, open-source automation server which can be used to automate all sorts of tasks relating to building, testing, and delivering or deploying software.\" Jenkins is the tool that provides CI/CD functionality for ALCF resources. Most importantly, it provides the mechanisms required for DevOps automation.</p> <p>Additional information, technical and user documentation, and community support can be found on the Jenkin's project website.</p>"},{"location":"sophia/compiling-and-linking/continuous-integration/#alcf-jenkins","title":"ALCF Jenkins","text":"<p>Log in to the ALCF Jenkins web portal using your ALCF credentials (ALCF username and cryptocard token password).</p>"},{"location":"sophia/compiling-and-linking/continuous-integration/#projects-using-ci","title":"Projects Using CI","text":"<p>Enabling a project to use CI requires some additional steps and configuration to get started. Once enabled for a project, users can access the Jenkins CI environment and configure CI jobs or pipelines for building and testing their project code.</p>"},{"location":"sophia/compiling-and-linking/continuous-integration/#on-boarding-with-ci","title":"On-Boarding with CI","text":"<p>To enable CI for your project, send an email to the ALCF Service Desk requesting CI functionality for your project and include the ALCF project shortname and the PI\u2019s name with the request.</p> <p>The project\u2019s PI will get an email with details and a new CI account associated with the project. This is a service account that the Jenkins CI tool will use when executing tasks associated with your project. The CI account will be listed as a project member and added to the project\u2019s group for access controls.</p>"},{"location":"sophia/compiling-and-linking/continuous-integration/#folders","title":"Folders","text":"<p>Each CI project will have a top-level \u2018folder\u2019 created with the project\u2019s name. Please do not delete the project folder: it is used for organization in the multi-project environment and is required for implementing the needed level of security. The project folder is where all of the project objects are stored, you can additionally create any subfolders, jobs, pipelines, etc. within your project folder to meet your CI needs.</p> <p>In the example below, we have a project named \u2018TestFromJanet2\u2019 with an associated folder.</p> <p> </p> CI folders screenshot"},{"location":"sophia/compiling-and-linking/continuous-integration/#nodes","title":"Nodes","text":"<p>Each CI project will have an assigned node for execution. Nodes execute jobs defined within a project, typically on the target system\u2019s login node. Currently there are CI nodes configured for HPC systems Theta and Cooley, as well as non-HPC nodes with 32 cores (Intel Xeon Processor E5-2683 v4) and 128 GB RAM for generic x86 processing with access to the Mira shared filesystems.</p> <p>In the example below, the node for this project is named \u2018TestFromJanet2-Theta. Jobs and pipeline steps triggered from Jenkins will execute on the TestFromJanet2-Theta node which has been configured to use host: thetalogin1 and will use the project\u2019s CI user ID (provided during on-boarding) to execute scripts or code just as if the end user had logged into the thetalogin1 node and executed the same set of actions manually from the command line.</p> <p> </p> CI folders screenshot"},{"location":"sophia/compiling-and-linking/continuous-integration/#job-configuration","title":"Job Configuration","text":"<p>When configuring any new job within a project there are some guidelines to follow for setting permissions and nodes. Project data is kept secure by setting up permissions at the project level and node selection controls where the job will execute.</p> <p>When creating jobs, enable project-based security, set the inheritance strategy, and add your project\u2019s group name to the permission matrix table. The example below has enabled project-based security, set the inheritance strategy to Do not inherit permission grants from other ACLs, and added the project\u2019s group name \u2018\u2018TestFromJanet2\u2019 to the permission matrix granting all rights to the group.</p> <p> </p> CI folders screenshot <p>To assign the node that the project will use to execute jobs, select the option Restrict where this project can be run and enter the project\u2019s assigned node. The example below has assigned the jobs to node: TestFromJanet2-Theta so that any time the job is executed, it runs on host: thetalogin1.</p> <p> </p> Execute"},{"location":"sophia/compiling-and-linking/continuous-integration/#common-jenkins-features","title":"Common Jenkins Features","text":""},{"location":"sophia/compiling-and-linking/continuous-integration/#version-control-features","title":"Version Control Features","text":"<p>Jenkins can connect to most common version control systems (VCS), including git/svn. The ALCF Jenkins instance can connect with local VCS hosted at at ANL as well as with external VCS, such as that hosted at Github.</p> <p>On the job configuration page, look for the section Source Code Management (SCM). If it is there already, add it to the job. The required fields for SCM are Repository URL and Credentials. The example below shows a connection to the ALCF internal Gitlab VCS and uses previously setup credentials.</p> <p> </p> Repository access <p>To use the new connection to the Git repository interactively, configure the job to be parameterized and add a a Git Parameter to the job. The example below shows the configuration to select a branch at build time.</p> <p> </p> Git Parameter <p>On the build screen, select from the drop-down menu the branch to be referenced during this job execution. The example below shows the list of available branches from the configured repository. It is automatically populated duing the Git connector configuration of the preceding steps. If a new branch is added to the Git repository, it will display in the populated list of avialable branches when the job runs in Jenkins.</p> <p> </p> Select branch"},{"location":"sophia/compiling-and-linking/continuous-integration/#build-steps","title":"Build Steps","text":"<p>Build steps are where users define executable tasks and jobs do something interesting within an envionment. A core component of Jenkens, build steps can take a few different forms and are morst commonly configured to call remote scripts for code building and deployment. A build step can even contain the shell script contents to execute on the remote machine.</p> <p> </p> Select branch <p>The example below uses the Execute Shell build step type and codes the shell logic within the Jenkins portal.</p> <p> </p> Execute shell"},{"location":"sophia/compiling-and-linking/continuous-integration/#pipelines","title":"Pipelines","text":"<p>Pipelines in Jenkins allow for more advanced execution logic and are written in Groovy. A pipeline can be added directly to your project as an object using the New Item link. More commonly, they are defined in a \"Jenkinsfile\" and stored in VCS along with the project code. The Jenkinsfile can be created and edited outside of the Jenkins system using any text editor.</p> <p>To add a pipeline manually, select Pipeline from the new New Item dialog box.</p> <p> </p> New Item dialog box <p>The pipeline can then be configured and edited from the project folder in the same way as jobs, as shown in the example below.</p> <p> </p> Pipeline configuration <p>To add a pipeline using a Jenkinsfile in SCM, add the pipeline object as shown below. On the pipeline configuration page, select Pipeline script from SCM and provide the SCM connection details along with the Script Path. The Script Path is the path-to and filename where the Jenkinsfile is located within the SCM repository. The example below uses a Jenkinsfile stored in the project source code from the ALCF Git repository, and the Jenkinsfile containing the Groovy code pipeline definition is located at scripts/Jenkinsfile from the repository root.</p> <p> </p> Pipeline script path"},{"location":"sophia/compiling-and-linking/continuous-integration/#triggers","title":"Triggers","text":"<p>Triggers are events that intitiate tasks in Jenkins. Triggers can be called a few different ways, including directly by a user via the Build Now action (a time-based trigger similar to a Cron system), or based on commits made to source control.</p> <p>The example below shows a time-based configuration to run the job on a regular schedule. Details on the scheduling syntax can be found by clicking the blue question mark to the right of the Schedule field.</p> <p> </p> Build Triggers"},{"location":"sophia/compiling-and-linking/continuous-integration/#console-output","title":"Console Output","text":"<p>Jenkins provides console output and saves this history for each job run. During job execution you can view the live output from the tasks in a display similar to what would be seen if the commands were run directly in an interactive console.</p> <p> </p> Console output"},{"location":"sophia/compiling-and-linking/continuous-integration/#credentials","title":"Credentials","text":"<p>Credentials are stored in Jenkins and used when connecting to remote resources that require authentication in a non-interactive manner. Once defined, credentials can be used throughout the Jenkins system when configuring jobs, SCM connections, SSH connections, etc.</p> <p>To add a set of credentials, click on Credentials from the available options on the left-hand navigation menu. Then select System and click on the link for Global credentials.</p> <p> </p> Credentials <p>Click Add Credentials from the left-hand navigation menu and provide the required information. The example below configures a new credential set of type \"SSH Username with private key.\" Make sure Scope is set to \"Global.\" Provide the username, private key (copy and paste), and key passphrase, and then give a pertinent ID and detailed description to help identify and organize stored credentials in the system.</p> <p> </p> Add credentials"},{"location":"sophia/compiling-and-linking/continuous-integration/#faqs","title":"FAQS","text":"<p>Why does my project's execution node say it is offline? Node services for executing project tasks are inititated when there is demand for the node. The process of starting the node services acan take up to one minute; the status change is displayed in the Jenkins web portal. When there is no longer demand for the node, the services will stop again after one minute of idle time.</p> <p>Why is my shell environment different when executing tasks on a Jenkins node? Since Jenkins uses SSH with no tty, any shell scripts need to have this at the top so that login scripts are run against the session:</p> <p><code>#!/bin/bash -1</code></p>"},{"location":"sophia/compiling-and-linking/continuous-integration/#glossary","title":"Glossary","text":"<p>Continuous Integration (CI) - The process of automating the build and testing of code every time developers commit changes to version control.</p> <p>Pipeline - A CI pipeline is a list of tasks or jobs that are defined and executed as a procedure within a project. Pipeline is analogous to workflow.</p> <p>Source Control Management (SCM) - A term used in Jenkins to describe objects related to version control.</p> <p>Version Control System (VCS) - Software that manages access, storage, and revision history for a code respository.</p>"},{"location":"sophia/compiling-and-linking/continuous-integration/#appendix","title":"Appendix","text":""},{"location":"sophia/compiling-and-linking/continuous-integration/#abbreviated-setup","title":"Abbreviated Setup","text":"<ul> <li>Request CI capabilities for your project by emailing the ALCF Service Desk.</li> <li>Add jobs and pipelines to the project folder space to handle code compiling and testing.</li> <li>Configure jobs with credentials, SCM integrations, and trigger components depending on the intended behavior for your project.</li> <li>Execute jobs and pipelines by invoking the configured triggers.</li> </ul>"},{"location":"sophia/data-science-workflows/building-python-packages/","title":"Building Python Packages","text":"<p>To build Python packages for ThetaGPU, there are two options: build on top of a bare-metal build or build on top of (and within) a singularity container. Additionally, you can build a new container from NVIDIA's docker images.</p>"},{"location":"sophia/data-science-workflows/building-python-packages/#build-on-thetagpu-compute-using-conda","title":"Build on ThetaGPU compute using Conda","text":"<p>To build on ThetaGPU compute and install your own packages, login to theta and then submit an interactive job to log on to ThetaGPU compute node. </p> <p>Please see Running PyTorch with Conda or Running TensorFlow with Conda for more information.</p>"},{"location":"sophia/data-science-workflows/building-python-packages/#building-on-top-of-a-container","title":"Building on top of a container","text":"<p>At the moment, you will need two shells to do this: have one open on a login node (for example, <code>thetaloginN</code>, and one open on a compute node (<code>thetagpuN</code>). First, start the container in interactive mode: <pre><code>singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/pytorch_20.08-py3.sif bash\n</code></pre> From here, you can create a virtual env for installation: <pre><code>export VENV_LOCATION=/path/to/virtualenv # replace this with your path! \npython -m venv --system-site-packages $VENV_LOCATION\n</code></pre> Note: sometimes, the venv package is available and if not, you can try <code>python -m virtualenv</code>. If neither are available, you can install it in your user directory: <pre><code>pip install --user virtualenv\n</code></pre> and it should work.</p> <p>Next time you log in, you'll have to start the container, and then run source <code>$VENV_LOCATION/bin/activate</code> to re-enable your installed packages.</p>"},{"location":"sophia/data-science-workflows/building-python-packages/#reaching-the-outside-world-for-pip-packages","title":"Reaching the outside world for pip packages","text":"<p>You'll notice right away when you try to pip install you can not, because the connection fails. You can, however, go through a proxy server for pip by enabling these variables: <pre><code>export HTTP_PROXY=http://theta-proxy.tmi.alcf.anl.gov:3128\nexport HTTPS_PROXY=https://theta-proxy.tmi.alcf.anl.gov:3128\n</code></pre> Now, you can pip install your favorite packages: <code>pip install mpi4py</code></p>"},{"location":"sophia/data-science-workflows/building-python-packages/#building-custom-packages","title":"Building custom packages","text":"<p>Most packages (HDF5, for example, or python packages) can be built and installed into your virtual env. Here are two common examples that aren't currently part of the pytorch container that may be useful.</p>"},{"location":"sophia/data-science-workflows/building-python-packages/#hdf5","title":"HDF5","text":"<p>You can find the source code for HDF5 on their website https://www.hdfgroup.org/downloads/hdf5/source-code. When downloaded and un-tarred, cdto the directory and run: <pre><code>./configure --prefix=$VENV_LOCATION # Add any other configuration arguments \nmake -j 64 \nmake install\n</code></pre> This should get you HDF5! For example, after this: <pre><code>(pytorch_20.08) Singularity&gt; which h5cc \n/home/cadams/ThetaGPU/venvs/pytorch_20.08/bin/h5cc # This is my virtualenv, success!\n</code></pre></p>"},{"location":"sophia/data-science-workflows/building-python-packages/#horovod","title":"Horovod","text":"<p>Horovod is useful for distributed training. To use it, you need it enabled within the container. <pre><code>git clone https://github.com/horovod/horovod.git \ncd horovod \ngit submodule update --init \npython setup.py build \npython setup.py install\n</code></pre> This should install Horovod within your container.</p>"},{"location":"sophia/data-science-workflows/data-science-software-availability/","title":"Data Science Software Availability","text":"<p>On ThetaGPU, currently we support the major deep learning frameworks through two paths: Singularity containers, based off of NVIDIA's Docker containers, and through bare-metal source builds. The bare-metal builds are for TensorFlow 2.X PyTorch. TensorFlow 1.X is supported only via NVIDIA's containers at this time.</p>"},{"location":"sophia/data-science-workflows/data-science-software-availability/#containers","title":"Containers","text":"<p>As of now, the NVIDIA containers with TensorFlow 1, 2 and PyTorch built against <code>cuda11</code>, <code>cudnn8</code> are available in singularity format here: <pre><code>$ ls /lus/theta-fs0/projects/datascience/thetaGPU/containers/ \npytorch_20.08-py3.sif tf1_20.08-py3.sif tf2_20.08-py3.sif\n</code></pre></p> <p>Execute a container interactively like this: <pre><code>$ singularity exec --nv -B /lus:/lus /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf1_20.08-py3.sif bash\n</code></pre></p>"},{"location":"sophia/data-science-workflows/gpu-monitoring/","title":"GPU Monitoring","text":"<p>Each GPU on ThetaGPU hosts 8 A100 GPUs. You can see information about these GPUs via the command <code>nvidia-smi</code>.</p> <p>Each GPU has 40Gb of on-GPU memory. When you run applications, you will know the GPU is in use when you see the memory increase and the GPU Utilization will be non-zero.</p> <p>You can target a specific GPU with <code>nvidia-smi -i 0</code> for the first GPU, for example.</p>"},{"location":"sophia/data-science-workflows/gpu-monitoring/#gpu-selection","title":"GPU Selection","text":"<p>In many application codes, you may want to specifiy which GPU is used. This is particular important in node-sharing applications where each GPU is running it's own code, which can be either in data-parallel model training, workflow based throughput jobs, etc. You can control individual process launches with:</p> <pre><code># Specify to run only on GPU 4: \nexport CUDA_VISIBLE_DEVICES=4 \n\n# Let your application see GPUS 0, 1, and 7: \nexport CUDA_VISIBLE_DEVICES=\"0,1,7\"\n</code></pre> <p>In these cases, the GPU orderings will appear as a consecutive list starting with 0.</p> <p>From inside an application, many software frameworks have ability to let you target specific GPUs, including tensorflow and pytorch:</p> <ul> <li>TensorFlow</li> <li>PyTorch</li> </ul>"},{"location":"sophia/data-science-workflows/gpu-node-queue-and-policy/","title":"GPU Node Queue and Policy","text":"<p>Note: Users will need an allocation on ThetaGPU to utilize the GPU nodes. Request for an allocation by filling out this form: Allocation Request.</p> <p>ThetaGPU is listed under Theta on the form.</p> <p>The GPU nodes are new and we expect the workload to be significantly different than it is on the KNL nodes. This document describes the current state of affairs, but we will monitor usage and adjust the policies as necessary.</p>"},{"location":"sophia/data-science-workflows/gpu-node-queue-and-policy/#nodes-vs-queue-vs-mig-mode","title":"Nodes vs Queue vs MIG mode","text":"<p>The GPU nodes are NVidia DGX A100 nodes and each node contains eight (8) A100 GPUs.</p> <p>You may request either entire nodes, or individual GPUs based on your job needs. What you will get is determined by the queue you submit to: - If it has node in the name, you will get nodes. - If it has GPU in the name, you will get GPUs.</p> <p>Note: The <code>-n</code> parameter in your <code>qsub</code> will match the resource type in the queue (<code>-n 2</code> in node queue will get you two full nodes, <code>-n 2</code> in a GPU queue will get you two GPUs). </p> <p>Additionally, the Nvidia A100 GPUs support a feature called \u201cMulti-Instance GPU\u201d (MIG) mode. This allows a single GPU to be shared by up to 7 different processes. We do not schedule at this level, but you may pass <code>\u2013attrs mig-mode=True</code> in with your qsub and we will set the node to MIG mode and you may take advantage of it in your job script.</p>"},{"location":"sophia/data-science-workflows/gpu-node-queue-and-policy/#queues","title":"Queues","text":"<p>There will be two primary queues: - full-node: This is the general production queue for jobs that require full nodes. - single-gpu: This is the general production queue for jobs that operate best on individual GPUs.</p> <p>And two debug queues: - debug-node: Submit to this queue if you need an entire node for your testing (for instance you are utilizing the NVLink) - debug-gpu: Submit to this queue if you need GPUs.</p> <p>Initially, we are relaxing our node restrictions to encourage early users. Please be courteous to your fellow users and do not monopolize the machine. We will tighten restrictions as required to manage the demonstrated workload. </p> <p>Here are the initial queue limits: - MinTime is 5 minutes - MaxTime is 12 hours - MaxRunning will be 2 full nodes or 16 individual GPUs</p>"},{"location":"sophia/data-science-workflows/gpu-node-queue-and-policy/#queue-restrictions","title":"Queue Restrictions","text":"<ul> <li>MaxQueued will be 100 jobs</li> <li>You may have at most 1152 node-hours or 9216 GPU hours in the queue at any time.</li> <li>You may not violate either of these policies.</li> <li>You could not submit (1000) 1 node-hour jobs because that would violate the MaxQueued of 100 jobs, nor could you submit (2) 1000 node-hour jobs because that would violate the MaxNodeHours limit.</li> <li>The initial queue policy will be simple First-In-First-Out (FIFO) based on priority with EASY backfill.</li> </ul>"},{"location":"sophia/data-science-workflows/pythonc-code-interoperability/","title":"Python/C++ Code Interoperability","text":"<p>These are the steps to build code that has Python/C++ code interoperability. </p>"},{"location":"sophia/data-science-workflows/pythonc-code-interoperability/#login-to-a-thetagpu-head-node","title":"Login to a ThetaGPU head node","text":"<pre><code>ssh thetagpusn1\n</code></pre>"},{"location":"sophia/data-science-workflows/pythonc-code-interoperability/#1-request-an-interactive-session-on-an-a100-gpu","title":"1. Request an interactive session on an A100 GPU","text":"<pre><code>qsub -n 1 -q default -A datascience -I -t 1:00:00\n</code></pre> <p>Following this, we need to execute a few commands to get setup with an appropriately optimized TensorFlow. These are: 3. Activate the TensorFlow 2.2 Singularity container: <pre><code>singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf2_20.08-py3.sif bash\n</code></pre></p>"},{"location":"sophia/data-science-workflows/pythonc-code-interoperability/#2-setup-access-to-the-internet","title":"2. Setup access to the internet","text":"<pre><code>export HTTP_PROXY=http://theta-proxy.tmi.alcf.anl.gov:3128 \nexport HTTPS_PROXY=https://theta-proxy.tmi.alcf.anl.gov:3128\n</code></pre> <p>Now that we can access the internet, we need to set up a virtual environment in Python (these commands should only be run the first time). <pre><code>python -m pip install --user virtualenv \nexport VENV_LOCATION=/home/rmaulik/THETAGPU_TF_ENV # Add your path here \npython -m virtualenv --system-site-packages $VENV_LOCATION \nsource $VENV_LOCATION/bin/activate \npython -m pip install cmake \npython -m pip install matplotlib \npython -m pip install sklearn\n</code></pre></p> <p><code>cmake</code> is required to build our C++ app and link to Python, and other packages may be pip installed as needed in your Python code. An example <code>MakeLists.txt</code> file for building with Python/C interoperability with examples can be found here.</p>"},{"location":"sophia/data-science-workflows/containers/containers/","title":"Containers on Theta(GPU)","text":""},{"location":"sophia/data-science-workflows/containers/containers/#building-using-docker","title":"Building using Docker","text":"<p>If you followed the <code>Dockerfile</code> instructions, using the Theta(GPU) specific <code>Dockerfile_thetagpu</code> you can build your container for theta gpu using: <pre><code>singularity build &lt;image_name&gt; docker://&lt;username&gt;/&lt;repo_name&gt;:&lt;tag&gt;\n# using tutorial example\nsingularity build my_image.simg docker://jtchilders/alcf_cwp_example:thetagpu\n</code></pre></p> <p></p> <p>Then you can submit a job to Theta(GPU) using the job submission script</p> <pre><code>module load cobalt/cobalt-gpu\nqsub -A &lt;project-name&gt; job_submission_thetagpu.sh ./my_image.simg\n</code></pre> <p>The output should look like this: <pre><code>C++ MPI\nHello world from processor thetagpu12, rank 4 out of 16 processors\nHello world from processor thetagpu12, rank 7 out of 16 processors\nHello world from processor thetagpu12, rank 1 out of 16 processors\nHello world from processor thetagpu12, rank 5 out of 16 processors\nHello world from processor thetagpu12, rank 6 out of 16 processors\nHello world from processor thetagpu12, rank 0 out of 16 processors\nHello world from processor thetagpu12, rank 2 out of 16 processors\nHello world from processor thetagpu12, rank 3 out of 16 processors\nHello world from processor thetagpu18, rank 14 out of 16 processors\nHello world from processor thetagpu18, rank 15 out of 16 processors\nHello world from processor thetagpu18, rank 13 out of 16 processors\nHello world from processor thetagpu18, rank 8 out of 16 processors\nHello world from processor thetagpu18, rank 9 out of 16 processors\nHello world from processor thetagpu18, rank 11 out of 16 processors\nHello world from processor thetagpu18, rank 12 out of 16 processors\nHello world from processor thetagpu18, rank 10 out of 16 processors\nPython MPI\nHello world from processor thetagpu18, rank 13 out of 16 processors\nHello world from processor thetagpu18, rank 8 out of 16 processors\nHello world from processor thetagpu18, rank 9 out of 16 processors\nHello world from processor thetagpu18, rank 14 out of 16 processors\nHello world from processor thetagpu18, rank 15 out of 16 processors\nHello world from processor thetagpu18, rank 11 out of 16 processors\nHello world from processor thetagpu18, rank 10 out of 16 processors\nHello world from processor thetagpu18, rank 12 out of 16 processors\nHello world from processor thetagpu12, rank 2 out of 16 processors\nHello world from processor thetagpu12, rank 5 out of 16 processors\nHello world from processor thetagpu12, rank 0 out of 16 processors\nHello world from processor thetagpu12, rank 6 out of 16 processors\nHello world from processor thetagpu12, rank 4 out of 16 processors\nHello world from processor thetagpu12, rank 1 out of 16 processors\nHello world from processor thetagpu12, rank 7 out of 16 processors\nHello world from processor thetagpu12, rank 3 out of 16 processors\n</code></pre></p>"},{"location":"sophia/data-science-workflows/containers/containers/#building-using-singularity-recipes","title":"Building using Singularity Recipes","text":"<p>While building using Docker on your local machine tends to be the easier method. There are sometimes reasons to build in the environment of the supercomputer. In this case, one can build a singularity container on ThetaGPU in an interactive session on a compute (or worker) node. First a recipe file is needed, below is an example singularity definition file which can also be found here. </p> <p>Detailed directions for recipe construction are available on the Singularity Recipe Page.</p>"},{"location":"sophia/data-science-workflows/containers/containers/#example-singularity-definition-file","title":"Example Singularity definition file","text":"<p>Here we have defined the base image from which to <code>bootstrap</code> our container. We are using an image from Docker Hub, <code>ubuntu:20.04</code>.</p> <pre><code>Bootstrap: docker\nFrom: ubuntu:20.04\n</code></pre> <p>The <code>%files</code> section lists files to copy from the host system (left path) to the container filesystem (right path)prior to build time.</p> <pre><code>%files\n    ../Local/source/* /usr/source/\n    ../Local/submit.sh /usr/\n</code></pre> <p>The <code>%environment</code> section defines environment variables that will be available to the container at runtime.</p> <pre><code>%environment\n    export PATH=$PATH:/mpich/install/bin\n    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/mpich/install/lib\n</code></pre> <p>The <code>%post</code> section executes within the container at build time on top of our <code>ubuntu:20.04</code> operating system. The <code>%post</code> section is therefore the place to perform installations of custom apps with syntax similar to BASH.</p> <pre><code>%post\n    #### INSTALL BASE PACKAGES NEEDED FOR MPI APPLICATIONS AND PYTHON3 ####\n    DEBIAN_FRONTEND=noninteractive\n    apt-get update -y \\\n    &amp;&amp; DEBIAN_FRONTEND=noninteractive \\\n    &amp;&amp; apt-get install -y build-essential libfabric-dev libibverbs-dev gfortran wget \\\n    &amp;&amp; apt-get install -y python3 python3-distutils python3-pip gcc\n\n    #### DOWNLOAD AND INSTALL MPICH AND MPI4PY ####\n    # Source is available at http://www.mpich.org/static/downloads/\n    # See installation guide of target MPICH version\n    # Ex: https://www.mpich.org/static/downloads/4.0.2/mpich-4.0.2-installguide.pdf\n    # These options are passed to the steps below\n    OPENMPI_VERSION_A=\"4.0\"\n    OPENMPI_VERSION_B=\"4.0.5\"\n    OPENMPI_CONFIGURE_OPTIONS=\"--prefix=/openmpi/install --disable-wrapper-rpath --disable-wrapper-runpath\"\n    OPENMPI_MAKE_OPTIONS=\"-j\"\n    mkdir -p openmpi\n    cd /openmpi\n    wget https://download.open-mpi.org/release/open-mpi/v${OPENMPI_VERSION_A}/openmpi-${OPENMPI_VERSION_B}.tar.gz\n    tar xfz openmpi-${OPENMPI_VERSION_B}.tar.gz  --strip-components=1\n   ./configure ${OPENMPI_CONFIGURE_OPTIONS}\n   make install ${OPENMPI_MAKE_OPTIONS}\n\n    export PATH=$PATH:/openmpi/install/bin\n   export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/openmpi/install/lib\n\n    pip install mpi4py\n\n    #### BUILD FILES ####\n    chmod +x /usr/submit.sh\n    mpicc -o /usr/source/mpi_hello_world /usr/source/mpi_hello_world.c\n</code></pre> <p>The <code>%runscript</code> section defines actions for the container to take when it is executed using <code>singularity run &lt;container_name&gt;</code>.</p> <pre><code>%runscript\n    exec /usr/submit.sh \"$@\"\n</code></pre> <p>The <code>%labels</code> section allows for custom metadata to be added to the container.</p> <pre><code>%labels\n        MAINTAINER Aditya atanikanti@anl.gov\n</code></pre> <p>The <code>%help</code> section can be used to define how to build and run the container.</p> <pre><code>%help\n        This is container is used to illustrate a mpi based def file to build a container running python and c programs. To build the container use singularity build --fakeroot mpi.sif mpi.def\n</code></pre>"},{"location":"sophia/data-science-workflows/containers/containers/#build-singularity-container-on-thetagpu-compute","title":"Build Singularity container on ThetaGPU compute","text":"<p>After logging on to Theta login nodes, launch an interactive job using the attrs <code>fakeroot=true</code>, <code>pubnet=true</code> and specifying the filesystems <code>filesystems=home,theta-fs0</code>.</p> <pre><code># on Theta login node, must load cobalt-gpu module to submit jobs to ThetaGPU\nmodule load cobalt/cobalt-gpu\nqsub -I -n 1 -t 01:00:00 -q single-gpu -A &lt;project_name&gt; --attrs fakeroot=true:pubnet=true:filesystems=home,theta-fs0\n</code></pre> <p>Before building the container make sure the ThetaGPU compute nodes have access to external resources, this is achieved by setting the <code>http_proxy</code> and <code>https_proxy</code> variables <pre><code># setup network proxy to reach outside world\nexport http_proxy=http://proxy.tmi.alcf.anl.gov:3128\nexport https_proxy=http://proxy.tmi.alcf.anl.gov:3128\n</code></pre></p> <p>Now build the container using <code>--fakeroot</code> where <code>&lt;def_filename&gt;.def</code> is the definition file we have defined in the example above and <code>&lt;image_name&gt;.sif</code> is the user defined image file name Using mpi.def example <pre><code># important you run this in the proper path because the file copies in\n# the `%files` section of the recipe uses relative paths on the host.\ncd \nsingularity build --fakeroot &lt;image_name&gt;.sif &lt;def_filename&gt;.def \n</code></pre></p>"},{"location":"sophia/data-science-workflows/containers/containers/#run-singularity-container-on-thetagpu-compute","title":"Run Singularity container on ThetaGPU compute","text":"<p>An example job submission script is here: job_submission_thetagpu.sh.</p> <p>First we define our job and our script takes the container name as an input parameter.</p> <pre><code>#!/bin/bash -l\n#COBALT -n 1\n#COBALT -t 00:10:00\n#COBALT -q single-gpu\n#COBALT --attrs filesystems=home,theta-fs0:pubnet=true\nCONTAINER=$1\n</code></pre> <p>Enable network access at run time by setting the proxy.</p> <pre><code>export http_proxy=http://proxy.tmi.alcf.anl.gov:3128\nexport https_proxy=http://proxy.tmi.alcf.anl.gov:3128\n</code></pre> <p>Setup our MPI settings, figure out number of nodes <code>NODES</code> and fix number of process per node <code>PPN</code> and multiply to get total MPI ranks <code>PROCS</code>.</p> <pre><code>NODES=`cat $COBALT_NODEFILE | wc -l`\nPPN=8 # GPUs per NODE\nPROCS=$((NODES * PPN))\necho NODES=$NODES  PPN=$PPN  PROCS=$PROCS\n</code></pre> <p>The OpenMPI installed on ThetaGPU must be used for MPI to properly run across nodes. Here the library path is added to <code>SINGULARITYENV_LD_LIBRARY_PATH</code>, which will be used by Singularity to set the container's <code>LD_LIBRARY_PATH</code> and therefore tell our executables where to find the MPI libraries.</p> <pre><code>MPI_BASE=/lus/theta-fs0/software/thetagpu/openmpi-4.0.5/\nexport LD_LIBRARY_PATH=$MPI_BASE/lib:$LD_LIBRARY_PATH\nexport SINGULARITYENV_LD_LIBRARY_PATH=$LD_LIBRARY_PATH\necho mpirun=$(which mpirun)\n</code></pre> <p>Finally the exectuable is launched. Notice on NVidia systems that the <code>singularity exec</code> or <code>singularity run</code> commands must use the <code>--nv</code> flag to pass important libraries/drivers from the host to the container environment.</p> <p><pre><code>mpirun -hostfile $COBALT_NODEFILE -n $PROCS -npernode $PPN singularity exec --nv -B $MPI_BASE $CONTAINER /usr/source/mpi_hello_world\n</code></pre> The job can be submitted using: <pre><code>qsub -A &lt;project-name&gt; job_submission_thetagpu.sh /path/to/my_image.sif\n</code></pre></p> <p>The output should look like this: <pre><code>C++ MPI\nHello world from processor thetagpu02, rank 12 out of 16 processors\nHello world from processor thetagpu02, rank 8 out of 16 processors\nHello world from processor thetagpu02, rank 10 out of 16 processors\nHello world from processor thetagpu02, rank 11 out of 16 processors\nHello world from processor thetagpu02, rank 13 out of 16 processors\nHello world from processor thetagpu02, rank 9 out of 16 processors\nHello world from processor thetagpu02, rank 14 out of 16 processors\nHello world from processor thetagpu02, rank 15 out of 16 processors\nHello world from processor thetagpu01, rank 0 out of 16 processors\nHello world from processor thetagpu01, rank 1 out of 16 processors\nHello world from processor thetagpu01, rank 2 out of 16 processors\nHello world from processor thetagpu01, rank 3 out of 16 processors\nHello world from processor thetagpu01, rank 4 out of 16 processors\nHello world from processor thetagpu01, rank 5 out of 16 processors\nHello world from processor thetagpu01, rank 6 out of 16 processors\nHello world from processor thetagpu01, rank 7 out of 16 processors\nPython MPI\nHello world from processor thetagpu02, rank 9 out of 16 processors\nHello world from processor thetagpu02, rank 10 out of 16 processors\nHello world from processor thetagpu02, rank 11 out of 16 processors\nHello world from processor thetagpu02, rank 15 out of 16 processors\nHello world from processor thetagpu02, rank 13 out of 16 processors\nHello world from processor thetagpu02, rank 8 out of 16 processors\nHello world from processor thetagpu02, rank 12 out of 16 processors\nHello world from processor thetagpu02, rank 14 out of 16 processors\nHello world from processor thetagpu01, rank 7 out of 16 processors\nHello world from processor thetagpu01, rank 3 out of 16 processors\nHello world from processor thetagpu01, rank 1 out of 16 processors\nHello world from processor thetagpu01, rank 4 out of 16 processors\nHello world from processor thetagpu01, rank 5 out of 16 processors\nHello world from processor thetagpu01, rank 6 out of 16 processors\nHello world from processor thetagpu01, rank 0 out of 16 processors\nHello world from processor thetagpu01, rank 2 out of 16 processors\n</code></pre></p>"},{"location":"sophia/data-science-workflows/containers/containers/#pre-existing-images-for-deep-learning","title":"Pre-existing Images for Deep Learning","text":"<p>There are several containers on ThetaGPU that will help you get started with deep learning experiments that can efficiently use the A100 GPUs. We have different optimized container for DL here <code>ls /lus/theta-fs0/software/thetagpu/nvidia-containers/</code></p> <p>The bootstap.def gives an example of how these containers were created.</p> <p>The image is bootstrapped from an NVidia image, in this case from a PyTorch build. One can also use the TensorFlow build. At the time of this writing, the latest tag for the PyTorch image was <code>22.04-py3</code>, but users should select the version that best suits their needs.</p> <p><pre><code>Bootstrap: docker\nFrom: nvcr.io/nvidia/pytorch:22.04-py3\n</code></pre> Next we need to install MPI support for cross-node parallel training.</p> <p><pre><code>%post\n\n    # Install mpi4py\n    CC=$(which mpicc) CXX=$(which mpicxx) pip install --no-cache-dir mpi4py\n\n    # Install horovod\n    CC=$(which mpicc) CXX=$(which mpicxx) HOROVOD_WITH_TORCH=1 pip install --no-cache-dir horovod\n</code></pre> Next build the container on a ThetaGPU compute node, following the instructions in the previous section. Then an example job submission script is here: job_submission_thetagpudl.sh.</p>"},{"location":"sophia/data-science-workflows/dl-frameworks/deepspeed/","title":"DeepSpeed","text":"<p>The base <code>conda</code> environment on ThetaGPU comes with Microsoft's DeepSpeed pre-installed. Instructions for using / cloning the base environment can be found here.</p> <p>We describe below the steps needed to get started with DeepSpeed on ThetaGPU.</p> <p>We focus on the <code>cifar</code> example provided in the DeepSpeedExamples repository, though this approach should be generally applicable for running any model with DeepSpeed support.</p>"},{"location":"sophia/data-science-workflows/dl-frameworks/deepspeed/#running-deepspeed-on-thetagpu","title":"Running DeepSpeed on ThetaGPU","text":"<p>Note</p> <p>The instructions below should be ran directly from a compute node. Explicitly, to request an interactive job (from <code>thetalogin</code>):</p> <pre><code>qsub-gpu -A &lt;project&gt; -n 2 -t 01:00 -q full-node \\\n    --attrs=\"filesystems=home,grand,eagle,theta-fs0:ssds=required\" \\\n    -I\n</code></pre> <p>Refer to GPU Node Queue and Policy.</p> <ol> <li> <p>Load <code>conda</code> module and activate base environment:     <pre><code>module load conda ; conda activate base\n</code></pre></p> </li> <li> <p>Clone    microsoft/DeepSpeedExamples    and navigate into the directory:     <pre><code>git clone https://github.com/microsoft/DeepSpeedExamples.git\ncd DeepSpeedExamples/cifar\n</code></pre></p> </li> <li> <p>Our newer conda environments should come with DeepSpeed pre-installed, but    in the event your environment has no <code>deepspeed</code>, it can be    installed<sup>2</sup> with <code>pip</code>:     <pre><code>$ which deepspeed\ndeepspeed not found\n$ python3 -m pip install --upgrade pip setuptools wheel\n$ DS_BUILD_OPS=1 python3 -m pip install \n</code></pre></p> </li> </ol> <p>Launching DeepSpeed</p> Launching with OpenMPILaunching with DeepSpeed <ol> <li> <p>Get total number of available GPUs:</p> <ol> <li>Count number of lines in <code>$COBALT_NODEFILE</code> (1 host per line)</li> <li>Count number of GPUs available on current host</li> <li><code>NGPUS = $((${NHOSTS}*${NGPU_PER_HOST}))</code> <pre><code>NHOSTS=$(wc -l &lt; \"${COBALT_NODEFILE}\")\nNGPU_PER_HOST=$(nvidia-smi -L | wc -l)\nNGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"\n</code></pre></li> </ol> </li> <li> <p>Launch with <code>mpirun</code><sup>1</sup>: <pre><code>mpirun \\\n    -n \"${NGPUS}\" \\\n    -npernode \"${NGPU_PER_HOST}\" \\\n    --hostfile \"${COBALT_NODEFILE}\" \\\n    -x PATH \\\n    -x LD_LIBRARY_PATH \\\n    -x PYTHONUSERBASE \\\n    -x http_proxy \\\n    -x https_proxy\n    python3 cifar10_deepspeed.py \\\n        --deepspeed_config ds_config-1.json\n</code></pre></p> </li> </ol> <ol> <li> <p>Create a DeepSpeed compliant <code>hostfile</code>, specifying the hostname and    number of GPUs (<code>slots</code>) for each of our available workers: <pre><code>cat $COBALT_NODEFILE &gt; hostfile\nsed -e 's/$/ slots=4/' -i hostfile\n</code></pre></p> </li> <li> <p>Create a <code>.deepspeed_env</code> containing the environment variables our    workers will need access to: <pre><code>echo \"PATH=${PATH}\" &gt;&gt; .deepspeed_env\necho \"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\" &gt;&gt; .deepspeed_env\necho \"http_proxy=${http_proxy}\" &gt;&gt; .deepspeed_env\necho \"https_proxy=${https_proxy}\" &gt;&gt; .deepspeed_env\n</code></pre></p> </li> </ol> <p>Warning</p> <p>The <code>.deepspeed_env</code> file expects each line to be of the form <code>KEY=VALUE</code>. Each of these will then be set as environment variables on each available worker specified in our <code>hostfile</code>.</p> <p>We can then run the <code>cifar10_deepspeed.py</code> module using DeepSpeed: Launch with DeepSpeed<pre><code>deepspeed --hostfile=hostfile cifar10_deepspeed.py \\\n    --deepspeed \\\n    --deepspeed_config ds_config.json\n</code></pre></p> <code>AssertionError: Micro batch sizer per gpu: 0 has to be greater than 0</code> <p>Depending on the details of your specific job, it may be necessary to modify the provided <code>ds_config.json</code>.</p> <p>If you encounter an error: <pre><code>thetagpu23: AssertionError: Micro batch size per gpu: 0 has to be greater than 0\n</code></pre> you can modify the <code>\"train_batch_size\": 16</code> variable in the provided <code>ds_config.json</code> to the (total) number of available GPUs, and explicitly set <code>\"gradient_accumulation_steps\": 1</code>, as shown below. <pre><code>$ export NHOSTS=$(wc -l &lt; \"${COBALT_NODEFILE}\")\n$ export NGPU_PER_HOST=$(nvidia-smi -L | wc -l)\n$ export NGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"\n$ echo $NHOSTS $NGPU_PER_HOST $NGPUS\n2 8 16\n$ # replace \"train_batch_size\" with $NGPUS in ds_config.json\n$ # and write to `ds_config-polaris.json`\n$ sed \\\n    \"s/$(cat ds_config.json| grep batch | cut -d ':' -f 2)/ ${NGPUS},/\" \\\n    ds_config.json \\\n    &gt; ds_config-polaris.json\n$ cat ds_config-polaris.json\n{\n    \"train_batch_size\": 16,\n    \"gradient_accumulation_steps\": 1,\n    ...\n}\n</code></pre></p> <ol> <li> <p>The flag <code>-x ENVIRONMENT_VARIABLE</code> ensures the <code>$ENVIRONMENT_VARIABLE</code> will be set in the launched processes.\u00a0\u21a9</p> </li> <li> <p>Additional details for installing DeepSpeed can be found int their docs   from: Installation   Details \u21a9</p> </li> </ol>"},{"location":"sophia/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/","title":"Distributed Training on ThetaGPU Using Data Parallelism","text":"<p>There are two schemes for distributed learning:</p> <ol> <li> <p>Model parallelization: in this scheme, disjoint subsets of a neural network are assigned to different devices. Therefore, all the computations associated to the subsets are distributed. Communication happens between devices whenever there is dataflow between two subsets. Model parallelization is suitable when the model is too large to be fitted into a single device (CPU/GPU) because of the memory capacity. However, partitioning the model into different subsets is not an easy task, and there might potentially introduce load imbalance issues limiting the scaling efficiency.\u202f </p> </li> <li> <p>Data parallelization: in this scheme, all the workers own a replica of the model. The global batch of data is split into multiple minibatches,\u202fand processed by different workers. Each worker computes the corresponding loss and gradients with respect to the data it posseses. Before the updating of the parameters at each epoch, the loss and gradients are averaged among all the workers through a collective operation. This scheme is relatively simple to implement. MPI_Allreduce is the only commu</p> </li> </ol> <p>Our recent presentation about the data parallel training can be found here: https://youtu.be/930yrXjNkgM</p> <p>In this documentation, we would like to show how to do data parallel training on ThetaGPU. </p>"},{"location":"sophia/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#software-environment-setup","title":"Software environment setup","text":"<p>We are still in the process of setting up the software stacks on ThetaGPU. Currently, one can get TensorFlow, PyTorch, and Horovod with the following setup script. <pre><code>source /lus/theta-fs0/software/datascience/thetagpu/anaconda3/setup.sh\n</code></pre></p>"},{"location":"sophia/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#tensorflow-with-horovod","title":"TensorFlow with Horovod","text":""},{"location":"sophia/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#1-initialize-horovod","title":"1. Initialize Horovod","text":"<p><pre><code>import horovod.tensorflow as hvd hvd.init()\n</code></pre> After this initialization, the rank ID and the number of processes can be refered as <code>hvd.rank()</code> and <code>hvd.size()</code>. Besides, one can also call <code>hvd.local_rank()</code> to get the local rank ID within a node. This is useful when we are trying to assign GPUs to each rank.</p>"},{"location":"sophia/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#2-assign-gpu-to-each-rank","title":"2. Assign GPU to each rank","text":"<p><pre><code>gpus = tf.config.experimental.list_physical_devices('GPU') \nfor gpu in gpus: \n    tf.config.experimental.set_memory_growth(gpu, True) \nif gpus: \n    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n</code></pre> In this case, we set one GPU per process: <code>ID=hvd.local_rank()</code></p>"},{"location":"sophia/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#3-scale-the-learning-rate","title":"3. Scale the learning rate","text":"<p>Typically, since we use multiple workers, the global batch is usually increases n times (n is the number of workers). The learning rate should increase proportionally as follows (assuming that the learning rate initially is 0.01). <pre><code>opt = tf.train.AdagradOptimizer(0.01*hvd.size())\n</code></pre></p>"},{"location":"sophia/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#4-wrap-the-optimizer-with-distributed-optimizer","title":"4. Wrap the optimizer with Distributed Optimizer","text":"<pre><code>opt = hvd.DistributedOptimizer(opt)\n</code></pre>"},{"location":"sophia/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#5-broadcast-the-model-from-rank-0","title":"5. Broadcast the model from rank 0","text":"<p>This is to make sure that all the workers will have the same starting point. <pre><code>hooks = [hvd.BroadcastGlobalVariablesHook(0)]\n</code></pre></p>"},{"location":"sophia/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#6-loading-data-according-to-rank-id","title":"6. Loading data according to rank ID","text":"<p>TensorFlow has some functions for parallel distribution of data. But for specific applications, the user might have to write their own data loader. </p> <p>In general, one has two ways to deal with the data loading: </p> <ol> <li> <p>Each worker randomly select one batch of data from the dataset at each step. In such case, each worker can see the entire dataset. It is important to make sure that the different worker have different random seeds so that they will get different data at each step.</p> </li> <li> <p>Each worker accesses a subset of dataset. One manually partition the entire dataset into different partions, and each rank access one of the partions. </p> </li> </ol> <p>In both cases, the total number of steps per epoch is <code>nsamples / hvd.size()</code>.</p>"},{"location":"sophia/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#7-checkpointing-on-root-rank","title":"7. Checkpointing on root rank","text":"<p>It is important to let only one process to do the checkpointing I/O lest perhaps the file been corrupted. </p> <pre><code>if hvd.rank() == 0: \n   checkpoint.save(checkpoint_dir)\n</code></pre>"},{"location":"sophia/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#8-average-metric-across-all-the-workers","title":"8. Average metric across all the workers","text":"<p>Notice that in the distributed training, any tensor are local to each worker. In order to get the global averaged value, one can use Horovod allreduce. Below is an example on how to do the average.</p> <p><pre><code>def tensor_average(val, name): \n     tensor = torch.tensor(val) \n     if (with_hvd): \n         avg_tensor = hvd.allreduce(tensor, name=name) \n     else: \n         avg_tensor = tensor \n   return avg_tensor.item()\n</code></pre> We provided some examples in: https://github.com/argonne-lcf/sdl_ai_workshop/blob/master/01_distributedDeepLearning/Horovod/tensorflow2_mnist.py</p>"},{"location":"sophia/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#pytorch-with-ddp","title":"PyTorch with DDP","text":"<p>PyTorch has its own native parallelization library called DDP. We will provide more details on how to run this on ThetaGPU. The current PyTorch on ThetaGPU does not have DDP built in. We will update to our users once we have DDP. </p> <p>For now, please refer to https://pytorch.org/tutorials/intermediate/ddp_tutorial.html</p>"},{"location":"sophia/data-science-workflows/dl-frameworks/distributed-training-using-data-parallelism/#mpi-profiling-for-data-parallel-training","title":"MPI Profiling for data parallel training","text":"<p>We support two ways for profling the performance of data parallel training. </p> <ol> <li>mpitrace library MPI trace allows us to get a flat profiling of all the MPI function calls involved during the training. To enable this, one can set the environment variable <pre><code>export LD_PRELOAD=/lus/theta-fs0/software/datascience/thetagpu/hpctw/lib/libmpitrace.so\n</code></pre> Then run the application as usual. MPI profiling results will be generated after the run finishes <code>mpi_profile.XXXX.[rank_id]</code>.</li> </ol> <p>Below is an example output: <pre><code>Data for MPI rank 0 of 8: \nTimes and statistics from MPI_Init() to MPI_Finalize(). \n----------------------------------------------------------------------- \nMPI Routine #calls avg. bytes time(sec) \n----------------------------------------------------------------------- \nMPI_Comm_rank 3 0.0 0.000 \nMPI_Comm_size 3 0.0 0.000 \nMPI_Bcast 520 197140.6 0.518 \nMPI_Allreduce 24561 208138.3 162.080 \nMPI_Gather 126 4.0 0.363 \nMPI_Gatherv 126 0.0 0.434 \nMPI_Allgather 2 4.0 0.000 \n----------------------------------------------------------------- \nMPI task 0 of 8 had the maximum communication time. \ntotal communication time = 163.396 seconds. \ntotal elapsed time = 187.298 seconds. \nuser cpu time = 4127.728 seconds. \nsystem time = 728.100 seconds. \nmax resident set size = 8403.938 MBytes. \n\nRank 0 reported the largest memory utilization : 8403.94 MBytes \nRank 0 reported the largest elapsed time : 187.30 sec \n----------------------------------------------------------------- \nMessage size distributions: \n                       MPI_Bcast      #calls   avg. bytes       time(sec) \n                                         126          4.0          0.008 \n                                           1          8.0          0.000 \n                                         121         25.0          0.006 \n                                          30        251.5          0.002 \n                                          32        512.0          0.002 \n                                          64       1024.0          0.005 \n                                          44       2048.0          0.003 \n                                          29       4092.8          0.003 \n                                          16       8192.0          0.032 \n\n                       MPI_Allreduce   #calls  avg. bytes       time(sec) \n                                        19780         8.0         90.822 \n                                         4576        24.0         18.239 \n                                           43      4004.0          0.295 \n                                            5   2780979.2          0.469 \n                                           50   8160289.2         20.893 \n                                            9  11803392.0          0.964 \n                                           48  28060640.0          3.293 \n                                           50  64731668.5         27.105 \n\n                       MPI_Gather      #calls   avg. bytes      time(sec) \n                                          126         4.0          0.363\n</code></pre> The useful information here is the message size distribution. </p> <ol> <li>Horovod Timeline To perform Horovod timeline analysis, one has to set the environment variable HOROVOD_TIMELINE which specifies the file for the output. export HOROVOD_TIMELINE=timeline.json This file is only recorded on rank 0, but it contains information about activity of all workers. You can then open the timeline file using the chrome://tracing facility of the Chrome browser.</li> </ol> <p>More details: https://horovod.readthedocs.io/en/stable/timeline_include.html</p>"},{"location":"sophia/data-science-workflows/dl-frameworks/running-pytorch-conda/","title":"Running PyTorch with Conda","text":"<p>Be aware that these builds use CUDA and will not work on login nodes, which do not have CUDA installed as there are no GPUs.</p> <p>One can test these software packages in an interactive session: <pre><code>qsub -I -q single-gpu -n 1 -t 30 -A &lt;project-name&gt; --attrs filesystems=&lt;list of filesystems&gt;\n</code></pre></p>"},{"location":"sophia/data-science-workflows/dl-frameworks/running-pytorch-conda/#pytorch-master-build","title":"PyTorch (master build)","text":"<p>Users can find the latest builds via the <code>module avail</code> conda command, which will list available builds such as <code>conda/2021-06-26</code> which is a module that was built on 2021-06-26. Use <code>module show conda/2021-06-26</code> or <code>module help conda/2021-06-26</code> to get high level info on which versions of the key packages and libraries that this particular module contains. </p> <p>This version can be used by: <pre><code>module load conda/2021-06-26 # loads conda into your environment, sets up appropriate CUDA libraries and environment variables\nconda activate # add entries to PATH for the environment and run any activation scripts that the environment may contain\n</code></pre></p> <p>This will setup a conda environment with the \"from scratch\" build of PyTorch.</p> <p>This package will also include builds of TensorFlow and Horovod tagged releases.</p>"},{"location":"sophia/data-science-workflows/dl-frameworks/running-pytorch-conda/#installing-packages","title":"Installing Packages","text":""},{"location":"sophia/data-science-workflows/dl-frameworks/running-pytorch-conda/#using-pip-install-user-not-recommended","title":"Using <code>pip install --user</code> (not recommended)","text":"<p>With the conda environment setup, one can install common Python modules using <code>pip install --users &lt;module-name&gt;</code> which will install packages in <code>$PYTHONUSERBASE/lib/pythonX.Y/site-packages</code>. The <code>$PYTHONUSERBASE</code> environment variable is automatically set when you load the base conda module, and is typically equal to <code>/home/$USER/.local/conda/YYYY-MM-DD</code> or  <code>/home/$USER/.local/thetagpu/conda/YYYY-MM-DD</code>, depending on the date of the module. </p> <p>Note, Python modules installed this way that contain command line binaries will not have those binaries automatically added to the shell's <code>$PATH</code>. To manually add the path: <pre><code>export PATH=$PYTHONUSERBASE/bin:$PATH\"\n</code></pre> Be sure to remove this location from <code>$PATH</code> if you deactivate the base Anaconda environment or unload the module. </p> <p>Cloning the Anaconda environment, or using <code>venv</code> are both more flexible and transparent when compared to <code>--user</code> installs. </p>"},{"location":"sophia/data-science-workflows/dl-frameworks/running-pytorch-conda/#using-conda-environments","title":"Using Conda Environments","text":"<p>If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via <code>conda install &lt;module&gt;</code> or <code>pip install &lt;module&gt;</code>.</p> <ol> <li>Setup the conda environment you want to use as instructed above.</li> <li>Create/edit your <code>$HOME/.condarc</code> file to include this these lines, replacing <code>&lt;project-name&gt;</code> with your project name. <code>&lt;path-to-your-project&gt;</code> is the path to the file system your project is on (e.g. <code>/lus/theta-fs0</code> or <code>/grand</code>or <code>/eagle</code>). By default, Conda will your <code>$HOME/.conda/*</code> area for caching files. </li> </ol> <p>Note: Since home directories are limited to 100GB, this fills up quickly. This addition tells Conda to use your project space for cache storage instead.</p> <pre><code>pkgs_dirs: \n  - &lt;path-to-your-project&gt;/&lt;project-name&gt;/conda/pkgs \nenvs_dirs: \n  - &lt;path-to-your-project&gt;/&lt;project-name&gt;/conda/envs\n</code></pre> <ol> <li>Clone the environment into a local path to which you have write access <pre><code>conda create --clone $CONDA_PREFIX -p &lt;path/to/env&gt;\n</code></pre></li> <li>Activate that environment: <pre><code>conda activate &lt;path/to/env&gt;\n</code></pre></li> </ol> <p>One should then be able to install modules natively.</p>"},{"location":"sophia/data-science-workflows/dl-frameworks/running-tensorflow-conda/","title":"Running TensorFlow with Conda","text":"<p>Be aware that these builds use CUDA and will not work on login nodes, which do not have CUDA installed as there are no GPUs.</p> <p>One can test these software packages in an interactive session: <pre><code>qsub -I -q single-gpu -n 1 -t 30 -A &lt;project-name&gt; --attrs filesystems=&lt;list of filesystems&gt;\n</code></pre></p>"},{"location":"sophia/data-science-workflows/dl-frameworks/running-tensorflow-conda/#tensorflow-master-build","title":"TensorFlow (master build)","text":"<p>Users can find the latest builds via the <code>module avail</code> conda command, which will list available builds such as <code>conda/2021-06-26</code> which is a module that was built on 2021-06-26. Use <code>module show conda/2021-06-26</code> or <code>module help conda/2021-06-26</code> to get high level info on which versions of the key packages and libraries that this particular module contains. </p> <p>This version can be used by: <pre><code>module load conda/2021-06-26 # loads conda into your environment, sets up appropriate CUDA libraries and environment variables\nconda activate # add entries to PATH for the environment and run any activation scripts that the environment may contain\n</code></pre> This will setup a conda environment with the \"from scratch\" build of TensorFlow.</p> <p>This package will also include builds of PyTorch and Horovod tagged releases.</p>"},{"location":"sophia/data-science-workflows/dl-frameworks/running-tensorflow-conda/#installing-packages","title":"Installing Packages","text":""},{"location":"sophia/data-science-workflows/dl-frameworks/running-tensorflow-conda/#using-pip-install-user-not-recommended","title":"Using <code>pip install --user</code> (not recommended)","text":"<p>With the conda environment setup, one can install common Python modules using <code>pip install --users &lt;module-name&gt;</code> which will install packages in <code>$PYTHONUSERBASE/lib/pythonX.Y/site-packages</code>. The <code>$PYTHONUSERBASE</code> environment variable is automatically set when you load the base conda module, and is typically equal to <code>/home/$USER/.local/conda/YYYY-MM-DD</code> or  <code>/home/$USER/.local/thetagpu/conda/YYYY-MM-DD</code>, depending on the date of the module. </p> <p>Note, Python modules installed this way that contain command line binaries will not have those binaries automatically added to the shell's <code>$PATH</code>. To manually add the path: <pre><code>export PATH=$PYTHONUSERBASE/bin:$PATH\"\n</code></pre> Be sure to remove this location from <code>$PATH</code> if you deactivate the base Anaconda environment or unload the module. </p> <p>Cloning the Anaconda environment, or using <code>venv</code> are both more flexible and transparent when compared to <code>--user</code> installs. </p>"},{"location":"sophia/data-science-workflows/dl-frameworks/running-tensorflow-conda/#using-conda-environments","title":"Using Conda Environments","text":"<p>If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via <code>conda install &lt;module&gt;</code> or <code>pip install &lt;module&gt;</code>.</p> <ol> <li> <p>Setup the conda environment you want to use as instructed above.</p> </li> <li> <p>Create/edit your <code>$HOME/.condarc</code> file to include this these lines, replacing <code>&lt;project-name&gt;</code> with your project name. <code>&lt;path-to-your-project&gt;</code> is the path to the file system your project is on (e.g. <code>/lus/theta-fs0</code> or <code>/grand</code>or <code>/eagle</code>). By default, Conda will your <code>$HOME/.conda/*</code> area for caching files. </p> </li> </ol> <p>Note: Since home directories are limited to 100GB, this fills up quickly. This addition tells Conda to use your project space for cache storage instead.</p> <p><pre><code>pkgs_dirs: \n   - /lus/theta-fs0/projects/&lt;project-name&gt;/conda/pkgs \nenvs_dirs: \n   - /lus/theta-fs0/projects/&lt;project-name&gt;/conda/envs\n</code></pre> 3. Clone the environment into a local path to which you have write access. <pre><code>conda create --clone $CONDA_PREFIX -p &lt;path/to/env&gt;\n</code></pre> 4. Activate that environment. <pre><code>conda activate &lt;path/to/env&gt;\n</code></pre></p> <p>One should then be able to install modules natively.</p>"},{"location":"sophia/data-science-workflows/dl-frameworks/tensorboard-instructions/","title":"TensorBoard Instructions","text":"<p>If you are able to install TensorBoard on your local machine, it is often easiest to copy the requisite files from ALCF file systems (via <code>sftp</code>, <code>scp</code>, Globus, etc.) to your local machine and run a TensorBoard there. </p> <p>However, if that is not possible, or if you have many and/or large files that TensorBoard needs to process located on ALCF file systems, there are several ways to run a TensorBoard server remotely. </p>"},{"location":"sophia/data-science-workflows/dl-frameworks/tensorboard-instructions/#tensorboard-server-on-a-thetagpu-compute-node","title":"TensorBoard server on a ThetaGPU compute node","text":"<p>This approach can be useful to have TensorBoard analyze live training progress. After you have logged into ThetaGPU, and have an interactive job running, you'll need to know the name of one of your worker nodes so you can SSH to it. <pre><code>PORT0=9991 \nPORT1=9992 \nPORT3=9993 \n# Select a theta login node N where N=[1-6] ssh -L $PORT0:localhost:$PORT1 $USER@thetaloginN.alcf.anl.gov \n\n# after reaching thetaloginN \n\n# Replace NN with your thetagpu worker node ssh -L $PORT1:thetagpuNN:$PORT3 $USER@thetagpusn1 \n# after reaching thetagpusn1 \n\n# login to worker node \nssh thetagpuNN \n\n# now setup your tensorflow environment \n# for instance run the conda setup.sh script created during the install_tensorflow.sh script \n\n# now run tensorboard \ntensorboard --logdir &lt;/path/to/logs&gt; --port $PORT3 --bind_all\n</code></pre></p>"},{"location":"sophia/data-science-workflows/dl-frameworks/tensorboard-instructions/#tensorboard-server-on-a-thetaknl-login-node","title":"TensorBoard server on a ThetaKNL login node","text":"<p>If you do not require the use of a GPU during analysis while TensorBoard runs, and you do not require a cutting-edge version of TensorBoard (this will load version 2.6.0), you can avoid additional SSH tunnel hops by running the TensorBoard server on a ThetaKNL login node: <pre><code>ssh -D &lt;some-port-number&gt;  theta.alcf.anl.gov\n\nmodule load conda/2021-09-22\nexport LD_LIBRARY_PATH=/soft/thetagpu/cuda/cuda_11.3.0_465.19.01_linux/lib64:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/soft/thetagpu/cuda/cuda_11.3.0_465.19.01_linux/extras/CUPTI/lib64/:$LD_LIBRARY_PATH\n</code></pre></p>"},{"location":"sophia/hardware-overview/theta-gpu-machine-overview/","title":"ThetaGPU Machine Overview","text":"<p>ThetaGPU is an extension of Theta and is comprised of 24 NVIDIA DGX A100 nodes. Each DGX A100 node comprises eight NVIDIA A100 Tensor Core GPUs and two AMD Rome CPUs that provide 22 with 320 GB of GPU memory and two nodes with 640 GB of GPU memory (8320 GB aggregately) of GPU memory for training artificial intelligence (AI) datasets, while also enabling GPU-specific and -enhanced high-performance computing (HPC) applications for modeling and simulation</p> <p>The DGX A100\u2019s integration into Theta is achieved via the ALCF\u2019s Cobalt HPC scheduler and shared access to a 10-petabyte Lustre filesystem. Fixed ALCF user accounts ensure a smooth onboarding process for the expanded system.</p> <p>A 15-terabyte solid-state drive offers up to 25 gigabits per second in bandwidth. The dedicated compute fabric comprises 20 Mellanox QM9700 HDR200 40-port switches wired in a fat-tree topology. ThetaGPU cannot utilize the Aries interconnect.</p>"},{"location":"sophia/hardware-overview/theta-gpu-machine-overview/#table-1-summarizes-the-capabilities-of-a-thetagpu-compute-node","title":"Table 1 summarizes the capabilities of a ThetaGPU compute node.","text":"COMPONENT COMPONENT AGGREGATE AMD Rome 64-core CPU 2 48 DDR4 Memory 1 TB on 320 GB &amp; 2 TB on 640 GB 26 TB NVIDIA A100 GPU 8 192 GPU Memory 22 nodes w/ 320 GB &amp; 2 nodes w/ 640 GB 8,320 GB HDR200 Compute Ports 8 192 HDR200 Storage Ports 2 48 100GbE Ports 2 48 3.84 TB Gen4 NVME drives 4 96"},{"location":"sophia/hardware-overview/theta-gpu-machine-overview/#thetagpu-login-nodes","title":"ThetaGPU Login Nodes","text":"<p>The Theta login nodes (see above) will be the intended method to access ThetaGPU.  At first, Cobalt jobs cannot be submitted from the Theta login nodes to run on the GPU nodes; until that is supported, users will need to login in to the ThetaGPU service nodes (thetagpusn1 or thetagpusn2) from the Theta login nodes, and from there Cobalt jobs can be submitted to run on the GPU nodes.</p>"},{"location":"sophia/hardware-overview/theta-gpu-machine-overview/#references","title":"References","text":"<ul> <li>Cray DVS</li> <li>Cray Aries</li> <li>Lustre</li> <li>IBM GPFS</li> </ul>"},{"location":"sophia/performance-tools/darshan/","title":"Darshan on ThetaGPU","text":""},{"location":"sophia/performance-tools/darshan/#overview","title":"Overview","text":"<p>Darshan instrumentation on ThetaGPU is not automatically included into the binary like Theta. The user must set the <code>LD_PRELOAD</code> variable as part of running the job.</p> <p>Logs will be generated in the directory: /lus/grand/logs/darshan/thetagpu/// <p>In order to view a log, use the darshan-parser utility. <pre><code>module load darshan \nmpirun ... -x LD_PRELOAD=$DARSHAN_PRELOAD\n</code></pre></p>"},{"location":"sophia/performance-tools/darshan/#more-information","title":"More information","text":""},{"location":"sophia/performance-tools/nvidia-nsight/","title":"NVIDIA Nsight","text":""},{"location":"sophia/performance-tools/nvidia-nsight/#references","title":"References","text":"<ul> <li>NVIDIA Nsight Systems Documentation</li> <li>NVIDIA Nsight Compute Documentation</li> </ul>"},{"location":"sophia/performance-tools/nvidia-nsight/#introduction","title":"Introduction","text":"<p>NVIDIA\u00ae Nsight\u2122 Systems provides developers a system-wide visualization of an applications performance. Developers can optimize bottlenecks to scale efficiently across any number or size of CPUs and GPUs on ThetaGPU. For further optimizations to compute kernels developers should use Nsight Compute.</p> <p>The NVIDIA Nsight Compute is an interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command line tool. </p> <p>In addition, the baseline feature of this tool allows users to compare results within the tool. NVIDIA Nsight Compute provides a customizable and data-driven user interface,  metric collection, and can be extended with analysis scripts for post-processing results.</p>"},{"location":"sophia/performance-tools/nvidia-nsight/#step-by-step-guide","title":"Step-by-step guide","text":""},{"location":"sophia/performance-tools/nvidia-nsight/#1-common-part-on-thetagpu","title":"1. Common part on thetaGPU","text":"<p>Build your application for ThetaGPU and submit your job script to ThetaGPU or start an interactive job mode on ThetaGPU as follows: <pre><code>$ module load cobalt/cobalt-gpu\n$ qsub -I -n 1 -t 30 -q full-node -A {your_project}\n</code></pre></p>"},{"location":"sophia/performance-tools/nvidia-nsight/#2-nsight-systems","title":"2. Nsight Systems","text":"<p>Run your application with Nsight Systems as follows: <pre><code>$ nsys profile -o {output_filename} --stats=true ./{your_application}\n</code></pre></p>"},{"location":"sophia/performance-tools/nvidia-nsight/#3-nsight-compute","title":"3. Nsight Compute","text":"<p>Run your application with Nsight Compute. <pre><code>$ ncu --set detailed -k {kernel_name} -o {output_filename} ./{your_application}\n</code></pre> Note: Without -o option, Nsight Compute provides performance data as a standard output</p>"},{"location":"sophia/performance-tools/nvidia-nsight/#4-post-processing-the-profiled-data","title":"4. Post-processing the profiled data","text":""},{"location":"sophia/performance-tools/nvidia-nsight/#post-processing-via-cli","title":"Post-processing via CLI.","text":"<pre><code>$ nsys stats {output_filename}.qdrep\n$ ncu -i {output_filename}.ncu-rep\n</code></pre>"},{"location":"sophia/performance-tools/nvidia-nsight/#post-processing-on-your-local-system-via-gui","title":"Post-processing on your local system via GUI","text":"<ul> <li>Install NVIDIA Nsight Systems and NVIDIA Nsight Compute after downloading both of them from the  NVIDIA Developer Zone.</li> <li>Download nsys output files (i.e., ending with .qdrep and . sqlite) to your local system, and then open them with NVIDIA Nsight Systems on your local system.</li> <li>Download ncu output files (i.e., ending with .ncu-rep) to your local system, and then open them with NVIDIA Nsight Compute on your local system.</li> </ul> <p>For more options for performance analysis with Nsight Systems and Nsight Compute: <pre><code>$ nsys --help\n$ ncu --help\n</code></pre></p>"},{"location":"sophia/performance-tools/nvidia-nsight/#a-quick-example","title":"A quick example","text":""},{"location":"sophia/performance-tools/nvidia-nsight/#nsight-systems","title":"Nsight Systems","text":""},{"location":"sophia/performance-tools/nvidia-nsight/#running-a-stream-benchmark-with-nsight-systems","title":"Running a stream benchmark with Nsight Systems","text":"<pre><code>jkwack@thetagpu18:~/HPC_benchmarks/BabelStream/JK_thetaGPU$ nsys profile -o JKreport-nsys-BableStream --stats=true ./cuda-stream\n\nWarning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n\nCollecting data...\n\nBabelStream\n\nVersion: 3.4\n\nImplementation: CUDA\n\nRunning kernels 100 times\n\nPrecision: double\n\nArray size: 268.4 MB (=0.3 GB)\n\nTotal size: 805.3 MB (=0.8 GB)\n\nUsing CUDA device A100-SXM4-40GB\n\nDriver: 11000\n\nFunction    MBytes/sec  Min (sec)   Max         Average     \n\nCopy        1381210.283 0.00039     0.00040     0.00039     \n\nMul         1339635.322 0.00040     0.00041     0.00040     \n\nAdd         1357739.235 0.00059     0.00061     0.00060     \n\nTriad       1366533.461 0.00059     0.00061     0.00060     \n\nDot         1210611.093 0.00044     0.00047     0.00046     \n\nProcessing events...\n\nCapturing symbol files...\n\nSaving temporary \"/tmp/nsys-report-b948-7122-9b9a-feb1.qdstrm\" file to disk...\n\nCreating final output files...\n\n\nProcessing [==============================================================100%]\n\nSaved report file to \"/tmp/nsys-report-b948-7122-9b9a-feb1.qdrep\"\n\nExporting 7098 events: [==================================================100%]\n\n\nExported successfully to\n\n/tmp/nsys-report-b948-7122-9b9a-feb1.sqlite\n\n\nGenerating CUDA API Statistics...\n\nCUDA API Statistics (nanoseconds)\n\n\nTime(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n\n-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n\n   44.8       280504347           4      70126086.8         1050249       276881346  cudaMalloc                                                                      \n\n   31.4       196878210         401        490968.1          381542          600948  cudaDeviceSynchronize                                                           \n\n   22.4       140280462         103       1361946.2          436597        32339232  cudaMemcpy                                                                      \n\n    1.0         6263864           4       1565966.0         1236542         1884610  cudaFree                                                                        \n\n    0.4         2729558         501          5448.2            4970           36269  cudaLaunchKernel                                                                \n\n\n\n\n\nGenerating CUDA Kernel Statistics...\n\nCUDA Kernel Statistics (nanoseconds)\n\n\nTime(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n\n-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n\n   24.7        58518170         100        585181.7          580347          594395  void add_kernel&lt;double&gt;(double const*, double const*, double*)                                                                                                                                                                                                                                                                               \n\n   24.6        58312184         100        583121.8          576987          595067  void triad_kernel&lt;double&gt;(double*, double const*, double const*)                                                                                                                                                                                                                                                                             \n\n   18.1        42942748         100        429427.5          419548          438333  void dot_kernel&lt;double&gt;(double const*, double const*, double*, int)                                                                                                                                                                                                                                                                          \n\n   16.5        39062588         100        390625.9          388733          392125  void mul_kernel&lt;double&gt;(double*, double const*)                                                                                                                                                                                                                                                                                              \n\n   16.0        37980930         100        379809.3          376541          392925  void copy_kernel&lt;double&gt;(double const*, double*)                                                                                                                                                                                                                                                                                             \n\n    0.2          521628           1        521628.0          521628          521628  void init_kernel&lt;double&gt;(double*, double*, double*, double, double, double)                                                                                                                                                                                                                                                                  \n\n\n\n\nGenerating CUDA Memory Operation Statistics...\n\nCUDA Memory Operation Statistics (nanoseconds)\n\n\nTime(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n\n-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n\n  100.0        94988808         103        922221.4            2335        32089877  [CUDA memcpy DtoH]                                                              \n\n\n\nCUDA Memory Operation Statistics (KiB)\n\n\n              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n\n-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n\n         786632.000             103             7637.204              2.000           262144.000  [CUDA memcpy DtoH]                                                              \n\n\n\n\n\nGenerating Operating System Runtime API Statistics...\n\nOperating System Runtime API Statistics (nanoseconds)\n\n\nTime(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n\n-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n\n   47.6      1184528854          22      53842220.6           10240       100064607  sem_timedwait                                                                   \n\n   36.2       901118663          20      45055933.1           16792       100119932  poll                                                                            \n\n   15.9       395394013        1456        271561.8            1012        17141907  ioctl                                                                           \n\n    0.2         4052477         105         38595.0            2064          111321  open64                                                                          \n\n    0.1         1716108          86         19954.7            1042          509715  mmap                                                                            \n\n    0.0          963824          51         18898.5            1363          771371  fopen                                                                           \n\n    0.0          208937           4         52234.3           42771           62549  pthread_create                                                                  \n\n    0.0          141128           3         47042.7           41178           58621  fgets                                                                           \n\n    0.0           52102          11          4736.5            1824           18145  munmap                                                                          \n\n    0.0           41950           6          6991.7            1783           19146  putc                                                                            \n\n    0.0           37641           5          7528.2            2444           13065  open                                                                            \n\n    0.0           32953          11          2995.7            1422            6402  write                                                                           \n\n    0.0           31581          23          1373.1            1042            1823  fclose                                                                          \n\n    0.0           16470           2          8235.0            1412           15058  sched_yield                                                                     \n\n    0.0            8927           2          4463.5            3437            5490  socket                                                                          \n\n    0.0            8586           1          8586.0            8586            8586  pipe2                                                                           \n\n    0.0            7324           3          2441.3            1593            3627  fwrite                                                                          \n\n    0.0            5782           2          2891.0            1844            3938  fread                                                                           \n\n    0.0            5751           1          5751.0            5751            5751  connect                                                                         \n\n    0.0            4369           2          2184.5            1714            2655  read                                                                            \n\n    0.0            3998           3          1332.7            1082            1603  fcntl                                                                           \n\n    0.0            1433           1          1433.0            1433            1433  fflush                                                                          \n\n    0.0            1252           1          1252.0            1252            1252  bind                                                                            \n\n\n\n\n\nGenerating NVTX Push-Pop Range Statistics...\n\nNVTX Push-Pop Range Statistics (nanoseconds)\n\n\n\n\n\nReport file moved to \"/gpfs/mira-home/jkwack/HPC_benchmarks/BabelStream/JK_thetaGPU/JKreport-nsys-BableStream.qdrep\"\n\nReport file moved to \"/gpfs/mira-home/jkwack/HPC_benchmarks/BabelStream/JK_thetaGPU/JKreport-nsys-BableStream.sqlite\"\n</code></pre>"},{"location":"sophia/performance-tools/nvidia-nsight/#nsight-compute","title":"Nsight Compute","text":""},{"location":"sophia/performance-tools/nvidia-nsight/#running-a-stream-benchmark-with-nsight-compute-for-triad_kernel","title":"Running a stream benchmark with Nsight Compute for triad_kernel","text":"<pre><code>jkwack@thetagpu18:~/HPC_benchmarks/BabelStream/JK_thetaGPU$ ncu --set detailed -k triad_kernel -o JKreport-ncu_detailed-triad_kernel-BableStream ./cuda-stream\n\nBabelStream\n\nVersion: 3.4\n\nImplementation: CUDA\n\nRunning kernels 100 times\n\nPrecision: double\n\nArray size: 268.4 MB (=0.3 GB)\n\nTotal size: 805.3 MB (=0.8 GB)\n\n==PROF== Connected to process 166971 (/gpfs/mira-home/jkwack/HPC_benchmarks/BabelStream/JK_thetaGPU/cuda-stream)\n\nUsing CUDA device A100-SXM4-40GB\n\nDriver: 11000\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 18 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\n==PROF== Profiling \"triad_kernel\": 0%....50%....100% - 19 passes\n\nFunction    MBytes/sec  Min (sec)   Max         Average     \n\nCopy        1336793.345 0.00040     0.00042     0.00041     \n\nMul         1307948.274 0.00041     0.00043     0.00042     \n\nAdd         1335561.797 0.00060     0.00062     0.00062     \n\nTriad       976.089     0.82503     1.00961     0.87930     \n\nDot         1081921.148 0.00050     0.00055     0.00053     \n\n==PROF== Disconnected from process 166971\n\n==PROF== Report: /gpfs/mira-home/jkwack/HPC_benchmarks/BabelStream/JK_thetaGPU/JKreport-ncu_detailed-triad_kernel-BableStream.ncu-rep\n</code></pre>"},{"location":"sophia/programming-models/kokkos/","title":"Kokkos on ThetaGPU","text":""},{"location":"sophia/programming-models/kokkos/#overview","title":"Overview","text":"<p>Kokkos implements a programming model in C++ for writing performance portable applications targeting all major HPC platforms. It provides abstractions for both parallel execution of code and data management. Kokkos is designed to target complex node architectures with N-level memory hierarchies and multiple types of execution resources. It can use OpenMP, etc as a backend programming model. For more information please visit https://github.com/kokkos/kokkos</p> <p>The Kokkos shared memory programming model is a C++ library, that provides the necessary architecture specific backends (e.g. OpenMP, CUDA, \u2026). To begin with, though, it is important to note that the Kokkos programming model is usable only in C/C++ codes. Hence, for those with Fortran codes, Kokkos must first be encapsulated within C/C++ functions and called from the main Fortran code.</p> <p>The purpose of this document is to provide guidance on using Kokkos on Cooley. Please see the following pages for tutorials and more information on Kokkos: Kokkos GitHub and Kokkos Tutorials. </p>"},{"location":"sophia/programming-models/kokkos/#using-kokkos-at-alcf","title":"Using Kokkos at ALCF","text":"<p>ALCF provides assistance with build instructions, compiling executables, submitting jobs, and providing prebuilt binaries. For questions, contact us at support@alcf.anl.gov.</p>"},{"location":"sophia/programming-models/kokkos/#building-kokkos-on-thetagpu","title":"Building Kokkos on ThetaGPU","text":"<p>Please follow the steps below to build Kokks on ThetaGPU.</p>"},{"location":"sophia/programming-models/kokkos/#step-1-clone-the-repository","title":"Step 1:  Clone the repository","text":"<p><pre><code>git clone https://github.com/kokkos/kokkos.git \ncd kokkos \nexport KOKKOS_PATH=\u201d${PWD}\u201d\n</code></pre> Note:  the Kokkos Project strives to keep the master branch stable.</p>"},{"location":"sophia/programming-models/kokkos/#step-2-make-sure-that-a-relatively-new-version-of-cmake-is-available","title":"Step 2:  Make sure that a relatively new version of CMake is available","text":"<pre><code>module load cmake/3.14.5\n</code></pre>"},{"location":"sophia/programming-models/kokkos/#step-3-create-a-build-directory","title":"Step 3:  Create a build directory","text":"<pre><code>mkdir build &amp;&amp; cd build\n</code></pre>"},{"location":"sophia/programming-models/kokkos/#step-4-generate-the-makefile","title":"Step 4:  Generate the Makefile","text":"<p><pre><code>cmake ../kokkos \\\n    -DCMAKE_CXX_COMPILER=CC \\\n    -DCMAKE_INSTALL_PREFIX=${PWD}/kokkos-install \\\n    -DKokkos_ENABLE_OPENMP=On \\\n    -DKokkos_ENABLE_HWLOC=On \\\n    -DKokkos_HWLOC_DIR=/usr \\\n    -DHWLOC_LIBRARY=/usr/lib64/libhwloc.so\n</code></pre> Note:  the default compiler on Theta should be sufficient to build Kokkos.</p>"},{"location":"sophia/programming-models/kokkos/#step-5-install-kokkos","title":"Step 5: Install Kokkos","text":"<p><pre><code>make install\n</code></pre> Note: This will end up installing Kokkos in <code>${KOKKOS_PATH}/build/kokkos-install</code>. </p> <p>If you wish to install it in a different directory, change <code>CMAKE_INSTALL_PREFIX</code> in step 4.</p>"},{"location":"sophia/programming-models/openmp/","title":"OpenMP on ThetaGPU","text":""},{"location":"sophia/programming-models/openmp/#openmp-threading-on-cpu","title":"OpenMP threading on CPU","text":"<p>All the compilers available on ThetaGPU supports OpenMP threading.</p>"},{"location":"sophia/programming-models/openmp/#openmp-offload-on-a100-gpu","title":"OpenMP offload on A100 GPU","text":"<p>A few compilers which support OpenMP offload are accessible on ThetaGPU. They are made available via modules.</p> <pre><code>$ module avail llvm\n\n--------- /lus/theta-fs0/software/environment/thetagpu/lmod/modulefiles ----------\n   llvm/main-20210112    llvm/release-12.0.0 (D)\n\n$ module avail nvhpc\n\n--------- /lus/theta-fs0/software/environment/thetagpu/lmod/modulefiles ----------\n   nvhpc-byo-compiler/20.9 (D)    nvhpc-nompi/20.9 (D)    nvhpc/20.9 (D)\n   nvhpc-byo-compiler/21.2        nvhpc-nompi/21.2        nvhpc/21.2\n   nvhpc-byo-compiler/21.3        nvhpc-nompi/21.3        nvhpc/21.3\n</code></pre>"},{"location":"sophia/programming-models/openmp/#llvm-clang-for-cc","title":"LLVM Clang for C/C++","text":"<ul> <li>Clang OpenMP offload features </li> <li>More details about the OpenMP runtime</li> </ul> <p>If there is an issue with the compiler, feel free to contact openmp-dev@lists.llvm.org</p>"},{"location":"sophia/programming-models/openmp/#warning-message","title":"Warning message","text":"<pre><code>clang-12 warning: Unknown CUDA version. version.txt: 11.0.205. Assuming the latest supported version 10.1 [-Wunknown-cuda-version]\n</code></pre> <p>Means CUDA 11 language features are not supported. As long as these features are not used, the full CUDA 11.x toolchain works. This warning can be ignored or suppressed by -Wno-unknown-cuda-version compiler option.</p>"},{"location":"sophia/programming-models/openmp/#compiling-example","title":"Compiling example","text":"<pre><code>module load llvm/release-12.0.0\nclang++ -fopenmp -fopenmp-targets=nvptx64 your_source.cpp\n</code></pre>"},{"location":"sophia/programming-models/openmp/#nvidia-hpc-sdk-for-ccfortran","title":"NVIDIA HPC SDK for C/C++/Fortran","text":"<p>OpenMP documentation</p> <p>For compiler bugs, please file bug reports at https://developer.nvidia.com after login</p>"},{"location":"sophia/programming-models/openmp/#compiling-example_1","title":"Compiling example","text":"<pre><code>module load nvhpc-sdk/nvhpc/21.3\nnvfortran -mp=gpu -gpu=cc80 your_source.f90\n</code></pre>"},{"location":"sophia/programming-models/raja/","title":"RAJA ThetaGPU","text":""},{"location":"sophia/programming-models/raja/#overview","title":"Overview","text":"<p>RAJA is a collection of C++ software abstractions, being developed at Lawrence Livermore National Laboratory (LLNL), that enable architecture portability for HPC applications. The overarching goals of RAJA are to: - Make existing (production) applications portable with minimal disruption - Provide a model for new applications so that they are portable from inception.</p> <p>RAJA targets portable, parallel loop execution by providing building blocks that extend the generally-accepted parallel for idiom.</p> <p>Additional information can be found at RAJA User Guide.</p>"},{"location":"sophia/programming-models/raja/#using-raja","title":"Using RAJA","text":"<p>RAJA provides a project template for how to use RAJA in an application project that uses CMake or Make. This is located at RAJA Project Template.</p>"},{"location":"sophia/programming-models/raja/#how-to-get-the-source-code","title":"How to get the source code","text":"<p>The RAJA source code lives at RAJA github. </p> <p>It can be cloned with git clone <code>--recursive https://github.com/llnl/raja.git</code>. The recursive clone will also clone RAJA's dependencies in the proper locations.</p>"},{"location":"sophia/programming-models/raja/#add-a-cmake-configuration-for-thetagpu","title":"Add a cmake configuration for ThetaGPU","text":"<pre><code>raja/host-configs/alcf-builds/thetagpu.cmake\n</code></pre> <pre><code>set(RAJA_COMPILER \"RAJA_COMPILER_GNU\" CACHE STRING \"\")\n\nset(CMAKE_CXX_COMPILER \"g++\" CACHE PATH \"\")\nset(CMAKE_C_COMPILER \"gcc\" CACHE PATH \"\")\n\nset(CMAKE_CXX_FLAGS_RELEASE \"-O3\" CACHE STRING \"\")\nset(CMAKE_CXX_FLAGS_RELWITHDEBINFO \"-O3\" CACHE STRING \"\")\nset(CMAKE_CXX_FLAGS_DEBUG \"-O0 -g\" CACHE STRING \"\")\n\nset(CUDA_COMMON_OPT_FLAGS -restrict; --gpu-architecture sm_80; -std c++11; --expt-extended-lambda)\nset(CUDA_COMMON_DEBUG_FLAGS -restrict; --gpu-architecture sm_80; -std c++11; --expt-extended-lambda)\n\nset(HOST_OPT_FLAGS -Xcompiler -O3 -Xcompiler -fopenmp)\n\nif(CMAKE_BUILD_TYPE MATCHES Release)\n  set(RAJA_NVCC_FLAGS -O3; ${CUDA_COMMON_OPT_FLAGS}; -ccbin; ${CMAKE_CXX_COMPILER} ; ${HOST_OPT_FLAGS} CACHE LIST \"\")\nelseif(CMAKE_BUILD_TYPE MATCHES RelWithDebInfo)\n  set(RAJA_NVCC_FLAGS -g; -G; -O3; ${CUDA_COMMON_OPT_FLAGS}; -ccbin; ${CMAKE_CXX_COMPILER} ; ${HOST_OPT_FLAGS} CACHE LIST \"\")\nelseif(CMAKE_BUILD_TYPE MATCHES Debug)\n  set(RAJA_NVCC_FLAGS -g; -G; -O0; ${CUDA_COMMON_DEBUG_FLAGS}; -ccbin; ${CMAKE_CXX_COMPILER} ; -Xcompiler -fopenmp CACHE LIST \"\")\nendif()\n\nset(RAJA_RANGE_ALIGN 4 CACHE INT \"\")\nset(RAJA_RANGE_MIN_LENGTH 32 CACHE INT \"\")\nset(RAJA_DATA_ALIGN 64 CACHE INT \"\")\n\nset(RAJA_HOST_CONFIG_LOADED On CACHE Bool \"\")\n</code></pre>"},{"location":"sophia/programming-models/raja/#now-build-on-the-thetagpu-compute-node","title":"Now build on the ThetaGPU compute node","text":"<pre><code>git clone --recursive https://github.com/llnl/raja.git\n\nmkdir build_alcf-thetagpu &amp;&amp; cd build_alcf-thetagpu\ncmake \\\n  -DCMAKE_BUILD_TYPE=Release \\\n  -C ../host-configs/alcf-builds/thetagpu.cmake \\\n  -DENABLE_OPENMP=On \\\n  -DENABLE_CUDA=On \\\n  -DCUDA_ARCH=sm_80 \\\n  -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda \\\n  -DCMAKE_INSTALL_PREFIX=../install_${BUILD_SUFFIX} \\\n  \"$@\" \\\n\n  ..\n</code></pre>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/","title":"Job and Queue Scheduling on ThetaGPU","text":""},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#queues-and-job-scheduling","title":"Queues and Job Scheduling","text":""},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#general-policy","title":"General Policy","text":"<p>We ask that all users follow good etiquette and be kind to one another.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#job-priority","title":"Job Priority","text":"<p>As with all Argonne Leadership Computing Facility production systems, job priority in the queue is based on several criteria:</p> <ul> <li>positive balance of your project</li> <li>size (in nodes) of the job, larger jobs receive higher priority</li> <li>the type of project (e.g. INCITE, ALCC, or discretionary)</li> <li>job duration - shorter duration jobs will accumulate priority more quickly, so it is best to specify the job run time as accurately as possible</li> </ul>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#reservations-and-scheduling-policy","title":"Reservations and Scheduling Policy","text":""},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#monday-maintenance","title":"Monday Maintenance","text":"<p>When the ALCF is on a regular business schedule, preventitive maintenance is typically scheduled on alternate Mondays. The showres command may be used to view pending and active maintenance reservations.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#big-run-mondays","title":"Big Run Mondays","text":"<p>As part of our regular maintenance procedures on Mondays, we will promote to the highest priority any jobs in the queued state requesting 802 nodes or more (.ie. capability jobs). Promotion is subject to operational discretion.</p> <p>We may also, at our discretion, take the opportunity to promote the priority of capability jobs if the system has been drained of jobs for any other reason.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#incitealcc-overburn-policy","title":"INCITE/ALCC Overburn Policy","text":"<p>If an INCITE or ALCC project has exhausted its allocation in the first 11 months of its allocation year, it is eligible for overburn running. At this point, capability jobs submitted by INCITE and ALCC projects will run in the default queue (instead of backfill) for the first 11 months of the allocation year until 125% of the project allocation has been consumed. Note that non-capability jobs will be routed to backfill queue.</p> <p>INCITE and ALCC projects needing additional overburn hours should e-mail support@alcf.anl.gov with a short description of what they plan to do with the additional hours, highlighting specific goals or milestones and the time expected to accomplish them. This will be reviewed by the scheduling committee, allocations committee, and ALCF management. Requests should be submitted 15 days before the start of the next quarter of the allocation year for full consideration. Non-capability jobs from projects that have exhausted their allocation will continue to run in backfill. </p> <p>To be clear, this policy does not constitute a guarantee of extra time, and we reserve the right to prioritize the scheduling of jobs submitted by projects that have not yet used 100% of their allocations, so the earlier that an INCITE or ALCC project exhausts its allocation, the more likely it is to be able to take full advantage of this policy.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#thetagpu-node-queues","title":"ThetaGPU Node Queues","text":"<p>Note: Users will need an allocation on ThetaGPU to utilize the GPU nodes. Request for an allocation by filling out this form: Allocation request. ThetaGPU is listed under Theta on the form.</p> <p>The GPU nodes are new and we expect the workload to be significantly different than it is on the KNL nodes.  This document describes the current state of affairs, but we will monitor usage and adjust the policies as necessary.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#nodes-vs-queue-vs-mig-mode","title":"Nodes vs Queue vs MIG mode","text":"<p>The GPU nodes are NVidia DGX A100 nodes and each node contains eight (8) A100 GPUs.  You may request either entire nodes, or a single GPU based on your job needs.  What you will get is determined by the queue you submit to (See Queues section below).  If it has node in the name, you will get nodes.  If it has GPU in the name, you will get a single GPU. Note that if you need more than a single GPU, you should submit to the full-node queue.</p> <p>Additionally, the Nvidia A100 GPUs support a feature called \u201cMulti-Instance GPU\u201d (MIG) mode.  This allows a single GPU to be shared by up to 7 different processes.  We do not schedule at this level, but to use MIG capabilities, you may pass <code>\u2013attrs mig-mode=True</code> in with your qsub and use the <code>nvidia-smi_mig</code> command (note the UNDERSCORE) just as you would calling <code>nvidia-smi mig ...</code> directly. Attempts to call <code>nvidia-smi mig ...</code>(no underscore) directly will result in an error message. The single-gpu host will, by default, not create an MIG instance and users will have direct access to the gpu. If you are not using MIG mode, your session will appear as if it were a normal full-node system with only one gpu. Note that as of 12/13/21, MIG mode is unavailable for full-node jobs.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#queues","title":"Queues","text":"<p>There are three primary queues:   - full-node: This is the general production queue for jobs that require full nodes. The -n parameter in your qsub will match the resource type in this queue i.e. -n 2 in node queue will get you two full nodes.   - bigmem -  2 of the nodes have 640 GB of memory compared to the other 22 nodes with 320 GB. Use this queue to access these 2 nodes by specifying <code>-q bigmem</code> in your script. A max of 2 nodes (-n 2) can be requested in this queue.   - single-gpu: This is the general production queue for jobs that operate best on a single GPUs. The -n parameter in your qsub should always be 1 as you can only submit to a single gpu. If you need more than 1 gpu, use the full-node queue.</p> <p>Here are the initial queue limits. You may not violate either of these policies.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#full-node-queue","title":"full-node queue:","text":"<ul> <li>MinTime is 5 minutes</li> <li>MaxTime is 12 hours</li> <li>MaxQueued will be 20 jobs</li> <li>MaxRunning will be 10 jobs</li> </ul>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#bigmem-queue","title":"bigmem queue:","text":"<ul> <li>MinTime is 5 minutes</li> <li>MaxTime is 12 hours</li> <li>MaxQueued is 2 jobs</li> <li>MaxRunning is 1 job</li> </ul>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#single-gpu-queue","title":"single-gpu queue:","text":"<ul> <li>MinTime is 5 minutes</li> <li>MaxTime is 1 hour</li> <li>MaxQueued is 1 job</li> <li>MaxRunning is 1 job</li> </ul> <p>The initial queue policy will be simple First-In-First-Out (FIFO) based on priority with EASY backfill.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#running-jobs-on-thetagpu","title":"Running Jobs On ThetaGPU","text":"<p>Note: Users will need an allocation on ThetaGPU to utilize the GPU nodes. Request for an allocation by filling out this form: Allocation request. ThetaGPU is listed under Theta on the form.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#running-on-multiple-gpu-nodes","title":"Running on multiple GPU nodes","text":"<p>Until there is tighter integration of Cobalt and mpirun on GPU nodes, the user will have to identify the nodes Cobalt assigned to their job and pass them as options to mpirun along with some other mpirun options.  The following shows 2 different code snippets on how to get the hosts allocated to the job and pass them to mpirun.</p> <p>option 1 - simplest</p> <ul> <li>mpirun -hostfile $COBALT_NODEFILE -n 16 -npernode 8 mpi-example-code</li> <li>where $COBALT_NODEFILE is a file that the -hostfile option can use.</li> </ul> <p>option 2 - little more complicated</p> <ul> <li>HOSTS=$(cat $COBALT_NODEFILE | sed ':a;N;$!ba;s/\\n/,/g')</li> <li>mpirun  --np 16 --host $HOSTS --oversubscribe ./mpi-example-code</li> </ul> <p>To specifically see how the MPI ranks were assigned, one could add --display-map --display-allocation to the mpirun options.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#controlling-which-cobalt-instance-gpu-for-my-commands","title":"Controlling which Cobalt instance (GPU) for my commands","text":"<p>IF YOU ARE ONLY USING KNL NODES NOTHING CHANGES AND YOU CAN IGNORE THIS</p> <p>Because of the difference in architectures and limitations in Cobalt V1, we are running two Cobalt instances, the existing one for the KNL nodes, which remains as is, and a second one for the GPU nodes.  You need to be able to control which instance you are interacting with and there are several ways to do so.   - As was true in the past, if you do nothing, the commands will default to the architecture associated with the host you are on when you issue it   - If you are on the Theta login nodes, commands will default to the KNL instance.   - If you are on a GPU node, for instance the build nodes, then commands will default to the GPU instance.   - You can set an environment variable to control which instance the default commands (qsub, qstat, etc) will interact with. The primary use case here will be users who only use GPU nodes, but are working from the Theta login nodes.  To do so, you may:     - <code>module load cobalt/cobalt-knl</code> which would make cobalt commands interact with the original Cobalt instance and launch jobs on the KNL nodes     - <code>module load cobalt/cobalt-gpu</code> which would make Cobalt commands interact with the new Cobalt instance and launch jobs on the GPU nodes     - you can also set COBALT_CONFIG_FILES=       - knl config: /etc/cobalt.knl.conf       - gpu config: /etc/cobalt.gpu.conf <p>You can use suffixed commands to explicitly control which instance you are interacting with. If you regularly use both types of nodes, this is the recommended path to avoid confusion and to prevent launching jobs on the wrong architecture.</p> <p>All the commands you are used to are there, they take the same command line parameters, etc., they just have either -knl or a -gpu suffix on them. For instance:   - qsub-knl  would submit a job to the KNL nodes   - qstat-gpu would check the queue status for the GPU nodes. <p>Requesting DGX nodes or individual GPUs The DGX nodes, which contain (8) A100 GPUs, are extremely powerful and it can be very difficult for a single job to efficiently use an entire node.  For this reason, you may request either full nodes (all 8 GPUS) or individual GPUs.  What you are assigned (a node or a GPU) is dependent on the queue you submit to:   - If the queue name ends in -node, you will get full nodes (8 A100 GPUs)    - If the queue name ends in -gpu, you will get an individual GPU   - The -n parameter on the qsub is the number of resources of the type in that queue. So, for example:     - <code>qsub -n 2 -q full-node &lt;rest of command line&gt;</code> would get you two full DGX nodes, which would be a total of (16) A100 GPUs     - <code>qsub -n 2 -q single-gpu &lt;rest of command line&gt;</code> would get you two A100 GPUs</p> <p>For reservations, you can only have one queue, and the resources in the queue need to be consistent, so your entire reservation must be in nodes or GPUs. If you need both, you will need two reservations, one for each type of resource.</p> <pre><code>- Node names are of the form thetagpu## where ## ranges from 01 to 24.  This is an entire node (8 GPUs)\n- GPU names are of the form thetagpu##-gpu# where the GPU numbers range from 0-7.\n</code></pre>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#multi-instance-gpu-mig-mode","title":"Multi-Instance GPU (MIG) mode","text":"<p>The A100 GPUs have a capability known as Multi-Instance GPU (MIG). This allows you a single A100 GPU to be reconfigured at a hardware level down to a maximum of 7 instances. The valid configuration are shown in a table on the MIG page referenced above. These instances appear as a GPU to the application. In order to use this feature, the GPU must be put into MIG mode and this requires a reset of the GPU.  At the current time, we are not supporting scheduling at the MIG level.  However, a user can request that their GPU be put in MIG mode and then they can reconfigure the GPU into a supported configuration from their job script. </p> <p>If you wish to have the resources you have requested put into MIG mode you can add either of these to your qsub command line: <code>--attrs mig-mode=True</code>.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#details-of-a-job-submission","title":"Details of a job submission","text":"<p>Details of the job submission are recorded in the <code>&lt;jobid&gt;.cobaltlog</code>. This file contains the qsub command and environment variables. The location of this file can be controlled with the <code>qsub --debuglog &lt;path&gt;</code> that defaults to the same place as the <code>.output</code> and <code>.error</code> files.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#jobs-stuck-in-starting-state","title":"Jobs stuck in \"starting\" state","text":"<p>If you submit a job and qstat shows it in \"starting\" state for 5 minutes or more, most likely your memory/numa mode selection requires rebooting some or all of the nodes your job was assigned. This process takes about 15 minutes, during which your job appears to be in the \"starting\" phase. When no reboots are required, the \"starting\" phase only lasts a matter of seconds.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#utime-and-stime","title":"\"utime\" and \"stime\"","text":"<p>At the bottom of a .ouput file, there is usually a line like: <pre><code>Application 3373484 resources: utime ~6s, stime ~6s, Rss ~5036, inblocks ~0, outblocks ~8\n</code></pre> The \"utime\" and \"stime\" values are user CPU time and system CPU time from the aprun and getrusage commands. They are rounded aggregate numbers scaled by the number of resources used, and are approximate. The aprun man page has more information about them."},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#cobalt-directives-on-the-second-line-of-job-script","title":"<code>#COBALT</code> directives on the second line of job script","text":"<p>If <code>#COBALT</code> directives are used inside a job submission script, then they must appear at the topmost lines of the script. <code>#COBALT</code> directives following a blank line will be ignored. Attempting to qsub the following example script will lead to the error message below.</p> <p><pre><code>&gt; cat submit.csh #!/bin/csh #COBALT -n 2 -t 2:00:00 -q full-node mpirun -np 20 -npernode 10 ./my_app &gt; qsub submit.csh Usage: qsub.py [options] [] Refer to man pages for JOBID EXPANSION and SCRIPT JOB DIRECTIVES. No required options provided\n</code></pre> A correct submission script would look like the following with the blank line removed. <pre><code>&gt; cat submit.csh\n\n#!/bin/csh\n\n#COBALT -n 2 -t 2:00:00 -q full-node\n\nmpirun -np 20 -npernode 10 ./my_app\n\n&gt; qsub submit.csh\n\n12345\n</code></pre></p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#job-submission-on-thetagpu","title":"Job Submission on ThetaGPU","text":"<p>The queuing system used on ThetaGPU is Cobalt. On ThetaGPU, Cobalt jobs may run either as script jobs or interactive mode jobs.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#script-method","title":"Script Method","text":"<p>In the script method, Cobalt will execute a user-supplied script when running a user\u2019s job. Following are the required flags to <code>qsub</code>, as well as some of the more common options. A complete list of options may be found as a part of the <code>qsub</code> manpage, available on any login node.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#required-flags","title":"Required Flags","text":"<pre><code>-n NN - number of nodes (-n 16 for 16 nodes)\n-t time - running time (-t 30 for 30 minutes, -t 01:10:20 for 1 hr 10 min 20 sec) \n-A Project - project (-A YourProject)\n</code></pre>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#common-options","title":"Common options","text":"<p><pre><code>--attrs - you may specify additional attributes for your job. \n          Multiple attribute key-value pairs are colon-delimited. \n          The following are common on the KNL nodes: \n          - filesystem: a comma-separated list of filesystems used while running your job\n          - location: a comma-separated list of node ids. Ranges may be hyphenated. \n          - mcdram: The desired MCDRAM mode of a job (default: cache) \n          - numa: The desired NUMA mode of a job (default: quad)\n\n          The following are common on the GPU nodes:          \n          - location: a comma-separated list of node names (not IDs). Ranges may NOT be hyphenated on the GPU nodes. \n          - mig-mode: Should the GPUs be put in Multi Instance GPU (MIG) mode (default: False) \n          - pubnet: Enable public network connectivity from compute nodes\n\nSee: https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html\n\n\n--env VAR1=1:VAR2=2:\u2026 - specify required environment variables\n-i file - give a file name to be used for stdin \n-O Name - name your job and stdout/stderr (-O Job1) \n-q queue - queue name (full-node, single-gpu, bigmem) (default: full-node)\n</code></pre> Note: Remember to give all options before the executable name.</p> <p>Users will need an allocation on ThetaGPU to utilize the GPU nodes. Request for an allocation on ThetaGPU bthrough our Director's Discretionary award. ThetaGPU is listed under Theta on the form.</p> <p>Users will need to load the <code>cobalt/cobalt-gpu</code> module before issuing a <code>qsub</code> command to access ThetaGPU.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#example-script","title":"Example Script","text":"<p><pre><code>module load cobalt/cobalt-gpu\n\nqsub -A YourProject -n 4 -t 30 -q full-node \\\n--env MYVAR=value1 -i inputdata -O Project1_out \\\n\u2013attrs filesystems=home,eagle \\\nprogram.exe progarg1\n</code></pre> The syntax for Cobalt scripting is slightly different than that of a PBS script. For more information, see Cobalt scripting.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#interactive-method","title":"Interactive Method","text":"<p>To run an \u201cinteractive mode\u201d job on ALCF Cray resources, add the \u201c-I\u201d (uppercase \"i\", not a lowercase \"L\") flag or \u201c--mode interactive\u201d to your qsub line and omit any executable. Your qsub submission will then wait until nodes are allocated to your job and Cobalt will start a shell on a job-launch node on your behalf. You may aprun against your assigned resources and run other interactive commands from this node. It is important to note that your shell is executed from a launch node and not from your compute head-node. Once your allocation ends, all apruns will be terminated, but your shell will remain for any cleanup actions that you choose to take.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#ensemble-jobs","title":"Ensemble Jobs","text":"<p>Users may run an \u201censemble job\u201d and combine runs into a single script. This can provide major enhancements to throughput, especially for large ensemble jobs. Users may run multiple jobs in sequence or may use multiple backgrounded apruns to subset their resources among multiple backend executables. There is a system limitation of 1000 simultaneous apruns per Cobalt script job.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#submitted-job-with-the-wrong-arguments","title":"Submitted Job with the Wrong Arguments","text":"<p>If you submit a job with the wrong arguments, you can modify without deleting it and resubmitting it. Most settings can be changed using <code>qalter</code>.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#example","title":"Example","text":"<p><pre><code> Usage: qalter [-d] [-v] -A &lt;project name&gt; -t &lt;time in minutes&gt;\n             --attrs filesystems=&lt;filesystem&gt;\n             -e &lt;error file path&gt; -o &lt;output file path&gt;\n             --dependencies &lt;jobid1&gt;:&lt;jobid2&gt;\n             -n &lt;number nodes of&gt; -h --proccount &lt;processor count&gt;\n             -M &lt;email address&gt; &lt;jobid1&gt; &lt;jobid2&gt;\n</code></pre> Note: To change the queue, use <code>qmove</code>. <pre><code>Usage: qmove &lt;queue name&gt; &lt;jobid&gt; &lt;jobid&gt;\n</code></pre></p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#changing-executable-after-job-submission","title":"Changing Executable after Job Submission","text":"<p>When a job is submitted via qsub, Cobalt records the path to the executable or script, but it does not make a copy. As a result, if the executable or script is modified when there is a deletion or modification, it will affect any jobs already submitted that use that executable. To avoid confusion, it is generally best to avoid making changes after job submission.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#holding-and-releasing-jobs","title":"Holding and Releasing Jobs","text":""},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#user-holds","title":"User Holds","text":"<p>To hold a job (prevent from running), use <code>qhold</code>. This will put the job in the <code>user_hold</code> state. <pre><code>qhold &lt;jobid&gt;\n</code></pre> To release a job in a user hold (<code>user_hold</code>) state, use <code>qrls</code>. <pre><code>qrls &lt;jobid&gt;\n</code></pre> A job may also be put into a user hold immediately upon submission by passing <code>qsub</code> the <code>-h</code> flag. <pre><code>qsub -n 8 -t 60 --attrs filesystems=home,grand -A MyProject -h myExe\n</code></pre></p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#dependency-holds","title":"Dependency Holds","text":"<p>For jobs in the <code>dep_hold</code> or <code>dep_fail</code> state, see the section on job dependencies.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#admin-holds","title":"Admin Holds","text":"<p>Jobs in the state <code>admin_hold</code> may be released only by a system administrator.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#maxrun-holds","title":"MaxRun Holds","text":"<p>Jobs may temporarily enter the state <code>maxrun_hold</code> if the user has reached the limit of per-user running jobs in a particular queue. No action is required; as running jobs complete, jobs in the <code>maxrun_hold</code> state will be automatically changed back to queued and eligible to run.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#job-dependencies","title":"Job Dependencies","text":"<p>To submit a job that waits until another job or jobs have completed, use the dependencies argument to qsub. For example, to submit a job that depends on job 12345: <pre><code>qsub -n 2 -t 10 --attrs filesystems=theta-fs0,grand,home -A yourproject --dependencies 12345 a.out\n</code></pre> For multiple dependencies, list and separate with colons. <pre><code>qsub -n 2 -t 30 -A yourproject --attrs filesystems=theta-fs0,grand,home --dependencies 12345:12346 a.out\n</code></pre> Jobs submitted with dependencies will remain in the state <code>dep_hold</code> until all the dependencies are fulfilled, then will proceed to the state queued.</p> <p>Note: In the event any of the dependencies do not complete successfully (nonzero exit status), the job will instead go into the state <code>dep_fail</code>. To manually release a job that is in either <code>dep_hold</code> or <code>dep_fail</code>: <pre><code>qrls --dependencies &lt;jobid&gt;\n</code></pre> or alternatively change the job's dependencies setting to \"none\": <pre><code>qalter --dependencies none &lt;jobid&gt;\n</code></pre></p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#customizing-the-output-of-qstat","title":"Customizing the Output of <code>qstat</code>","text":"<p>Default fields displayed by the <code>qstat</code> command may be changed by setting the <code>QSTAT_HEADER</code> environment variable. <pre><code>export QSTAT_HEADER=\"JobID:JobName:User:WallTime:RunTime:Nodes:State:attrs:Queue\"\"\nqstat\n\nJobID   JobName                           User      WallTime  RunTime   Nodes  State      attrs             Queue\n     =======================================================================================================================================\n     104927  N/A                               user1     02:00:00  01:20:45  128    running    {'numa': 'quad', 'mcdram': 'cache'}  backfill\n     104941  N/A                               user2     00:20:00  N/A       2048   queued     {'numa': 'quad', 'mcdram': 'flat'}   backfill\n     104934  xxxx.yyyyy                        user3     04:00:00  01:10:12  32     running    {'numa': 'quad', 'mcdram': 'cache'}  default\n     104948  Xxx-YY_ZZ                         user4     02:00:00  00:15:03  128    running    {'numa': 'quad', 'mcdram': 'cache'}  default\n     104919  aaaaa_0000_bbbb_c                 user5     06:00:00  01:50:21  64     running    {'numa': 'quad', 'mcdram': 'cache'}  default\n     104945  aaaa_bb_cccc-d_eee_f.hhhhhh.iiii  user6     06:00:00  00:18:25  100    running    {'numa': 'quad', 'mcdram': 'cache'}  default\n     104848  bbbbbb                            user7     01:00:00  N/A       3624   queued     {'numa': 'quad', 'mcdram': 'cache'}  default\n     ....\n</code></pre> One may specify column headers via the <code>--header</code> flag to <code>qstat</code>.</p> <p>Available field names can be seen by entering <code>qstat -fl &lt;jobid&gt;</code> for any current jobid.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#redirecting-standard-input","title":"Redirecting Standard Input","text":"<p>To redirect the standard input to a job, do not use the <code>&lt;</code> redirection operator on the <code>qsub</code> command line. This simply redirects standard input to <code>qsub</code>, not the job itself. Instead, use the qsub option <code>-i</code>. <pre><code># WRONG\nqsub -t 10 -n 1 --attrs filesystems=home a.out &lt; my_input_file.dat\n\n# RIGHT\nqsub -t 10 -n 1 --attrs filesystems=home -i my_input_file.dat a.out\n</code></pre></p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#project-names","title":"Project Names","text":"<p>You can find active project names that your account is associated with by running the command: <pre><code>sbank allocations\n</code></pre> If an account is associated with more than one project, a job must be submitted by using a specific project name using <code>-A</code> or by setting the environment variable <code>COBALT_PROJ</code>.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#sbank","title":"Sbank","text":"<p>The sbank database is updated hourly. This means transactions against your account can take up to an hour before they show up.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#submitting-into-backfill-nodes","title":"Submitting into Backfill Nodes","text":"<p>Sometimes the scheduler will try to clear up room for a large job. During these times, although not many jobs may be running, new jobs are not being scheduled as expected.</p> <p>At such times, backfill nodes may be available. While nodes are being drained for a larger job, other user jobs may be backfilled onto these resources, provided that their requested wall time is less than the remaining drain time of the set of resources. For instance, suppose that 16 nodes are being drained to allow a 16-node job to run. Of the 16 nodes, perhaps eight are empty and the other eight are running an eight-node job that has 2 hours of wall time left. This allows the opportunity to run a 2-hour, eight-node job in the backfill here.</p> <p>To discover available backfill, run the nodelist command. This command can only be run on the service nodes (<code>thetagpusn1-2</code>), it cannot be run on the compute nodes.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#example_1","title":"Example","text":"<p><pre><code>nodelist\nHost             Queue                                       State      Backfill\n==================================================================================\n[...]\nthetagpu16       CompBioAffin:backfill:full-node             down       -       \nthetagpu16-gpu0  single-gpu                                  idle       -       \nthetagpu16-gpu1  single-gpu                                  allocated  -       \nthetagpu16-gpu2  single-gpu                                  allocated  -       \nthetagpu16-gpu3  single-gpu                                  allocated  -       \nthetagpu16-gpu4  single-gpu                                  idle       -       \nthetagpu16-gpu5  single-gpu                                  idle       -       \nthetagpu16-gpu6  single-gpu                                  idle       -       \nthetagpu16-gpu7  single-gpu                                  idle       -       \nthetagpu17       CompBioAffin:backfill:full-node             allocated  -       \nthetagpu17-gpu0  single-gpu                                  down       -       \nthetagpu17-gpu1  single-gpu                                  down       -       \nthetagpu17-gpu2  single-gpu                                  down       -       \nthetagpu17-gpu3  single-gpu                                  down       -       \nthetagpu17-gpu4  single-gpu                                  down       -       \nthetagpu17-gpu5  single-gpu                                  down       -       \nthetagpu17-gpu6  single-gpu                                  down       -       \nthetagpu17-gpu7  single-gpu                                  down       -\n[...]\n</code></pre> In this example, a four-node job with a maximum wall time of 4 hours and 59 minutes can be run during this backfill. The backfill times will not always be identical and will depend on the mix of jobs on the partitions that are being drained.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#submitting-to-specific-nodes","title":"Submitting to Specific Nodes","text":"<p>In rare cases, there may be a need to target specific hardware. This may be accomplished using <code>--attrs location=</code>.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#example_2","title":"Example","text":"<p><pre><code>qsub -t 10 -n 2 --attrs filesystems=grand location=thetagpu01:thetagpu02 myprog.exe \n</code></pre> This will force the job to run on those specific nodes. Should that location become unschedulable, for instance, due to a failed node, the job will not be allowed to run anywhere else, without resetting the location attribute. If more nodes are specified in the location field than are required to fill a job\u2019s requested node count, then the first n nodes available in the location set will be used.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#running-with-a-group-of-users","title":"Running with a Group of Users","text":"<p>Sometimes it is useful to allow other users to run Cobalt commands on a given job such as <code>qhold</code>, <code>qrls</code>, or <code>qdel</code>. A list of users can be allowed to run commands on your job by submitting a list of users to <code>qsub</code>, <code>cqsub</code>, or <code>qalter</code> using the flag <code>--run_users</code>. Specified users need not be in the same project under which the job was submitted.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#example_3","title":"Example","text":"<p><pre><code>qsub -t 10 -n 16 --attrs filesystems=grand,eagle,home location=thetagpu01:thetagpu02 --run_users frodo:sam:pippin myprog.exe\n</code></pre> As a convenience, all users belonging to the project under which a job was submitted can be added to a list of users that may control a job by using the <code>--run_project</code> flag.</p> <p>Users who have been added to the list can run any command that the job-submitter could run on a job. This includes <code>qhold</code>, <code>qrls</code>, <code>qalter</code>, and <code>qdel</code>.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#group-running-and-file-system-groups","title":"Group Running and File System Groups","text":"<p>While setting this list of users allows any of the listed users to run Cobalt commands on a job, it does not do anything about the permissions of any files involved with the job. Those must be handled by the user(s) setting appropriate permissions on their directories to allow users in their group to read and write files as appropriate. If your project needs a group on the file system to share files or a user needs to be added, email User Support.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#more-information","title":"More Information","text":"<p>For more information on Cobalt commands, their options, consult the manpages on the system. The same information may be found online in the Cobalt Command Reference.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#using-the-job-resource-manager-commands-options-and-examples","title":"Using the Job Resource Manager: Commands, Options, and Examples","text":"<p>This document provides examples of how to submit jobs on our systems. It also provides examples of commands that can be used to query the status of jobs, what partitions are available, etc. For an introduction to using the job resource manager and running jobs on ThetaGPU, see Running Jobs on ThetaGPU. </p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#submit-a-job-request","title":"Submit a Job Request","text":"<p>Use <code>qsub</code> to submit a job. Unlike jobs on the ALCF Blue Gene systems, all jobs on ThetaGPU are either script or interactive.</p> <p>Run the script <code>jobscript.sh</code> with 2 nodes for a maximum of 15 minutes: <pre><code>qsub -n 2 -t 15 --attrs filesystems=theta-fs0 jobscript.sh\n</code></pre> To submit jobs to a particular queue, use <code>qsub -q &lt;queue_name&gt;</code>.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#charge-a-job-to-a-project","title":"Charge a Job to a Project","text":"<p>Use <code>qsub -A &lt;project_name&gt;</code> to charge a job to a particular project.</p> <p>To run <code>jobscript.sh</code> with 2 nodes for a maximum of 15 minutes and charge the job to MyProject: <pre><code>qsub -n 2 -t 15 --attrs filesystems=grand,home -A MyProject jobscript.sh\n</code></pre> To see which projects you are a member of: <pre><code>projects\n</code></pre> You can use the environment variable <code>COBALT_PROJ</code> to set your default project. qsub -A takes precedence over <code>COBALT_PROJ</code>.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#delete-a-job-from-the-queue","title":"Delete a Job from the Queue","text":"<p>To delete a job from the queue, use the qdel command. For example for job with ID of 34586. <pre><code>qdel 34586\n</code></pre> Depending on the stage of a job\u2019s lifetime, qdel may not complete immediately, especially if the delete is issued during startup on a job that is changing memory modes and rebooting a node. If the job does not ultimately terminate, contact support@alcf.anl.gov with the jobid so that an administrator can take appropriate cleanup actions and administratively terminate the job.</p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#query-partition-availability","title":"Query Partition Availability","text":"<p>To determine which partitions are currently available to the scheduler, use the nodelist command. This command provides a list of node ids, names, queue, and state as well as any backfill windows. This command can only be run on the service nodes (<code>thetagpusn1-2</code>), it cannot be run on the compute nodes. </p> <p>For example on <code>thetagpusnX</code> it displays: <pre><code>Host             Queue                                       State      Backfill\n================================================================================== \n[...]\nthetagpu16       CompBioAffin:backfill:full-node             down       -       \nthetagpu16-gpu0  single-gpu                                  idle       -       \nthetagpu16-gpu1  single-gpu                                  allocated  -       \nthetagpu16-gpu2  single-gpu                                  allocated  -       \nthetagpu16-gpu3  single-gpu                                  allocated  -       \nthetagpu16-gpu4  single-gpu                                  idle       -       \nthetagpu16-gpu5  single-gpu                                  idle       -       \nthetagpu16-gpu6  single-gpu                                  idle       -       \nthetagpu16-gpu7  single-gpu                                  idle       -       \nthetagpu17       CompBioAffin:backfill:full-node             allocated  -       \nthetagpu17-gpu0  single-gpu                                  down       -       \nthetagpu17-gpu1  single-gpu                                  down       -       \nthetagpu17-gpu2  single-gpu                                  down       -       \nthetagpu17-gpu3  single-gpu                                  down       -       \nthetagpu17-gpu4  single-gpu                                  down       -       \nthetagpu17-gpu5  single-gpu                                  down       -       \nthetagpu17-gpu6  single-gpu                                  down       -       \nthetagpu17-gpu7  single-gpu                                  down       -       \nthetagpu18       CompBioAffin:backfill:full-node             allocated  -       \n[...]\n</code></pre></p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#specifying-filesystems","title":"Specifying Filesystems","text":"<p>On ThetaGPU and other systems running Cobalt at the ALCF your job submission should specify which filesystems your will be using.  In the event that a filesystem becomes unavailable, this information is used to preserve jobs that would use that filesystem while allowing other jobs that are not using an affected filesystem to proceed to run normally.</p> <p>You may specify your filesystem by adding <code>filesystems=&lt;list of filesystems&gt;</code> to the <code>--attrs</code> argument of qsub in Cobalt. Valid filesystems are <code>home</code>, <code>eagle</code>, <code>grand</code>, and <code>theta-fs0</code>. The list is comma-delimited. </p> <p>For example, to request the <code>home</code> and <code>eagle</code> filesystems for your job you would add <code>filesystems=home,eagle</code> to your <code>qsub</code> command. If this is not specified a warning will be printed and then the job will be tagged as requesting all filesystems and may be held unnecessarily if a filesystem is not currently available. The warnings are written to stderr of <code>qsub</code> and <code>qalter</code> commands that change the value of the <code>--attrs</code> flag.  Scripts that are parsing stderr from these utilities may encounter errors from the additional warnings if filesystems are not specified in these commands.</p> <p>If a job is submitted while a filesystem it requested is marked down, the job will automatically be placed into a <code>user_hold</code> status and a warning message will be printed, but the job will be otherwise queued. The job is also placed into <code>admin_hold</code> status by a sysadmin script. Once the affected filesystem has been returned to normal operation, the <code>admin_hold</code> is released. You are responsible for releasing the <code>user_hold</code> once you receive the message that the affected filesystem has been returned to normal operation. The job cannot run until both the holds are released.</p> <p>If a job requesting a filesystem that is marked down is already in the queue, it will be placed on <code>admin_hold</code> status and will be released once the filesystem is operational. <pre><code>qsub -n 1 -t 30 -q full-node --attrs filesystems=home,grand -A Project ./my_job.sh\n</code></pre> To update the filesystems list for your job, use <code>qalter</code>. Note that <code>qalter --attrs</code> is a replace and not an update operation. This means that you should once again specify all the attributes that you had in the original <code>qsub</code> command. <pre><code>qalter --attr filesystems=home,eagle:mig-mode=True &lt;jobid&gt;\n</code></pre> To release user hold: <pre><code>qrls &lt;jobid&gt;\n</code></pre></p>"},{"location":"sophia/queueing-and-running-jobs/job-and-queue-scheduling/#references","title":"References","text":""}]}